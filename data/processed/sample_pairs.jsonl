{"paper_id": "762", "paper_text": "TITLE: PERCEIVING FROM LOW FIDELITY VISUAL INPUT\n\nABSTRACT: Humans perceive their surroundings in great detail even though most of our visual field is reduced to low-fidelity color-deprived (e.g., dichromatic) input by the retina. In contrast, most deep learning architectures deploy computational resources homogeneously to every part of the visual input. Is such a prodigal deployment of resources necessary? In this paper, we present a framework for investigating the extent to which connectionist architectures can perceive an image in full detail even when presented with low acuity, distorted input. Our goal is to initiate investigations that will be fruitful both for engineering better networks and also for eventually testing hypotheses on the neural mechanisms responsible for our own visual system’s ability to perceive missing information. We find that networks can compensate for low acuity input by learning global feature functions that allow the network to fill in some of the missing details. For example, the networks accurately perceive shape and color in the periphery, even when 75% of the input is achromatic and low resolution. On the other hand, the network is prone to similar mistakes as humans; for example, when presented with a fully grayscale landscape image, it perceives the sky as blue when the sky is actually a red sunset.\n\n1 INTRODUCTION\nMost deep learning architectures process every visual input component when performing a task; for example, the input layer of many ImageNet architectures considers all pixels in every region of a preprocessed image when learning an image classifier or making classification decisions (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2014). In contrast, the human visual system has just a small fovea of high resolution chromatic input allowing it to more judiciously budget computational resources (Lennie, 2003). In order to receive additional information in the field of view, we make either covert or overt shifts of attention. Overt shifts of attention or eyemovements allow us to bring the fovea over particular locations in the environment that are relevant to current behavior. To avoid the serial nature of processing as demanded from overt shifts of attention, our visual system can also engage in covert shifts of attention in which the eyes remain fixated on one location but attention is deployed to a different location.\nYet, even without resorting to overt shifts of attention, we still perceive the world in high detail. This is somewhat remarkable if you consider that the human retina receives an estimated 10 million bits per second which far exceeds the computational resources available to our visual system to assimilate at any given time (Koch et al., 2006). Our own fovea takes up only 4% of the entire retina (Michels & CP Rice, 1990) and is solely responsible for sharp central full color vision with maximum acuity; acuity which diminishes rapidly with eccentricity from the fovea (Cowey &\n∗Email: fwick@cs.umb.edu\nRolls, 1974). As a result, visual performance is best at the fovea and progressively worse towards the periphery (Low, 1946). Indeed, our visual cortex is receiving distorted color-deprived visual input except for the central two degrees of the visual field (Hansen et al., 2009). Additionally, we have blind spots in the retina that receive no visual input. Yet, we are mostly unaware of these distortions. Even when confronted with actual blurry or distorted visual input, our visual system is good at extracting the scene contents and context. For instance, our system can recognize faces and emotions expressed by those faces in resolutions as low as 16 x 16 pixels (Sinha et al., 2006). We can reliably extract contents of a scene from the gist of an image (Oliva, 2005) even at low resolutions (Potter & Levy, 1969; Judd et al., 2011). Recently, Ullman et al. (Ullman et al., 2016) has shown that our visual system is capable of recognizing contents of images from critical feature configurations (called minimal recognizable images or MIRCs) that current deep learning systems cannot utilize for similar tasks. These MIRCS resemble foveations on an image and their results reveal that the human visual system employs features and processes that are not used by current deep networks. Similarly, little attention has been given to how these networks deal with distorted or noisy inputs. We draw inspiration from the abilities of the human visual system and propose a framework to study questions related to whether an artificial neural network can learn to perceive an image from low fidelity input.\nIn this paper, we want to understand what kind of information can be gleaned from low-fidelity inputs. What can be gleaned from a single foveal glimpse? What is the most predictive region of an image? We present a framework for studying such questions based on autoencoders. In contrast to traditional or denoising autoencoders (Vincent et al., 2008), which learn to reconstruct the original image in the presence of random salt and pepper noise, our autoencoders attempt to reconstruct original high-detail inputs from systematically corrupted lower-detail foveated versions of those images (that is, images that are entirely low detail except perhaps a small “fovea” of high detail). Thus, we have taken to calling them defoveating autoencoders (DFAE). We find that even relatively simple DFAE architectures are able to perceive color, shape and contrast information, but fail to recover high-frequency information (e.g., textures) when confronted with extremely impoverished input. Interestingly, as the amount of detail present in the input diminishes, the structure of the learnt features becomes increasingly global.\n\n2 RELATED WORK\nCorrupting the input or hidden layers via noise to improve neural networks is an area of active study (Bishop, 1995; LeCun et al., 1989; Vincent et al., 2010; Rifai et al., 2011; Xie et al., 2012; Schuler et al., 2013). A highly related framework is denoising autoencoders in which the input (or sometimes hidden layer) is corrupted and the network must reproduce the non-corrupted output (Vincent et al., 2010). However, the form of our input corruption is systematic, not random. Further, the emphasis of our work is to understand to what extent a given architecture can perform perceptual filling-in (Komatsu, 2006), our brain’s ability to perceive content that is not explicitly present in our visual input, from retina-like distorted inputs. We study this capability by varying the type of input distortion and examining (qualitatively and quantitatively) the ability of the network to perceive.\nNote that one can consider a low-resolution image as yet another type of noisy input. Thus, another area of related work is image super-resolution in which the goal is to learn a transform from a lowresolution image to a high-resolution image (Behnke, 2001; Cui et al., 2014; Dong et al., 2014), and also image denoising (Jain & Seung, 2009). These are exciting applications for deep learning, but again, our emphasis is on studying a specific set of scientific questions rather than engineering a solution to a specific image-cleaning problem.\nThere has been tremendous interest in applying attention to deep learning architectures (Larochelle & Hinton, 2010; Mnih et al., 2014; Bahdanau et al., 2014; Xu et al., 2015). Such work has lead to improvements in tasks ranging from machine translation (Bahdanau et al., 2014) to image captioning (Xu et al., 2015). In many of these frameworks, attention interacts with the limited resources in various ways (either by sequentially directing attention (Mnih et al., 2014) or by allowing the network to recall relevant information from the past history (Xu et al., 2015)). Our work is complementary in that we aim to study the limited resource itself: in this work, a single foveal glimpse. We further suggest a way of extending our framework to incorporate attention, but we save investigation for future work.\n\n3 FRAMEWORK: DEFOVEATING AUTOENCODERS (DFAE)\nWe now present a framework for studying the extent to which neural networks can “perceive” an image given various types of low-detail (or foveated) inputs. We begin by specifying a space of neural network architectures and by precisely defining a notion of perceives that we can measure. It is important that the framework is general and not dependent on a specific task such as image classification in which, for example, the ability to learn domain-specific discriminating features might make it easy to solve the classification problem without fully modeling the structure of the input. This is undesirable because then we are unable to trust classification accuracy as a reliable surrogate for perceiving.\nWith this in mind, we focus instead on generative models of the raw input data itself, specifically autoencoders (AE). The AE’s hidden units h are analogous to the intermediate neurons in our visual system that capture features and structure of the visual input. Similarly, the AE’s weights W forge visual memories of the training set and are thus analogous to long-term memory. When these weights are properly trained, the activations of the hidden units reflect how the network is perceiving a novel input. However, since these units are not directly interpretable, we indirectly measure how well the network perceives by evaluating the similarity between the original and generated (high-detail) images: the more similar the images are, the better the network is able to perceive.\nMore formally, let x be the original input image and x̂ = φ(x) be a lower-detail foveated version of that image. That is, a version of the image which is mostly low-detail (e.g., downsampled, black-and-white, or both) except for possibly a small portion which is high-detail (mimicking our own fovea). For example, if we encode images as vectors of floats between 0 and 1 (reflecting pixel intensities in RGB or grayscale) then we might define a class of foveation functions as φ : [0, 1]n → [0, 1]m s.t. m n and the foveation function might downsample the original image according to the eccentricity from the image center while also removing most of the vector components corresponding to color. We then employ the autoencoder to defoveate x̂ by generating a high-quality output image y = f(x̂;W ) in which, for example, y ∈ [0, 1]n. Finally, we can then measure the similarity between y and x as (1) a surrogate for how well the network perceives from the foveated input and (2) part of a loss function to train the network.\nIn summary, DFAEs simply comprise:\n1. A foveation function that filters an original image by removing detail (color, downsampling, blurring, etc). We will later make this the independent variables in our experiments so we can study the effect of different types of input distortion.\n2. An autoencoder network that inputs the low-detail foveated input, but is trained to output the high-detail original image.\n3. A loss function for measuring the quality of the reconstruction against the original image and for training the network.\nNote that much like denoising autoencoders, these autoencoders reconstruct an original image from a corrupted version. However, the form of corruption is a systematic foveation instead of random noise (Vincent et al., 2008). Thus, we have termed these models defoveating autoencoders or DFAEs.\n\n3.1 DFAE ARCHITECTURE AND LOSS FUNCTION\nIn our experiments, we study DFAEs with fully connected layers. That is, DFAEs of the form: x̂ = φ(x) h(0) = tanh ( W (0)x̂ ) h(i) = tanh ( W (i)h(i−1) ) for i = 1, · · · , k − 1 y = σ ( W (k)h(k−1)\n) where σ is the logistic function. The sigmoid in the final layer conveniently allows us to compare the pixel intensities between the generated image y and the original image x directly, without having to post-process the output values. We experiment with the number of hidden units per-layer as well as the number of layers. For training, we could employ the traditional mean-squared (MSE) error or cross-entropy loss, but we found that the domain-specific loss function of peak signal-to-noise ratio\n(PSNR) yielded much better training behavior. The PSNR between generated image a y = f(φ(x)) and its original input x is defined as follows:\nLH(x, y) = log10\n( 1√\nMSE(x, y)\n) where MSE(x, y) = n−1(x− y)T (x− y) (1)\nNetwork parameters were initialized at random in the range [-0.1,0.1] and loss was minimized by stochastic gradient descent with adagrad updates (Duchi et al., 2011).\n\n3.2 RECURRENT DFAES FOR SEQUENCES OF FOVEATIONS\nThe above architecture is useful for studying single foveations, which is the primary focus of this work. However, we remark that it is straightforward to augment DFAEs with recurrent connections to handle a sequence of foveations similar to what has has been done for solving classification tasks with attention (Mnih et al., 2014). First, augment the foveation function φ to include a locus ` on which the fovea is centered. Second, a saccade function s(ht;Ws) predicts such a locus from the DFAE’s current hidden states ht, and finally we make the hidden state recurrent via a function g(ht−1,Wg). Putting this all together yields the following architecture:\nx̂t = φ(xt, `t) foveate the image at location ` ht = fe(g(ht−1;Wg), x̂t;W ) encode: compute new hidden states `t = s(ht;Ws) compute new locus for next foveation yt = fd(ht) decode: reconstruct high detail image\nNow the DFAE can handle a sequence of foveations, making it to possible to study the effects of overt attention and also explore the ability to learn from a sequence of foveations (like a language model) rather than a ground-truth image (the analog of which is not available to a human). However, interactions between attention and input acuity are beyond the scope of this work.\n\n4 EXPERIMENTS\nWe are interested in the question of whether an artificial neural network can perceive an image from foveated input images and we evaluate this ability by measuring how well the autoencoders can reconstruct a full-resolution image from the low-detail foveated input. In these experiments, we fix the architecture of our network to the family described in the previous section and vary the type of foveation, the number of hidden units and the number of layers and study the learnt features and reconstruction accuracy. We address the following questions: Can the network perceive aspects of the image that are not present in the input? What can it perceive and under what conditions? Can the network perceive color in the periphery? How much capacity is required to perceive missing details? What is the nature of network’s learnt solution to the problem of filling-in?\n\n4.1 FOVEATION FUNCTIONS\nIn our experiments, we study several different foveation functions. In many cases, downsampling is employed as part of the foveation function for which we employ the nearest neighbor interpolation algorithm. We chose nearest neighbor because it is especially brutal in that it does not smooth over neighboring pixels when interpolating. Foveation functions include:\n• downsampled factor d (DS-D): no fovea is present, the entire image is uniformly downsampled by a factor of d using the nearest neighbor interpolation method. For example, a factor of 4 transforms a 28x28 image to a 7x7 image and approximately 94% of the pixels are removed. For color images we downsample each channel (RGB) separately, resulting in color distortion. We test factors of 2, 4, 7 (MNIST) and 2, 4, 8 (CIFAR).\n• scotoma r (SCT-R): entire regions (r = 25%, 50% and 75%) of the image are removed (by setting the intensities to 0) to create a blind spot/region, but the rest of the image remains at the original resolution. We experiment with the location of the scotoma (centered or not).\n• fovea r (FOV-R): only a small fovea r of high resolution (r = 25% or 6%); the rest of the image is downsampled by a factor of 4.\n• achromatic r (ACH-R): only a region of size r has color; color is removed from the periphery by averaging the RGB channels into a single grayscale channel.\n• fovea-achromatic r (FOVA-R): combines the fovea r with the achromatic region: only the foveated region is in color, the rest of the image is in grayscale and downsampled by a factor of 4.1\n1Note that the achromatic periphery we study here is a more severe distortion than the human periphery, which has dichromatic color reception; though this varies from one individual to another.\n\n4.2 DATASETS AND PRE-PROCESSING\nWe used two datasets in our experiments: MNIST and CIFAR100. The MNIST database consists of 28 x 28 handwritten digits and has a training set of 60,000 examples and a test set of 10,000 examples. Therefore each class has 6000 examples. The CIFAR100 dataset consists of 32 x 32 color images of 100 classes. Some examples of classes are: flowers, large natural outdoor scenes, insects, people, vehicles etc. Each class has 600 examples. The training set consists of 50,000 images and the test set consists of 10,000 images. We trained DFAEs on the MNIST and CIFAR100 dataset (in grayscale and color). We normalized the datasets so that the pixel values are between 0 and 1 and additionally, zero-centered them. This step corresponds to local brightness and contrast normalization. Aside from this, we do no other preprocessing such as patch extraction or whitening.\n\n4.3 THE EFFECT OF INPUT ACUITY ON THE NETWORK\nThe purpose of this experiment is to study how various levels of input acuity affect the performance of the network for the case in which no fovea is available. In addition to input acuity (downsampling factors of 1,2,4,7,8), the other variables to consider are the number of hidden units per layer and the number of layers. In pilot experiments we determined that when the number of hidden units was less than the downsampled input size, DFAEs performed poorly; further, that —unlike CNNs for image classification—additional hidden layers (beyond two) did not improve performance much. Therefore, we report results with overcomplete representations of just one or two hidden layers. First, to establish baselines, we compare one of the networks to various interpolation-based upsampling methods available in image editing software (nearest-neighbor, bilinear, bicubic). The single layer DFAE outperforms these standard algorithms on both datasets (see Figure 1).\nFor qualitative evaluation, Figure 2 contains examples of the reconstructed images by a single layer DFAE. The images produced by the DFAE is compared to upsampled reconstructions via bilinear interpolation. Especially on MNIST, DFAEs can correctly extract the contents of a downsampled input even when 94% of pixels are removed. A compelling example is that even when faced with a blank input (due to aggressive nearest-neighbor downsampling), as seen in Figure 2a, the DFAE can correctly perceive the digit 1. Though in general, the performance of DFAEs suffered when the input was downsampled beyond a factor of 4. When the DFAE made predictions based on the extremely impoverished input, most of the reconstructions were incorrect. Yet, interestingly, the incorrect reconstructions still resembled digits. We hypothesize that this is due to the global feature functions\nlearnt by the network. Indeed, we observe in Figure 3 that for MNIST, as the downsampling factor increases, the global structure in the features2 also increases: when the input was downsampled by a factor of two (resp. four, seven), it was forced to learn stroke like features (resp. full/partial digits, superimposed digits). A curiously similar result was observed by Vincent et al. Vincent et al. (2010), where their denoising autoencoder learnt global structures when it was trained on randomly noisy inputs.\nOn the other hand, many filters learnt on CIFAR100 images were not directly interpretable. Though, in some cases the network learnt global features such as a specific color gradients or locally circular blobs which probably enabled it to be better at reconstructing low frequency shape information, color gist, and landscapes particularly well. The reconstructed natural images as seen in Figure 2b show that the DFAE learnt a smoothing and centering function. The DFAEs employed here could predict the shape of objects in the natural images but not the high frequency details (e.g., texture).\n\n4.4 RECONSTRUCTING FOVEATED INPUTS\nUntil now, we evaluated DFAEs on uniformly downsampled images, but we are especially interested in the case for which some high resolution input is available, more closely resembling the retina. In this section, we evaluate DFAEs on foveated inputs, SCT-R and FOV-R, as described in Section 4.1.\nThe scotoma allows us to isolate the contribution of the fovea alone (without help from the periphery). Variable-sized areas of region (r = 25%, 50%, 75%, 75% centered) were removed from the original input. The location of removal was chosen randomly from the four quadrants of the input image, except for the condition where 75% of the image around the center was removed. Since\n2each feature is the the final output layer weights corresponding to a particular hidden unit\na majority of the input images have a subject of interest, we tested if the central region contained enough information to reconstruct the rest of the image.\nThe reconstructions in Figure 5a show the DFAE does not perform well when r > 25%. When r = 50%, the DFAE can only reconstruct landscapes, shape and symmetry, demonstrating its ability to extract low frequency information. When r = 75% and 75% centered, the reconstruction process breaks down and the DFAE cannot predict the input beyond the given region of information. The filters learnt under these conditions look similar to the downsampled condition, but with larger blobs.\nIn FOV-R inputs, r is the same as SCT-R inputs and we chose to use downsampling factor 4 for regions outside the fovea since previous experiments revealed that DFAEs cannot reconstruct inputs downsampled beyond this factor. Figure 5b shows the reconstructed images from FOV-R inputs and Figure 5c show the error rate of reconstruction. The cluster of red lines with lower error rates show that the DFAE performed considerably well with FOV-R than SCT-R inputs. The performance was better ( 1% error for r = 75% centered) than an DFAE trained with uniformly downsampled inputs (1.5% error). This result is not surprising, given that FOV-R contains additional information from regions outside the fovea. These results suggests that a small number of foveations containing rich details might be all these neural networks need to extract contents of the input in higher detail.\n\n4.5 RECONSTRUCTING COLOR FROM FOVEATED INPUTS\nIt is well known that the human visual system loses chromatic sensitivity towards the periphery of the retina. Recently, there has been interest in how deep networks, specifically convolutional neural networks (CNNs), can learn to color grayscale images (Dahl) and learn artistic style Gatys et al. (2015). Specifically in Dahl reconstructions from grayscale images, numerous cases of the colorized images produced were muted or sepia colored. The problem of colorization which is inherently illposed was treated as a classification task in these studies. Can DFAEs perceive color if it is absent in the input?\nWe investigated this question using ACH-R and FOVA-R inputs described in section 5.1. The regions of color tested were r = 0% or no color, 6%, 25% and 100% or full color. Figure 6a and 6b show examples of color reconstructions of the these input types. When the DFAE is trained with full color ACH-R inputs, it can make mistakes in reconstructing the right color as seen in Figure 6a. For example: it colors the yellow flower as pink and the purplish-red landscape as blue. When the input is grayscale (no color, r = 0%), the colorizations are gray, muted, sepia toned or simply incorrect in the case of landscapes. But if there is a fovea of color, the single layer DFAE can reconstruct the colorizations correctly. Of course, if the fovea of color is reduced, i.e. 6%, the color reconstruction accuracy falls off but not too drastically. For example, it predicts a yellowish tone for the sunflower among a bed of brown leaves. The critical result is that the performance difference between 100% or full colored inputs and foveated color inputs is small as seen in Figure 6c and 6d. These results suggest that color reconstructions can be just as accurate if these networks can figure out the color of one region of the image accurately as opposed to every region in the image. Similar to the human visual system, these networks are capable of determining accurate colors in the periphery if color information is available at foveation.\n\n5 DISCUSSION AND CONCLUSIONS\nWe presented a framework for studying whether a given neural network architecture can perceive visual details not explicitly present in the input. Using the framework, we studied a simple fullyconnected network and found it could fill in missing details such as shape, color and contrast, but not texture. We found that the network compensates for missing details by learning global features which when appropriately activated by the surround, effectively infer the missing information. Thus, the network performs perceptual filling-in even though the layers lack the lateral connections and grid-like structure that have been postulated in the isomorphic filling in theory (von der Heydt et al., 2003). Further, the network appears to learn correlations between color and shape: when presented a sunflower input in which only the dark brown center and the inner yellow petals were in color, the network correctly perceived that the surrounding leaves are green; in the case of landscapes, the network incorrectly perceived the sky as blue, even when a small fovea of red sunset is present in the input. Upon inspection, the network indeed learns global color features and these features may\nbe activated by input patterns corresponding to certain shapes causing the network to occasionally over-generalize, but further investigation is needed to fully confirm this hypothesis.\nA potentially interesting line of future research would be to employ the DFAE framework to test the existing hypotheses for how our own visual system performs perceptual filling in: e.g., the process by which our brain infers information that is not explicitly present in our sensory input from the surround (Komatsu, 2006). The specific architectures we studied here are a far cry from the true mechanisms, but in future work we could implement several architectures that capture the essence of competing theories: Which neural architectures exhibit similar behaviors and abilities as humans? Will the architecture be fooled in the same way as humans by optical illusions such as the Cornsweet illusion or the Troxler effect? When does it perceive “The Dress” as blue and gold or white and black? By answering questions such as these, the DFAE framework could provide evidence for one theory (or neural mechanism) over another, and these theories can be further investigated with behavioral and neurological experiments.", "ground_truth": "ICLR 2017 conference submission\n\n---\n\nFilling in the details: Perceiving from low fidelity visual input\n\n---\n\nHumans perceive their surroundings in great detail even though most of our visual field is reduced to low-fidelity color-deprived (e.g., dichromatic) input by the retina. In contrast, most deep learning architectures deploy computational resources homogeneously to every part of the visual input. Is such a prodigal deployment of resources necessary? In this paper, we present a framework for investigating the extent to which connectionist architectures can perceive an image in full detail even when presented with low acuity, distorted input. Our goal is to initiate investigations that will be fruitful both for engineering better networks and also for eventually testing hypotheses on the neural mechanisms responsible for our own visual system's ability to perceive missing information. We find that networks can compensate for low acuity input by learning global feature functions that allow the network to fill in some of the missing details. For example, the networks accurately perceive shape and color in the periphery, even when 75\\% of the input is achromatic and low resolution. On the other hand, the network is prone to similar mistakes as humans; for example, when presented with a fully grayscale landscape image, it perceives the sky as blue when the sky is actually a red sunset.\n\n---\n\nThis paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability. Specifically, the paper proposed to use Auto-Encoder (AE) as the network to reconstruct the low fidelity of visual input. Moreover, similar to Mnih et al. (2014), the paper also proposed to use a recurrent fashion to mimic the sequential behavior the human visual system. \n\nI think the paper is well motivated. However, there are several concerns:\n1. The baselines of the paper are too weak. Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models. The author should compare with the STOA methods such as\n\n---\n\nICLR committee final decision\n\n---\n\nICLR 2017 pcs\n\n---\n\nThe program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers find this direction of exploration to be interesting, but a bit preliminary at the moment. Authors are strongly encouraged to incorporate reviewer comments to make future iterations of the work stronger.\n\n---\n\n06 Feb 2017\n\n---\n\nICLR 2017 conference AnonReviewer1\n\n---\n\nsome interesting cases, but lacks focus\n\n---\n\nThis paper aims to characterize the perceptual ability of a neural network under different input conditions. This is done by manipulating the input image x in various ways (e.g. downsamplig, foveating), and training an auto-encoder to reconstruct the original full-resolution image. MSE and qualitative results are shown and compared for the different input conditions.\n\nUnfortunately, this paper seems to lack focus, presenting a set of preliminary inspections with few concrete conclusions. For example, at the end of sec 4.4, \"This result is not surprising, given that FOV-R contains additional information .... These results suggests that a small number of foveations containing rich details might be all these neural networks need....\". But this hypothesis is left dangling: What detailed regions are needed, and from where? For what sort of tasks?\n\nSecondly, it isn't clear to me what reconstruction behaviors are caused by a fundamental perception of the input, and what are artifacts of the autoencoder and pixelwise l2 loss? A prime example is texture, which the autoencoder fails to recover. But with a pixelwise loss, the network must predict high-frequency textures nearly pixel-for-pixel at training time; if this is impossible, then it will generate a pixelwise average of the training samples --- a flat region. So then the network's inability to reconstruct textures is due to a problem generating them, specifically averaging from the training loss, not necessarily an issue in perceiving textures. A network trained a different way (perhaps an adversarial network) may infer a texture is there, even if it wouldn't be able to generate it in a pixelwise l2 sense.\n\nSimilarly, the ability to perform color reconstruction given a color glimpse I think has much to do with disambiguating the color of an object/scene: If there is an ambiguity, the network won't know which to \"choose\" (white flower or yellow flower?) and output an average, which is why there are so many sepia tones. However, in its section on this, the paper only measures the reconstruction error for different amounts of color given, and does not drill very far into any hypotheses for why this behavior occurs.\n\nThere are some interesting measurements here, such as the amount of color needed in the foveation to reconstruct a color image, and the discussion on global features, which may start to get at a mechanism by which glimpses may propagate to an entire reconstruction. But overall it's hard to know what to take away from this paper. What are larger concrete conclusions that can be garnered from the details, and what mechanisms bring them about? Can these be more thoroughly explored with more focus?\n\n---\n\n18 Dec 2016\n\n---\n\nICLR 2017 conference AnonReviewer2\n\n---\n\nInteresting idea, evaluation could be improved\n\n---\n\nI like the idea the paper is exploring. Nevertheless I see some issues with the analysis:\n\n- To get a better understanding of the quality of the results, I think at least some state-of-the-art comparisons should be included (e.g. by setting d times d pixel patches too their average and applying a denoising autoencoder). If they perform significantly better, then this indicates that the presented model is not yet taking all the information from the input image that could be used.\n- SCT-R and FOV-R are supposed to test how much information can be restored from the Fovea alone as opposed to the Fovea together with low resolution periphery. However, there is an additional difference between the two conditions: According to the paper, in SCT-R, part of the image was set to zero, while in FOV-R it was removed alltogether. With only one or two hidden layers, I could easily imagine this making a difference.\n- On page 4, you compare the performance of FOV-R (1% error) with that of DS-D (1.5%) and attribute this to information about the periphery that the autoencoder extracts from the fovea. While this might be the case, at least part of the reduced error will be due to the fact that the fovea is (hopefully) perfectly reconstructed. To answer the actual question \"how much additional information about the periphery can be extracted from the fovea\", you should consider calculating the error only in the periphery, i.e. the part of the image where DS-D and FOV-R got exactly the same input for. Then any decreased error is only due to the additional fovea information.\n\nOther issues:\n- The images in Figure 2 (a) and (b) in the rows \"factor 2\", \"factor 4\", \"factor 8\" look very blurry. There seems some interpolation to be going on (although slighly different than the bilinear interpolation). This makes it hard to asses how much information is in these images. I think it would be much more insightfull to print them with \"nearest\" interpolation.\n- Figure 3 caption too vague. Maybe add something like footnote 2?\n- Often figures appear too early in paper which leads to lots of distance between text and figures.\n\n---\n\n16 Dec 2016\n\n---\n\nICLR 2017 conference AnonReviewer3\n\n---\n\nThis paper is well motivated\n\n---\n\nThis paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability. Specifically, the paper proposed to use Auto-Encoder (AE) as the network to reconstruct the low fidelity of visual input. Moreover, similar to Mnih et al. (2014), the paper also proposed to use a recurrent fashion to mimic the sequential behavior the human visual system. \n\nI think the paper is well motivated. However, there are several concerns:\n1. The baselines of the paper are too weak. Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models. The author should compare with the STOA methods such as\n\n---\n\n16 Dec 2016\n\n---\n\nConclusions and significance?\n\n---\n\nICLR 2017 conference AnonReviewer1\n\n---\n\n02 Dec 2016\n\n---\n\nWhy does the paper use AE in the framework? What is the potential usage of the model? Is there any quantitative comparison with other attention-based models?\n\n---\n\nICLR 2017 conference AnonReviewer3\n\n---\n\n02 Dec 2016\n\n---\n\nSome questions\n\n---\n\nICLR 2017 conference AnonReviewer2\n\n---\n\n02 Dec 2016\n\n---\n\nFarahnaz A. Wick, Michael L. Wick, Marc Pomplun\n\n---\n\nUsing generative models to create images from impoverished input similar to those received by our visual cortex"}
{"paper_id": "391", "paper_text": "TITLE: RECURRENT NEURAL NETWORKS\n\nABSTRACT: Recurrent Neural Networks (RNN) are widely used to solve a variety of problems and as the quantity of data and the amount of available compute have increased, so have model sizes. The number of parameters in recent state-of-the-art networks makes them hard to deploy, especially on mobile phones and embedded devices. The challenge is due to both the size of the model and the time it takes to evaluate it. In order to deploy these RNNs efficiently, we propose a technique to reduce the parameters of a network by pruning weights during the initial training of the network. At the end of training, the parameters of the network are sparse while accuracy is still close to the original dense neural network. The network size is reduced by 8× and the time required to train the model remains constant. Additionally, we can prune a larger dense network to achieve better than baseline performance while still reducing the total number of parameters significantly. Pruning RNNs reduces the size of the model and can also help achieve significant inference time speed-up using sparse matrix multiply. Benchmarks show that using our technique model size can be reduced by 90% and speed-up is around 2× to 7×.\n\n1 INTRODUCTION\nRecent advances in multiple fields such as speech recognition (Graves & Jaitly, 2014; Amodei et al., 2015), language modeling (Józefowicz et al., 2016) and machine translation (Wu et al., 2016) can be at least partially attributed to larger training datasets, larger models and more compute that allows larger models to be trained on larger datasets.\nFor example, the deep neural network used for acoustic modeling in Hannun et al. (2014) had 11 million parameters which grew to approximately 67 million for bidirectional RNNs and further to 116 million for the latest forward only GRU models in Amodei et al. (2015). And in language modeling the size of the non-embedding parameters (mostly in the recurrent layers) have exploded even as various ways of hand engineering sparsity into the embeddings have been explored in Józefowicz et al. (2016) and Chen et al. (2015a).\nThese large models face two significant challenges in deployment. Mobile phones and embedded devices have limited memory and storage and in some cases network bandwidth is also a concern. In addition, the evaluation of these models requires a significant amount of computation. Even in cases when the networks can be evaluated fast enough, it will still have a significant impact on battery life in mobile devices (Han et al., 2015).\nInference performance of RNNs is dominated by the memory bandwidth of the hardware, since most of the work is simply reading in the parameters at every time step. Moving from a dense calculation to a sparse one comes with a penalty, but if the sparsity factor is large enough, then the smaller amount of data required by the sparse routines becomes a win. Furthermore, this suggests that if the parameter sizes can be reduced to fit in cache or other very fast memory, then large speedups could be realized, resulting in a super-linear increase in performance.\nThe more powerful server class GPUs used in data centers can generally perform inference quickly enough to serve one user, but in the data center performance per dollar is very important. Techniques\n∗Now at Google Brain eriche@google.com\nthat allow models to be evaluated faster enable more users to be served per GPU increasing the effective performance per dollar.\nWe propose a method to reduce the number of weights in recurrent neural networks. While the network is training we progressively set more and more weights to zero using a monotonically increasing threshold. By controlling the shape of the function that maps iteration count to threshold value, we can control how sparse the final weight matrices become. We prune all the weights of a recurrent layer; other layer types with significantly fewer parameters are not pruned. Separate threshold functions can be used for each layer, although in practice we use one threshold function per layer type. With this approach, we can achieve sparsity of 90% with a small loss in accuracy. We show this technique works with Gated Recurrent Units (GRU) (Cho et al., 2014) as well as vanilla RNNs.\nIn addition to the benefits of less storage and faster inference, this technique can also improve the accuracy over a dense baseline. By starting with a larger dense matrix than the baseline and then pruning it down, we can achieve equal or better accuracy compared to the baseline but with a much smaller number of parameters.\nThis approach can be implemented easily in current training frameworks and is agnostic to the optimization algorithm. Furthermore, training time does not increase unlike previous approaches such as in Han et al. (2015). State of the art results in speech recognition generally require days to weeks of training time, so a further 3-4× increase in training time is undesirable.\n\n2 RELATED WORK\nThere have been several proposals to reduce the memory footprint of weights and activations in neural networks. One method is to use a fixed point representation to quantize weights to signed bytes and activations to unsigned bytes (Vanhoucke et al., 2011). Another technique that has been tried in the past is to learn a low rank factorization of the weight matrices. One method is to carefully construct one of the factors and learn the other (Denil et al., 2013). Inspired by this technique, a low rank approximation for the convolution layers achieves twice the speed while staying within 1% of the original model in terms of accuracy (Denton et al., 2014). The convolution layer can also be approximated by a smaller set of basis filters (Jaderberg et al., 2014). By doing this they achieve a 2.5x speedup with no loss in accuracy. Quantization techniques like k-means clustering of weights can also reduce the storage size of the models by focusing only on the fully connected layers (Gong et al., 2014). A hash function can also reduce memory footprint by tying together weights that fall in the same hash bucket (Chen et al., 2015b). This reduces the model size by a factor of 8.\nYet another approach to reduce compute and network size is through network pruning. One method is to use several bias techniques to decay weights (Hanson & Pratt, 1989). Yet another approach is to use the diagonal terms of a Hessian matrix to construct a saliency threshold and used this to drop weights that fall below a given saliency threshold (LeCun et al., 1989). In this technique, once a weight has been set to 0, the network is retrained with these weights frozen at 0. Optimal Brain Surgeon is another work in the same vein that prunes weights using the inverse of a Hessian matrix with the additional advantage of no re-training after pruning (Hassibi et al., 1993).\nBoth pruning and quantization techniques can be combined to get impressive gains on AlexNet trained on the ImageNet dataset (Han et al., 2015). In this case, pruning, quantization and subsequent Huffman encoding results in a 35x reduction in model size without affecting accuracy. There has also been some recent work to shrink model size for recurrent and LSTM networks used in automatic speech recognition (ASR) (Lu et al., 2016). By using a hybrid strategy of using Toeplitz matrices for the bottom layer and shared low-rank factors on the top layers, they were able to reduce the parameters of a LSTM by 75% while incurring a 0.3% increase in word error rate (WER).\nOur method is a pruning technique that is computationally efficient for large recurrent networks that have become the norm for automatic speech recognition. Unlike the methods that need to approximate a Hessian (LeCun et al., 1989; Hassibi et al., 1993) our method uses a simple heuristic to choose the threshold used to drop weights. Yet another advantage, when compared to methods that need re-training (Han et al., 2015), is that our pruning technique is part of training and needs no additional re-training. Even though our technique requires judicious choice of pruning hyperparameters, we feel that it is easier than choosing the structure of matrices to guide the sparsification\nfor recurrent networks (Lu et al., 2016). Another approach for pruning feed forward neural networks for speech recognition is using simple threshold to prune all weights (Yu et al., 2012) at a particular epoch. However, we find that gradual pruning produces better results than hard pruning.\n\n3 IMPLEMENTATION\nOur pruning approach involves maintaining a set of masks, a monotonically increasing threshold and a set of hyper parameters that are used to determine the threshold. During model initialization, we create a set of binary masks, one for each weight in the network that are all initially set to one. After every optimizer update step, each weight is multiplied with its corresponding mask. At regular intervals, the masks are updated by setting all parameters that are lower than the threshold to zero.\nThe threshold is computed using hyper-parameters shown in Table 1. The hyper-parameters control the duration, rate and frequency of pruning the parameters for each layer. We use a different set of hyper-parameters for each layer type resulting in a different threshold for each layer type. The threshold is updated at regular intervals using the hyper-parameters according to Algorithm 1. We don’t modify the gradients in the back-propagation step. It is possible for the updates of a pruned weight to be larger than the threshold of that layer. In this case, the weight will be involved in the forward pass again.\nWe provide heuristics to help determine start itr, ramp itr and end itr in table 1. After picking these hyper parameters and assuming that ramp slope(φ) is 1.5× start slope (θ), we calculate (θ) using equation 1.\nθ = 2 ∗ q ∗ freq\n2 ∗ (ramp itr − start itr) + 3 ∗ (end itr − ramp itr) (1)\nIn order to determine q in equation 1, we use an existing weight array from a previously trained model. The weights are sorted using absolute values and we pick the weight corresponding to the 90th percentile as q. This allows us to pick reasonable values for the hyper-parameters required for pruning. A validation set can be used to fine tune these parameters.\nWe only prune the weights of the recurrent and linear layers but not the biases or batch norm parameters since they are much fewer in number compared to the weights. For the recurrent layers, we prune both the input weight matrix and the recurrent weight matrix. Similarly, we prune all the weights in gated recurrent units including those of the reset and update gates.\n\n4 EXPERIMENTS\nWe run all our experiments on a training set of 2100 hours of English speech data and a validation set of 3.5 hours of multi-speaker data. This is a small subset of the datasets that we use to train our\nAlgorithm 1 Pruning Algorithm current itr = 0 while training do\nfor all parameters do param = (param and mask ) if current itr > start itr and current itr < end itr then\nif (current itr mod freq) == 0 then if current itr < ramp itr then = θ ∗ (current itr − start itr + 1)/freq\nelse = (θ ∗ (ramp itr − start itr + 1) + φ ∗ (current itr − ramp itr + 1))/freq end if mask = abs(param) <\nend if end if\nend for current itr += 1\nend while\nstate-of-the-art automatic speech recognition models. We train the models using Nesterov SGD for 20 epochs. Besides the hyper-parameters for determining the threshold, all other hyper-parameters remain unchanged between the dense and sparse training runs. We find that our pruning approach works well for vanilla bidirectional recurrent layers and forward only gated recurrent units.\n\n4.1 BIDIRECTIONAL RNNS\nWe use the Deep Speech 2 model for these experiments. As shown in Table 2, this model has 2 convolution layers, followed by 7 bidirectional recurrent layers and a CTC cost layer. Each recurrent linear layer has 1760 hidden units, creating a network of approximately 67 million parameters. For these experiments, we prune the linear layers that feed into the recurrent layers, the forward and backward recurrent layers and fully connected layer before the CTC layer. These experiments use clipped rectified-linear units (ReLU) σ(x) = min(max(x, 0), 20) as the activation function.\nIn the sparse run, the pruning begins shortly after the first epoch and continues until the 10th epoch. We chose these hyper-parameters so that the model has an overall sparsity of 88% at the end of pruning, which is 8x smaller than the original dense model. The character error rate (CER) on the devset is about 20% worse relative to the dense model as shown in Table 3.\nAn argument against this sparsity result might be that we are taking advantage of a large model that overfits our relatively small dataset. In order to test this hypothesis, we train a dense model with 704 hidden units in each layer, that has approximately the same number of parameters as the final sparse model. Table 3 shows that this model performs worse than the sparse models. Thus sparse model is a better approach to reduce parameters than using a dense model with fewer hidden units.\nIn order to recover the loss in accuracy, we train sparse models with larger recurrent layers with 2560 and 3072 hidden units. Figure 1a shows the training and dev curves for these sparse models compared to the dense baseline model. These experiments use the same hyper-parameters (except for small changes in the pruning hyper-parameters) and the same dataset as the baseline model. As we see in Table 3, the model with 2560 hidden units achieves a 0.75% relative improvement compared to the dense baseline model, while the model with 3072 hidden units has a 3.95% improvement. The dense 2560 model also improves the CER by 11.85% relative to the dense baseline model. The sparse 2560 model is about 12% worse than the corresponding dense model. Both these large models are pruned to achieve a final sparsity of around 92%. These sparse larger models have significantly fewer parameters than the baseline dense model.\nWe also compare our gradual pruning approach to the hard pruning approach proposed in Yu et al. (2012). In their approach, all parameters below a certain threshold are pruned at particular epoch. Table 4 shows the results of pruning the RNN dense baseline model at different epochs to achieve final parameter count ranging from 8 million to 11 million. The network is trained for the same\nnumber of epochs as the gradual pruning experiments. These hard threshold results are compared with the RNN Sparse 1760 model in Table 3. For approximately same number of parameters, gradual pruning is 7% to 9% better than hard pruning.\nWe conclude that pruning models to achieve sparsity of around 90% reduces the relative accuracy of the model by 10% to 20%. However, for a given performance requirement, it is better to prune a larger model than to use a smaller dense model. Gradually pruning a model produces better results than hard pruning.\n\n4.2 GATED RECURRENT UNITS\nWe also experimented with GRU models shown in Table 5, that have 2560 hidden units in the GRU layer and a total of 115 million parameters. For these experiments, we prune all layers except the convolution layers since they have relatively fewer parameters.\nFigure 1b compares the training and dev curves of a sparse GRU model a dense GRU model. The sparse GRU model has a 13.8% drop in the accuracy relative to the dense model. As shown in Table 3, the sparse model has an overall sparsity of 88.6% with 13 million parameters. Similar to the RNN models, we train a sparse GRU model with 3568 hidden units. The dataset and the hyperparameters are not changed from the previous GRU experiments. This model has an overall sparsity of 91.82% with 17.8 million parameters. As shown in Table 3, the model with 3568 hidden units is only 2.2% worse than the baseline dense GRU model. We expect to match the performance of the GRU dense network by slightly lowering the sparsity of this network or by increasing the hidden units for the layers.\nIn addition, we experimented with pruning only the GRU layers and keeping all the parameters in fully connected layers. The accuracy for these experiments is around 7% worse than the baseline dense model. However, this model only achieves 50% compression due to the size of the fully connected layers.\n\n5.1 COMPUTE TIME\nThe success of deep learning in recent years have been driven by large models trained on large datasets. However this also increases the inference time after the models have been deployed. We can mitigate this effect by using sparse layers.\nA General Matrix-Matrix Multiply (GEMM) is the most compute intensive operation in evaluating a neural network model. Table 6 compares times for GEMM for recurrent layers with different number\nTable 6: GEMM times for recurrent layers with different sparsity\nLAYER SIZE SPARSITY LAYER TYPE TIME (µsec) SPEEDUP\n1760 0% RNN 56 1 1760 95% RNN 20 2.8 2560 95% RNN 29 1.93 3072 95% RNN 48 1.16 2560 0% GRU 313 1 2560 95% GRU 46 6.80 3568 95% GRU 89 3.5\nof hidden units that are 95% sparse. The performance benchmark was run using NVIDIA’s CUDNN and cuSPARSE libraries on a TitanX Maxwell GPU and compiled using CUDA 7.5. All experiments are run on a minibatch of 1 and in this case, the operation is known as a sparse matrix-vector product (SpMV). We can achieve speed-ups ranging from 3x to 1.15x depending on the size of the recurrent layer. Similarly, for the GRU models, the speed-ups range from 7x to 3.5x. However, we notice that cuSPARSE performance is substantially lower than the approximately 20x speedup that we would expect by comparing the bandwidth requirements of the 95% sparse and dense networks. State of the art SpMV routines can achieve close to device memory bandwidth for a wide array of matrix shapes and sparsity patterns (see Baxter (2016) and Liu et al. (2013)). This means that the performance should improve by the factor that parameter counts are reduced. Additionally, we find that the cuSPARSE performance degrades with larger batch sizes. It should be possible for a better implementation to further exploit the significant reuse of the weight matrix provided by large batch sizes.\n\n5.2 COMPRESSION\nPruning allows us to reduce the memory footprint of a model which allows them to be deployed on phones and other embedded devices. The Deep Speech 2 model can be compressed from 268 MB to around 32 MB (1760 hidden units) or 64 MB (3072 hidden units). The GRU model can be compressed from 460 MB to 50 MB. These pruned models can be further quantized down to float16 or other smaller datatypes to further reduce the memory requirements without impacting accuracy.\n\n6.1 PRUNING CHARACTERISTICS\nFigure 2a shows the sparsity of all the recurrent layers with the same hyper-parameters used to prune the layers. The layers are ordered such that layer 1 is closest to input and layer 14 is the final recurrent layer before the cost layer. We see that the initial layers are pruned more aggressively compared to the final layers. We also performed experiments where the hyper parameters are different for the recurrent layers resulting in equal sparsity for all the layers. However, we get higher CER for these experiments. We conclude that to get good accuracy, it is important to prune the final layers slightly less than the initial ones.\nIn Figure 2b, we plot the pruning schedule of a 95% sparse recurrent layer of the bidirectional model trained for 20 epochs (55000 iterations). We begin pruning the network at the start of the second epoch at 2700 iterations. We stop pruning a layer after 10 epochs (half the total epochs) are complete at 27000 iterations. We see that nearly 25000 weights are pruned before 5 epochs are complete at around 15000 iterations. In our experiments, we’ve noticed that pruning schedules that are a convex curve tend to outperform schedules with a linear slope.\n\n6.2 PERSISTENT KERNELS\nPersistent Recurrent Neural Networks (Diamos et al., 2016) is a technique that increases the computational intensity of evaluating an RNN by caching the weights in on-chip memory such as caches, block RAM, or register files across multiple timesteps. A high degree of sparsity allows significantly large Persistent RNNs to be stored in on-chip memory. When all the weights are stored in float16, a NVIDIA P100 GPU can support a vanilla RNN size of about 2600 hidden units. With the same datatype, at 90% sparsity, and 99% sparsity, a P100 can support RNNs with about 8000, and 24000 hidden units respectively. We expect these kernels to be bandwidth limited out of the memory that is used to store the parameters. This offers the potential of a 146x speedup compared to the TitanX GPU if the entire RNN layer can be stored in registers rather than the GPU DRAM of a TitanX.\nAdditionally, sparse matrix multiplication involves scheduling and load balancing phases to divide the work up evenly over thousands of threads and to route corresponding weights and activations to individual threads. Since the sparsity patterns for RNNs are fixed over many timesteps these scheduling and load balancing operations can be factored outside of the loop, performed once, and reused many times.\n\n7 CONCLUSION AND FUTURE WORK\nWe have demonstrated that by pruning the weights of RNNs during training we can find sparse models that are more accurate than dense models while significantly reducing model size. These sparse models are especially suited for deployment on mobile devices and on back-end server farms due to their small size and increased computational efficiency. Even with existing sub-optimal sparse matrix-vector libraries we realize speed-ups with these models. This technique is orthogonal to quantization techniques which would allow for even further reductions in model size and corresponding increase in performance.\nWe wish to investigate whether these techniques can generalize to language modeling tasks and if they can effectively reduce the size of embedding layers. We also wish to compare the sparsity generated by our pruning technique to that obtained by L1 regularization.\nWe are investigating training techniques that don’t require maintaining dense matrices for a significant portion of the calculation. Further work remains to implement optimal small batch sparse matrix-dense vector routine for GPUs and ARM processors that would help in deployment.\n\nACKNOWLEDGMENTS\nWe would like to thank Bryan Catanzaro for helpful discussions related to this work.", "ground_truth": "ICLR 2017 conference submission\n\n---\n\nExploring Sparsity in Recurrent Neural Networks\n\n---\n\nRecurrent neural networks (RNN) are widely used to solve a variety of problems and as the quantity of data and the amount of available compute have increased, so have model sizes. The number of parameters in recent state-of-the-art networks makes them hard to deploy, especially on mobile phones and embedded devices. The challenge is due to both the size of the model and the time it takes to evaluate it. In order to deploy these RNNs efficiently, we propose a technique to reduce the parameters of a network by pruning weights during the initial training of the network. At the end of training, the parameters of the network are sparse while accuracy is still close to the original dense neural network. The network size is reduced by 8× and the time required to train the model remains constant. Additionally, we can prune a larger dense network to achieve better than baseline performance while still reducing the total number of parameters significantly. Pruning RNNs reduces the size of the model and can also help achieve significant inference time speed-up using sparse GEMMs. Benchmarks show that using our technique model size can be reduced by 90% and speed-up is around 2× to 7×.\n\n---\n\nICLR committee final decision\n\n---\n\nICLR 2017 pcs\n\n---\n\nHere is a summary of the reviews:\n \n Strengths\n Experiments are done on state-of-the-art networks, on a real speech recognition problem (R3, R1)\n Networks themselves are of a very large size (R3)\n Computational gains are substantial (R3, R4)\n Paper is clear (R1)\n \n Weaknesses\n Experiments are all done on a private dataset (R3)\n No comparison to other pruning approaches (e.g. Han et al.) (R3); AC notes that reviewers added new results which compare to an existing pruning method\n No comparison to distillation techniques (R1)\n Paper doesn't present much novelty in terms of ideas (R3)\n \n The AC encouraged feedback from the reviewers following author rebuttal and paper improvements. Reviewers stated that the improvements made to the paper made it publishable but was still closer to the threshold. R1 who had originally rated the paper 3: a \"clear reject\" updated the score to 6 (just above acceptance).\n \n Considering the reviews and discussions, the AC thinks that this paper is a poster accept. There are no serious flaws, the improvements made to the paper during the discussion paper have satisfied the reviewers, and this is an important topic with practical benefits; evaluated on a real large-scale problem.\n\n---\n\n06 Feb 2017\n\n---\n\nRevision with larger GRU model\n\n---\n\nSharan Narang\n\n---\n\nBased on the feedback from one of the reviewers, we have trained a larger sparse GRU model. This model is 2.2% worse than the GRU Dense baseline while 3.5 times faster than the dense GRU model. This larger GRU network recoups most of the loss in performance due to pruning. We believe that we can match the Dense GRU baseline performance by slightly reducing the sparsity of the network or increasing the number of hidden units. We thank the reviewer for this helpful suggestion. I have uploaded a new revision of this paper with this result.\n\n---\n\n18 Jan 2017\n\n---\n\nNew Revision\n\n---\n\nSharan Narang\n\n---\n\nI have uploaded a new revision which includes results using the pruning method proposed in \"Exploiting sparseness in deep neural networks for large vocabulary speech recognition\" by Yu et. al. The results show that gradual pruning used in our approach performs better (in terms of accuracy) than hard pruning. We thank the reviewers for the suggestion to compare our method with this pruning approach.\n\n---\n\n14 Jan 2017\n\n---\n\nstructurally sparse RNNs\n\n---\n\n(anonymous)\n\n---\n\nThe paper is well motivated to explore the sparsity in RNNs after lots of works on sparse CNNs.\nIn general, methods in CNNs could be generalized to RNNs. It would be more comprehensive to compare the proposed method with those methods used in CNNs. For the speedup part, it would be more convictive to compare with structurally-sparse DNNs, which are recently proposed in CNNs:\n(1)\n\n---\n\n17 Dec 2016\n\n---\n\nICLR 2017 conference AnonReviewer3\n\n---\n\nReview: Exploring Sparsity in Recurrent Neural Networks\n\n---\n\nThe paper proposes a method for pruning weights in neural networks during training to obtain sparse solutions. The approach is applied to an RNN-based system which is trained and evaluated on a speech recognition dataset. The results indicate that large savings in test-time computations can be obtained without affecting the task performance too much. In some cases the method can actually improve the evaluation performance.\n\nThe experiments are done using a state-of-the-art RNN system and the methodology of those experiments seems sound. I like that the effect of the pruning is investigated for networks of very large sizes. The computational gains are clearly substantial. It is a bit unfortunate that all experiments are done using a private dataset. Even with private training data, it would have been nice to see an evaluation on a known test set like the HUB5 for conversational speech. It would also have been nice to see a comparison with some other pruning approaches given the similarity of the proposed method to the work by Han et al. [2] to verify the relative merit of the proposed pruning scheme. While single-stage training looks more elegant at first sight, it may not save much time if more experiments are needed to find good hyperparameter settings for the threshold adaptation scheme. Finally, the dense baseline would have been more convincing if it involved some model compression tricks like training on the soft targets provided by a bigger network.\n\nOverall, the paper is easy to read. The table and figure captions could be a bit more detailed but they are still clear enough. The discussion of potential future speed-ups of sparse recurrent neural networks and memory savings is interesting but not specific to the proposed pruning algorithm. The paper doesn’t motivate the details of the method very well. It’s not clear to me why the threshold has to ramp up after a certain period time for example. If this is based on preliminary findings, the paper should mention that.\n\nSparse neural networks have been the subject of research for a long time and this includes recurrent neural networks (e.g., sparse recurrent weight matrices were standard for echo-state networks [1]). The proposed method is also very similar to the work by Han et al. [2], where a threshold is used to prune weights after training, followed by a retraining phase of the remaining weights. While I think that it is certainly more elegant to replace this three stage procedure with a single training phase, the proposed scheme still contains multiple regimes that resemble such a process by first training without pruning followed by pruning at two different rates and finally training without further pruning again. The main novelty of the work would be the application of such a scheme to RNNs, which are typically more tricky to train than feedforward nets.\n\nImproving scalability is an important driving force of the progress in neural network research. While I don’t think the paper presents much novelty in ideas or scientific insight, it does show that weight pruning can be successfully applied to large practical RNN systems without sacrificing much in performance. The fact that this is possible with such a simple heuristic is a result worth sharing.\n\nPros:\nThe proposed method is successful at reducing the number of parameters in RNNs substantially without sacrificing too much in performance.\nThe experiments are done using a state-of-the-art system for a practical application.\n\nCons:\nThe proposed method is very similar to earlier work and barely novel.\nThere is no comparison with other pruning methods.\nThe data is private and this prevents others from replicating the results.\n\n[1] Jaeger, H. (2001). The “echo state” approach to analyzing and training recurrent neural networks-with an erratum note. Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148, 34.\n\n[2] Han, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for efficient neural networks. In Advances in Neural Information Processing Systems, 2015.\n\n---\n\n15 Dec 2016\n\n---\n\nICLR 2017 conference AnonReviewer4\n\n---\n\nsparsity vs accuracy\n\n---\n\nSummary: The paper presents a technique to convert a dense to sparse network for RNNs. The algorithm will increasingly set more weights to zero during the RNN training phase. This provides a RNN model with less storage requirement and higher inference rate. \n\nPros:\nProposes a pruning method that doesn’t need re-training and doesn’t affect the training phase of RNN. The method achieves 90% sparsity, and hence less number of parameters.\n\nCons & Questions:\nJudiciously choosing hyper parameters for different models and different applications wouldn’t be cumbersome? In equation 1, is q the sparsity of final model? Is there a formula to know what is sparsity, number of parameters and accuracy of final model given a set of hyper parameters, before going through training? (Questions answered)\n\nIn table3, we see a trade-off between number of units and sparsity to achieve better number of parameters or accuracy, or in table5 better speed. Good, but where are the results for GRU sparse big? I mean, accuracy must be similar and still get decent compression rate and speed up. Just like RNN Sparse medium compared with RNN Dense. I can’t see much advantage of pruning and getting high speed-up if you are sacrificing so much accuracy. (Issue fixed with updated data)\n\nWhy sparsity for table3 and table5 are different? In text: “average sparsity of 88%” but in table5 is 95%? Are the models used in table3 different from table5? (Issue fixed)\n\n---\n\n15 Dec 2016 (modified: 22 Jan 2017)\n\n---\n\nICLR 2017 conference AnonReviewer1\n\n---\n\nMuch better connection to prior work, but the baselines could still be stronger\n\n---\n\nUpdated review: 18 Jan. 2017\n\nThanks to the authors for including a comparison to the previously published sparsity method of Yu et al., 2012. The comparison is plausible, though it would be clearer if the authors were to state that the best comparison for the results in Table 4 is the \"RNN Sparse 1760\" result in Table 3.\n\nI have updated my review to reflect my evaluation of the revised paper, although I am also leaving the original review in place to preserve the history of the paper.\n\nThis paper has three main contributions. (1) It proposes an approach to training sparse RNNs in which weights falling below a given threshold are masked to zero, and a schedule is used for the threshold in which pruning is only applied after a certain number of iterations have been performed and the threshold increases over the course of training. (2) It provides experimental results on a Baidu-internal task with the Deep Speech 2 network architecture showing that applying the sparsification to a large model can lead to a final, trained model which has better performance and fewer non-zero parameters than a dense baseline model. (3) It provides results from timing experiments with the cuSPARSE library showing that there is some potential for faster model evaluation with sufficiently sparse models, but that the current cuSPARSE implementation may not be optimal.\n\nPros\n+ The paper is mostly clear and easy to understand.\n+ The paper tackles an important, practical problem in deep learning: how to successfully deploy models at the lowest possible computational and memory cost.\n\nCons\n- As a second baseline, this paper should compare to \"distillation\" approaches (e.g.,\n\n---\n\n13 Dec 2016 (modified: 18 Jan 2017)\n\n---\n\nWhen weights are added again, which value do they initially have?\n\n---\n\nICLR 2017 conference AnonReviewer3\n\n---\n\n01 Dec 2016\n\n---\n\nWhy not use a teacher-student baseline?\n\n---\n\nICLR 2017 conference AnonReviewer1\n\n---\n\n29 Nov 2016\n\n---\n\nTraining time\n\n---\n\nICLR 2017 conference AnonReviewer4\n\n---\n\nIn introduction: \"... unlike previous approaches\nsuch as in Han et al. (2015). State of the art results in speech recognition generally require between\ndays and weeks of training time, so a further 3-4× increase in training time is undesirable.\"\n\nBut, according to Han et al. (2015), \"Huffman coding doesn’t require training and is implemented\noffline after all the fine-tuning is finished.\"\n\nBoth yours and Han et al. (2015) use a weight pruning technique. Intuitively, they should have similar training time for LSTM models.\nWhere does 3-4x extra training time comes from Han et al. (2015) but doesn't have in your approach?\n\n---\n\n29 Nov 2016\n\n---\n\nHave tried on LSTM?\n\n---\n\nICLR 2017 conference AnonReviewer4\n\n---\n\n29 Nov 2016\n\n---\n\nICLR Paper Format\n\n---\n\nTara N Sainath\n\n---\n\nDear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!\n\n---\n\n07 Nov 2016\n\n---\n\nSharan Narang, Greg Diamos, Shubho Sengupta, Erich Elsen\n\n---\n\nReduce parameter count in recurrent neural networks to create smaller models for faster deployment"}
{"paper_id": "319", "paper_text": "ABSTRACT: Attention plays a critical role in human visual experience. Furthermore, it has recently been demonstrated that attention can also play an important role in the context of applying artificial neural networks to a variety of tasks from fields such as computer vision and NLP. In this work we show that, by properly defining attention for convolutional neural networks, we can actually use this type of information in order to significantly improve the performance of a student CNN network by forcing it to mimic the attention maps of a powerful teacher network. To that end, we propose several novel methods of transferring attention, showing consistent improvement across a variety of datasets and convolutional neural network architectures. Code and models for our experiments are available at https://github.com/szagoruyko/attention-transfer.\n\n1 INTRODUCTION\nAs humans, we need to pay attention in order to be able to adequately perceive our surroundings. Attention is therefore a key aspect of our visual experience, and closely relates to perception - we need to keep attention to build a visual representation, possessing detail and coherence.\nAs artificial neural networks became more popular in fields such as computer vision and natural language processing in the recent years, artificial attention mechanisms started to be developed as well. Artificial attention lets a system “attend” to an object to examine it with greater detail. It has also become a research tool for understanding mechanisms behind neural networks, similar to attention used in psychology.\nOne of the popular hypothesis there is that there are non-attentional and attentional perception processes. Non-attentional processes help to observe a scene in general and gather high-level information, which, when associated with other thinking processes, helps us to control the attention processes and navigate to a certain part of the scene. This implies that different observers with different knowledge, different goals, and therefore different attentional strategies can literally see the same scene differently. This brings us to the main topic of this paper: how attention differs within artificial vision systems, and can we use attention information in order to improve the performance of convolutional neural networks ? More specifically, can a teacher network improve the performance of another student network by providing to it information about where it looks, i.e., about where it concentrates its attention into ?\nTo study these questions, one first needs to properly specify how attention is defined w.r.t. a given convolutional neural network. To that end, here we consider attention as a set of spatial maps that essentially try to encode on which spatial areas of the input the network focuses most for taking its output decision (e.g., for classifying an image), where, furthermore, these maps can be defined w.r.t. various layers of the network so that they are able to capture both low-, mid-, and high-level representation information. More specifically, in this work we define two types of spatial attention maps: activation-based and gradient-based. We explore how both of these attention maps change over various datasets and architectures, and show that these actually contain valuable information\nthat can be used for significantly improving the performance of convolutional neural network architectures (of various types and trained for various different tasks). To that end, we propose several novel ways of transferring attention from a powerful teacher network to a smaller student network with the goal of improving the performance of the latter (Fig. 1).\nTo summarize, the contributions of this work are as follows:\n• We propose attention as a mechanism of transferring knowledge from one network to another\n• We propose the use of both activation-based and gradient-based spatial attention maps • We show experimentally that our approach provides significant improvements across a va-\nriety of datasets and deep network architectures, including both residual and non-residual networks\n• We show that activation-based attention transfer gives better improvements than fullactivation transfer, and can be combined with knowledge distillation\nThe rest of the paper is structured as follows: we first describe related work in section 2, we explain our approach for activation-based and gradient-based attention transfer in section 3, and then present experimental results for both methods in section 4. We conclude the paper in section 5.\n\n2 RELATED WORK\nEarly work on attention based tracking Larochelle & Hinton (2010), Denil et al. (2012) was motivated by human attention mechanism theories Rensink (2000) and was done via Restricted Bolzmann Machines. It was recently adapted for neural machine translation with recurrent neural networks, e.g. Bahdanau et al. (2014) as well as in several other NLP-related tasks. It was also exploited in computer-vision-related tasks such as image captioning Xu et al. (2015), visual question answering Yang et al. (2015), as well as in weakly-supervised object localization Oquab et al. (2015) and classification Mnih et al. (2014), to mention a few characteristic examples. In all these tasks attention proved to be useful.\nVisualizing attention maps in deep convolutional neural networks is an open problem. The simplest gradient-based way of doing that is by computing a Jacobian of network output w.r.t. input (this leads to attention visualization that are not necessarily class-discriminative), as for example in Simonyan et al. (2014). Another approach was proposed by Zeiler & Fergus (2014) that consists of attaching a network called “deconvnet” that shares weights with the original network and is used to project certain features onto the image plane. A number of methods was proposed to improve gradientbased attention as well, for example guided backpropagation Springenberg et al. (2015), adding a change in ReLU layers during calculation of gradient w.r.t. previous layer output. Attention maps obtained with guided backpropagation are non-class-discriminative too. Among existing methods\nfor visualizing attention, we should also mention class activation maps Zhou et al. (2016), which are based on removing top average-pooling layer and converting the linear classification layer into a convolutional layer, producing attention maps per each class. A method combining both guided backpropagation and CAM is Grad-CAM by Selvaraju et al. (2016), adding image-level details to class-discriminative attention maps.\nKnowledge distillation with neural networks was pioneered by Hinton et al. (2015); Bucila et al. (2006), which is a transfer learning method that aims to improve the training of a student network by relying on knowledge borrowed from a powerful teacher network. Although in certain special cases shallow networks had been shown to be able to approximate deeper ones without loss in accuracy Lei & Caruana (2014), later work related to knowledge distillation was mostly based on the assumption that deeper networks always learn better representations. For example, FitNets Romero et al. (2014) tried to learn a thin deep network using a shallow one with more parameters. The introduction of highway Srivastava et al. (2015) and later residual networks He et al. (2015) allowed training very deep architectures with higher accuracy, and generality of these networks was experimentally showed over a large variety of datasets. Although the main motivation for residual networks was increasing depth, it was later shown by Zagoruyko & Komodakis (2016) that, after a certain depth, the improvements came mostly from increased capacity of the networks, i.e. number of parameters (for instance, a wider deep residual network with only 16 layers was shown that it could learn as good or better representations as very thin 1000 layer one, provided that they were using comparable number of parameters).\nDue to the above fact and due to that thin deep networks are less parallelizable than wider ones, we think that knowledge transfer needs to be revisited, and take an opposite to FitNets approach - we try to learn less deep student networks. Our attention maps used for transfer are similar to both gradient-based and activation-based maps mentioned above, which play a role similar to “hints” in FitNets, although we don’t introduce new weights.\n\n3 ATTENTION TRANSFER\nIn this section we explain the two methods that we use for defining the spatial attention maps of a convolutional neural network as well as how we transfer attention information from a teacher to a student network in each case.\n\n3.1 ACTIVATION-BASED ATTENTION TRANSFER\nLet us consider a CNN layer and its corresponding activation tensor A ∈ RC×H×W , which consists of C feature planes with spatial dimensionsH×W . An activation-based mapping function F (w.r.t. that layer) takes as input the above 3D tensor A and outputs a spatial attention map, i.e., a flattened 2D tensor defined over the spatial dimensions, or\nF : RC×H×W → RH×W . (1)\nTo define such a spatial attention mapping function, the implicit assumption that we make in this section is that the absolute value of a hidden neuron activation (that results when the network is evaluated on given input) can be used as an indication about the importance of that neuron w.r.t. the specific input. By considering, therefore, the absolute values of the elements of tensor A, we can construct a spatial attention map by computing statistics of these values across the channel dimension (see Fig. 3). More specifically, in this work we will consider the following activation-based spatial attention maps:\n• sum of absolute values: Fsum(A) = ∑C i=1 |Ai|\n• sum of absolute values raised to the power of p (where p > 1): F psum(A) = ∑C i=1 |Ai|p\n• max of absolute values raised to the power of p (where p > 1): F pmax(A) = maxi=1,C |Ai|p\nwhere Ai = A(i, :, :) (using Matlab notation), and max, power and absolute value operations are elementwise (e.g. |Ai|p is equivalent to abs(Ai).∧p in Matlab notation).\nWe visualized activations of various networks on several datasets, including ImageNet classification and localization, COCO object detection, face recognition, and fine-grained recognition. We were mostly focused on modern architectures without top dense linear layers, such as Network-InNetwork, ResNet and Inception, which have streamlined convolutional structure. We also examined networks of the same architecture, width and depth, but trained with different frameworks with significant difference in performance. We found that the above statistics of hidden activations not only have spatial correlation with predicted objects on image level, but these correlations also tend to be higher in networks with higher accuracy, and stronger networks have peaks in attention where weak networks don’t (e.g., see Fig. 4). Furthermore, attention maps focus on different parts for different layers in the network. In the first layers neurons activation level is high for low-level gradient points, in the middle it is higher for the most discriminative regions such as eyes or wheels, and in the top layers it reflects full objects. For example, mid-level attention maps of a network trained for face recognition Parkhi et al. (2015) will have higher activations around eyes, nose and lips, and top level activation will correspond to full face (Fig. 2).\nConcerning the different attention mapping functions defined above, these can have slightly different properties. E.g.:\n• Compared to Fsum(A), the spatial map F psum(A) (where p > 1) puts more weight to spatial locations that correspond to the neurons with the highest activations, i.e., puts more weight to the most discriminative parts (the larger the p the more focus is placed on those parts with highest activations).\n• Furthermore, among all neuron activations corresponding to the same spatial location, F pmax(A) will consider only one of them to assign a weight to that spatial location (as opposed to F psum(A) that will favor spatial locations that carry multiple neurons with high activations).\nTo further illustrate the differences of these functions we visualized attention maps of 3 networks with sufficient difference in classification performance: Network-In-Network (62% top-1 val accuracy), ResNet34 (73% top-1 val accuracy) and ResNet-101 (77.3% top-1 val accuracy). In each network we took last pre-downsampling activation maps, on the left for mid-level and on the right for top pre-average pooling activations in fig. 4. Top-level maps are blurry because their original spatial resolution is 7 × 7. It is clear that most discriminative regions have higher activation levels, e.g. face of the wolf, and that shape details disappear as the parameter p (used as exponent) increases.\nIn attention transfer, given the spatial attention maps of a teacher network (computed using any of the above attention mapping functions), the goal is to train a student network that will not only make correct predictions but will also have attentions maps that are similar to those of the teacher. In general, one can place transfer losses w.r.t. attention maps computed across several layers. For instance, in the case of ResNet architectures, one can consider the following two cases, depending on the depth of teacher and student:\n• Same depth: possible to have attention transfer layer after every residual block\n• Different depth: have attention transfer on output activations of each group of residual blocks\nSimilar cases apply also to other architectures (such as NIN, in which case a group refers to a block of a 3 × 3, 1 × 1, 1 × 1 convolutions). In fig. 5 we provide a schematic illustration of the different depth case for residual network architectures.\nWithout loss of generality, we assume that transfer losses are placed between student and teacher attention maps of same spatial resolution, but, if needed, attention maps can be interpolated to match their shapes. Let S, T and WS , WT denote student, teacher and their weights correspondingly, and let L(W, x) denote a standard cross entropy loss. Let also I denote the indices of all teacher-student activation layer pairs for which we want to transfer attention maps. Then we can define the following total loss:\nLAT = L(WS , x) + β\n2 ∑ j∈I ‖ QjS ‖QjS‖2 − QjT ‖QjT ‖2 ‖p , (2)\nwhere QjS = vec(F (A j S)) and Q j T = vec(F (A j T )) are respectively the j-th pair of student and teacher attention maps in vectorized form, and p refers to norm type (in the experiments we use p = 2). As can be seen, during attention transfer we make use of l2-normalized attention maps, i.e., we replace each vectorized attention map Q with Q‖Q‖2 (l1 normalization could be used as well). It\nis worth emphasizing that normalization of attention maps is important for the success of the student training.\nAttention transfer can also be combined with knowledge distillation Hinton et al. (2015), in which case an additional term (corresponding to the cross entropy between softened distributions over labels of teacher and student) simply needs to be included to the above loss. When combined, attention transfer adds very little computational cost, as attention maps for teacher can be easily computed during forward propagation, needed for distillation.\n\n3.2 GRADIENT-BASED ATTENTION TRANSFER\nIn this case we define attention as gradient w.r.t. input, which can be viewed as an input sensitivity map Simonyan et al. (2014), i.e., attention at an input spatial location encodes how sensitive the output prediction is w.r.t. changes at that input location (e.g., if small changes at a pixel can have a large effect on the network output then it is logical to assume that the network is “paying attention” to that pixel). Let’s define the gradient of the loss w.r.t input for teacher and student as:\nJS = ∂\n∂x L(WS, x), JT =\n∂\n∂x L(WT, x) (3)\nThen if we want student gradient attention to be similar to teacher attention, we can minimize a distance between them (here we use l2 distance but other distances can be employed as well):\nLAT (WS,WT, x) = L(WS, x) + β\n2 ||JS − JT ||2 (4)\nAs WT and x are given, to get the needed derivative w.r.t. WS :\n∂\n∂WS LAT =\n∂\n∂WS L(WS, x) + β(JS − JT )\n∂2\n∂WS∂x L(WS, x) (5)\nSo to do an update we first need to do forward and back propagation to get JS and JT , compute the second error β2 ||JS − JT ||2 and propagate it second time. The second propagation is similar to forward propagation in this case, and involves second order mixed partial derivative calculation ∂ 2\n∂WS∂x .\nThe above computation is similar to the double backpropagation technique developed by Drucker & LeCun (1992) (where the l2 norm of the gradient w.r.t. input is used as regularizer). Furthermore, it can be implemented efficiently in a framework with automatic differentiation support, even for modern architectures with sophisticated graphs. The second backpropagation has approximately the same cost with first backpropagation, excluding forward propagation.\nWe also propose to enforce horizontal flip invariance on gradient attention maps. To do that we propagate horizontally flipped images as well as originals, backpropagate and flip gradient attention maps back. We then add l2 losses on the obtained attentions and outputs, and do second backpropagation:\nLsym(W, x) = L(W, x) + β 2 || ∂ ∂x L(W, x)− flip( ∂ ∂x L(W,flip(x)))||2 , (6)\nwhere flip(x) denotes the flip operator. This is similar to Group Equivariant CNN approach by Cohen & Welling (2016), however it is not a hard constraint. We experimentally find that this has a regularization effect on training.\nWe should note that in this work we consider only gradients w.r.t. the input layer, but in general one might have the proposed attention transfer and symmetry constraints w.r.t. higher layers of the network.\n\n4 EXPERIMENTAL SECTION\nIn the following section we explore attention transfer on various image classification datasets. We split the section in two parts, in the first we include activation-based attention transfer and gradientbased attention transfer experiments on CIFAR, and in the second activation-based attention trans-\nfer experiments on larger datasets. For activation-based attention transfer we used Network-InNetwork Lin et al. (2013) and ResNet-based architectures (including the recently introduced Wide Residual Networks (WRN) Zagoruyko & Komodakis (2016)), as they are most performant and set strong baselines in terms of number of parameters compared to AlexNet or VGG, and have been explored in various papers across small and large datasets. On Scenes, CUB and ImageNet we experimented with ResNet-18 and ResNet-34. As for gradient-based attention, we constrained ourselves to Network-In-Network without batch normalization and CIFAR dataset, due to the need of complex automatic differentiation.\n\n4.1 CIFAR EXPERIMENTS\nWe start with CIFAR dataset which has small 32 × 32 images, and after downsampling top activations have even smaller resolution, so there is not much space for attention transfer. Interestingly, even under this adversarial setting, we find that attention transfer seems to give reasonable benefits, offering in all cases consistent improvements. We use horizontal flips and random crops data augmentations, and all networks have batch normalization. We find that ZCA whitening has negative effect on validation accuracy, and omit it in favor of simpler meanstd normalization. We raise Knowledge Distillation (KD) temperature for ResNet transfers to 4, and use α = 0.9 (see Hinton et al. (2015) for an explanation of these parameters).\n\n4.1.1 ACTIVATION-BASED ATTENTION TRANSFER\nResults of attention transfer (using F 2sum attention maps) for various networks on CIFAR-10 can be found in table 1. We experimented with teacher/student having the same depth (WRN-16-2/WRN16-1), as well as different depth (WRN-40-1/WRN-16-1, WRN-40-2/WRN-16-2). In all combinations, attention transfer (AT) shows significant improvements, which are also higher when it is combined with knowledge distillation (AT+KD).\nTo verify if having at least one activation-based attention transfer loss per group in WRN transfer is important, we trained three networks with only one transfer loss per network in group1, group2 and group3 separately, and compared to a network trained with all three losses. The corresponding results were 8.11, 7.96, 7.97 (for the separate losses) and 7.93 for the combined loss (using WRN16-2/WRN-16-1 as teacher/student pair). Each loss provides some additional degree of attention transfer.\nWe also explore which attention mapping functions tend to work best using WRN-16-1 and WRN16-2 as student and teacher networks respectively (table 2). Interestingly, sum-based functions work very similar, and better than max-based ones. From now on, we will use sum of squared attention mapping function F 2sum for simplicity. As for parameter β in eq. 2, it usually varies about 0.1, as we set it to 103 divided by number of elements in attention map and batch size for each layer. In case of combinining AT with KD we decay it during traning in order to simplify learning harder examples.\n\n4.1.2 ACTIVATION-BASED AT VS. TRANSFERRING FULL ACTIVATION\nTo check if transferring information from full activation tensors is more beneficial than from attention maps, we experimented with FitNets-style hints using l2 losses on full activations directly, with 1 × 1 convolutional layers to match tensor shapes, and found that improvements over baseline student were minimal (see column F-ActT in table 1). For networks of the same width different depth we tried to regress directly to activations, without 1× 1 convolutions. We also use l2 normalization before transfer losses, and decay β in eq. 2 during training as these give better performance. We find that AT, as well as full-activation transfer, greatly speeds up convergence, but AT gives much\nbetter final accuracy improvement than full-activation transfer (see fig. 7(b), Appendix). It seems quite interesting that attention maps carry information that is more important for transfer than full activations.\nattention mapping function error no attention transfer 8.77 Fsum 7.99 F 2sum 7.93 F 4sum 8.09 F 1max 8.08\nTable 2: Test error of WRN16-2/WRN-16-1 teacher/student pair for various attention mapping functions. Median of 5 runs test errors are reported.\nnorm type error baseline (no attention transfer) 13.5 min-l2 Drucker & LeCun (1992) 12.5 grad-based AT 12.1 KD 12.1 symmetry norm 11.8 activation-based AT 11.2\nTable 3: Performance of various gradient-based attention methods on CIFAR-10. Baseline is a thin NIN network with 0.2M parameters (trained only on horizontally flipped augmented data and without batch normalization), min-l2 refers to using l2 norm of gradient w.r.t. input as regularizer, symmetry norm - to using flip invariance on gradient attention maps (see eq. 6), AT - to attention transfer, and KD - to Knowledge Distillation (both AT and KD use a wide NIN of 1M parameters as teacher).\n\n4.1.3 GRADIENT-BASED ATTENTION TRANSFER\nFor simplicity we use thin Network-In-Network model in these experiments, and don’t apply random crop data augmentation with batch normalization, just horizontal flips augmentation. We also only use deterministic algorithms and sampling with fixed seed, so reported numbers are for single run experiments. We find that in this setting network struggles to fit into training data already, and turn off weight decay even for baseline experiments. In future we plan to explore gradient-based attention for teacher-student pairs that make use of batch normalization, because it is so far unclear how batch normalization should behave in the second backpropagation step required during gradientbased attention transfer (e.g., should it contribute to batch normalization parameters, or is a separate forward propagation with fixed parameters needed).\nWe explored the following methods:\n• Minimizing l2 norm of gradient w.r.t. input, i.e. the double backpropagation method Drucker & LeCun (1992);\n• Symmetry norm on gradient attention maps (see eq. 6);\n• Student-teacher gradient-based attention transfer;\n• Student-teacher activation-based attention transfer.\nResults for various methods are shown in table 3. Interestingly, just minimizing l2 norm of gradient already works pretty well. Also, symmetry norm is one the best performing attention norms, which we plan to investigate in future on other datasets as well. We also observe that, similar to activationbased attention transfer, using gradient-based attention transfer leads to improved performance. We also trained a network with activation-based AT in the same training conditions, which resulted in the best performance among all methods. We should note that the architecture of student NIN without batch normalization is slightly different from teacher network, it doesn’t have ReLU activations before pooling layers, which leads to better performance without batch normalization, and worse with. So to achieve the best performance with activation-based AT we had to train a new teacher, with batch normalization and without ReLU activations before pooling layers, and have AT losses on outputs of convolutional layers.\n\n4.2 LARGE INPUT IMAGE NETWORKS\nIn this section we experiment with hidden activation attention transfer on ImageNet networks which have 224 × 224 input image size. Presumably, attention matters more in this kind of networks as spatial resolution of attention maps is higher.\n\n4.2.1 TRANSFER LEARNING\nTo see how attention transfer works in finetuning we choose two datasets: Caltech-UCSD Birds-2002011 fine-grained classification (CUB) by Wah et al. (2011), and MIT indoor scene classification (Scenes) by Quattoni & Torralba (2009), both containing around 5K images training images. We took ResNet-18 and ResNet-34 pretrained on ImageNet and finetuned on both datasets. On CUB we crop bounding boxes, rescale to 256 in one dimension and then take a random crop. Batch normalization layers are fixed for finetuning, and first group of residual blocks is frozen. We then took finetuned ResNet-34 networks and used them as teachers for ResNet-18 pretrained on ImageNet, with F 2sum attention losses on 2 last groups. In both cases attention transfer provides significant improvements, closing the gap between ResNet-18 and ResNet-34 in accuracy. On Scenes AT works as well as KD, and on CUB AT works much better, which we speculate is due to importance of intermediate attention for fine-grained recognition. Moreover, after finetuning, student’s attention maps indeed look more similar to teacher’s (Fig. 6, Appendix).\n\n4.2.2 IMAGENET\nTo showcase activation-based attention transfer on ImageNet we took ResNet-18 as a student, and ResNet-34 as a teacher, and tried to improve ResNet-18 accuracy. We added only two losses in the 2 last groups of residual blocks and used squared sum attention F 2sum. We also did not have time to tune any hyperparameters and kept them from finetuning experiments. Nevertheless, ResNet-18 with attention transfer achieved 1.1% top-1 and 0.8% top-5 better validation accuracy (Table. 5 and\nFig. 7(a), Appendix), we plan to update the paper with losses on all 4 groups of residual blocks.\nWe were not able to achieve positive results with KD on ImageNet. With ResNet-18-ResNet-34 student-teacher pair it actually hurts convergence with the same hyperparameters as on CIFAR. As it was reported that KD struggles to work if teacher and student have different architecture/depth (we observe the same on CIFAR), so we tried using the same architecture and depth for attention transfer. On CIFAR both AT and KD work well in this case and improve convergence and final accuracy, on ImageNet though KD converges significantly slower (we did not train until the end due to lack of computational resources). We also could not find applications of FitNets, KD or similar methods on ImageNet in the literature. Given that, we can assume that proposed activation-based AT is the first knowledge transfer method to be successfully applied on ImageNet.\n\n5 CONCLUSIONS\nWe presented several ways of transferring attention from one network to another, with experimental results over several image recognition datasets. It would be interesting to see how attention transfer works in cases where spatial information is more important, e.g. object detection or weaklysupervised localization, which is something that we plan to explore in the future.\nOverall, we think that our interesting findings will help further advance knowledge distillation, and understanding convolutional neural networks in general.\n\nA APPENDIX\nA.1 FIGURES AND TABLES\nvideostore\n0 1 2 3 4 5 6 7\n0\n2\n4\n6\n8\n10\nbookshop, bookstore, bookstall\n0 1 2 3 4 5 6 7\n0\n2\n4\n6\n8\n10\nvideostore\n0 1 2 3 4 5 6 7\n0\n2\n4\n6\n8\n10\nvideostore\n0 1 2 3 4 5 6 7\n0\n2\n4\n6\n8\n10\nvideostore\nhospitalroom\n0 2 4 6 8 10\n0 1 2 3 4 5 6 7\ndesk\n0 2 4 6 8 10\n0 1 2 3 4 5 6 7\nhospitalroom\n0 2 4 6 8 10\n0 1 2 3 4 5 6 7\nhospitalroom\n0 2 4 6 8 10\n0 1 2 3 4 5 6 7\nhospitalroom\ndentaloffice\n0 1 2 3 4 5 6 7 ResNet-18-ImageNet\n0\n2\n4\n6\n8\n10\ndesk\n0 1 2 3 4 5 6 7 ResNet-18-scenes\n0\n2\n4\n6\n8\n10\ndentaloffice\n0 1 2 3 4 5 6 7 ResNet-18-scenes-AT\n0\n2\n4\n6\n8\n10\ndentaloffice\n0 1 2 3 4 5 6 7 ResNet-34-scenes\n0\n2\n4\n6\n8\n10\ndentaloffice\nA.2 IMPLEMENTATION DETAILS\nThe experiments were conducted in Torch machine learning framework. Double propagation can be implemented in a modern framework with automatic differentiation support, e.g. Torch, Theano, Tensorflow. For ImageNet experiments we used fb.resnet.torch code, and used 2 Titan X cards with data parallelizm in both teacher and student to speed up training. Code and models for our experiments are available at https://github.com/szagoruyko/attention-transfer.", "ground_truth": "ICLR 2017 conference submission\n\n---\n\nPaying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer\n\n---\n\nAttention plays a critical role in human visual experience. Furthermore, it has recently been demonstrated that attention can also play an important role in the context of applying artificial neural networks to a variety of tasks from fields such as computer vision and NLP. In this work we show that, by properly defining attention for convolutional neural networks, we can actually use this type of information in order to significantly improve the performance of a student CNN network by forcing it to mimic the attention maps of a powerful teacher network. To that end, we propose several novel methods of transferring attention, showing consistent improvement across a variety of datasets and convolutional neural network architectures.\n\n---\n\nICLR committee final decision\n\n---\n\nICLR 2017 pcs\n\n---\n\nImportant task (attention models), interesting distillation application, well-written paper. The authors have been responsive in updating the paper, adding new experiments, and being balanced in presenting their findings. I support accepting this paper.\n\n---\n\n06 Feb 2017\n\n---\n\nAnswer to reviewers\n\n---\n\nSergey Zagoruyko\n\n---\n\nWe would like to thank the reviewers for their comments and for providing us with valuable feedback. We already updated the paper with new results on ImageNet, and are also going to further update the paper based on the reviewer comments as explained in our responses below.\nWe also plan to release the code for our experiments next week.\n\n---\n\n29 Dec 2016\n\n---\n\nICLR 2017 conference AnonReviewer3\n\n---\n\nofficial review\n\n---\n\nThe paper proposes a new way of transferring knowledge.\nI like the idea of transferring attention maps instead of activations.\nHowever, the experiments don’t show a big improvement compared with knowledge distillation alone and I think more experiments are required in IMAGENET section.\nI would consider updating the score if the authors extend the last section 4.2.2.\n\n---\n\n21 Dec 2016\n\n---\n\nICLR 2017 conference AnonReviewer2\n\n---\n\nReview\n\n---\n\nThe paper presented a modified knowledge distillation framework that minimizes the difference of the sum of statistics across the a feature map between the teacher and the student network. The authors empirically demonstrated the proposed methods outperform the fitnet style distillation baseline. \n\nPros:\n+ The author evaluated the proposed methods on various computer vision dataset \n+ The paper is in general well-written\n\nCons: \n- The method seems to be limited to the convolutional architecture\n- The attention terminology is misleading in the paper. The proposed method really just try to distill the summed squared(or other statistics e.g. summed lp norm) of activations in a hidden feature map.\n- The gradient-based attention transfer seems out-of-place. The proposed gradient-based methods are never compared directly to nor are used jointly with the \"attention-based\" transfer. It seems like a parallel idea added to the paper that does not seem to add much value.\n- It is also not clear how the induced 2-norms in eq.(2) is computed. Q is a matrix \\in \\mathbb{R}^{H \\times W} whose induced 2-norm is its largest singular value. It seems computationally expensive to compute such cost function. Is it possible the authors really mean the Frobenius norm?\n\nOverall, the proposed distillation method works well in practice but the paper has some organization issues and unclear notation.\n\n---\n\n20 Dec 2016 (modified: 24 Jan 2017)\n\n---\n\nICLR 2017 conference AnonReviewer1\n\n---\n\nSome nice results, but it is not clear what are the advantages/drawbacks of the different attention maps\n\n---\n\nThis paper proposes to investigate attention transfers between a teacher and a student network. \n\nAttention transfer is performed by minimising the l2 distance between the teacher/student attention maps at different layers, in addition to minimising the classification loss and optionally a knowledge distillation term.\nAuthors define several activation based attentions (sum of absolute feature values raise at the power p or max of values raised at the power p). They also propose a gradient based attention (derivative of the Loss w.r.t. inputs). \n\nThey evaluate their approaches on several datasets (CIFAR, Cub/Scene, Imagenet) showing that attention transfers does help improving the student network test performance. However, the student networks performs worst than the teacher, even with attention.\n\nFew remarks/questions:\n- in section 3 authors claim that networks with higher accuracy have a higher spatial correlation between the object and the attention map. While Figure 4 is compelling, it would be nice to have quantitative results showing that as well.\n- how did you choose the hyperparameter values, it would be nice to see what is the impact of $\\beta$.\n- it would be nice to report teacher train and validation loss in Figure 7 b)\n- from the experiments, it is not clear what at the pros/cons of the different attention maps\n- AT does not lead to better result than the teacher. However, the student networks have less parameters. It would be interesting to characterise the corresponding speed-up. If you keep the same architecture between the student and the teacher, is there any benefit to the attention transfer?\n\nIn summary:\nPros:\n- Clearly written and well motivated.\n- Consistent improvement of the student with attention compared to the student alone.\nCons:\n- Students have worst performances than the teacher models.\n- It is not clear which attention to use in which case?\n- Somewhat incremental novelty relatively to Fitnet\n\n---\n\n16 Dec 2016\n\n---\n\nthe definition of activation tensor\n\n---\n\nZehao Huang\n\n---\n\nHi, Sergey.\n\nI am confused about the definition of activation tensor in section 3.1.\n\nIs it obtained before or after ReLU activation function?\n\nIf it's got after ReLu, there is no need adding absolute function.\n\nThanks!\n\n---\n\n12 Dec 2016\n\n---\n\nNotation question\n\n---\n\nICLR 2017 conference AnonReviewer2\n\n---\n\n03 Dec 2016\n\n---\n\nPre review questions\n\n---\n\nICLR 2017 conference AnonReviewer1\n\n---\n\n01 Dec 2016\n\n---\n\nSergey Zagoruyko, Nikos Komodakis"}
{"paper_id": "503", "paper_text": "ABSTRACT: This paper presents a novel model for multimodal learning based on gated neural networks. The Gated Multimodal Unit (GMU) model is intended to be used as an internal unit in a neural network architecture whose purpose is to find an intermediate representation based on a combination of data from different modalities. The GMU learns to decide how modalities influence the activation of the unit using multiplicative gates. It was evaluated on a multilabel scenario for genre classification of movies using the plot and the poster. The GMU improved the macro f-score performance of single-modality approaches and outperformed other fusion strategies, including mixture of experts models. Along with this work, the MM-IMDb dataset is released which, to the best of our knowledge, is the largest publicly available multimodal dataset for genre prediction on movies.\n\n1 INTRODUCTION\nRepresentation learning methods have received a lot of attention by researchers and practitioners because of its successful application to complex problems in areas such as computer vision, speech recognition and text processing (LeCun et al., 2015). Most of these efforts have concentrated on data involving one type of information (images, text, speech, etc.), despite data being naturally multimodal. Multimodality refers to the fact that the same real-world concept can be described by different views or data types. Collaborative encyclopedias (such as Wikipedia) describe a famous person through a mixture of text, images and, in some cases, audio. Users from social networks comment events like concerts or sport games with small phrases and multimedia attachments (images/videos/audios). Medical records are represented by a collection of images, sound, text and signals, among others. The increasing availability of multimodal databases from different sources has motivated the development of automatic analysis techniques to exploit the potential of these data as a source of knowledge in the form of patterns and structures that reveal complex relationships (Bhatt & Kankanhalli, 2011; Atrey et al., 2010). In recent years, multimodal tasks have acquired attention by the representation learning community. Strategies for visual question answering (Antol et al., 2015), or image captioning (Vinyals et al., 2015; Xu et al., 2015; Johnson et al., 2015) have developed interesting ways of combining different representation learning architectures.\nMost of these models are focused on mapping from one modality to another or solving an auxiliary task to create a common representation with the information of all modalities. In this work, we design a novel module that combines multiple sources of information, which is optimized with respect to the end goal objective function. Our proposed module is based on the idea of gates for selecting which parts of the input are more likely to contribute for correctly generating the desired output. We\nuse multiplicative gates that assign importance to various features simultaneously, creating a rich multimodal representation that does not require manual tuning, but instead it learns directly from the training data. Our gated model can be reused in different network architectures for solving different tasks, and can be optimized end-to-end with other modules in the architecture using standard gradient-based optimization algorithms.\nAs an application use case, we explore the task of identifying a movie genre based on its plot and its poster. Genre classification has several application areas like document categorization (Kanaris & Stamatatos, 2009), recommendation systems (Makita & Lenskiy, 2016a), and information retrieval systems, among others. Figure 1 depicts the challenging task of assigning genres to a particular movie based solely on the usage of one modality. Such predictions were done with MaxoutMLP w2v and VGG transfer approaches (See Section 3), both of them are models based on representation learning. It can be seen that even a human might be confused if both modalities are not available. The main hypothesis of this work is that a model using gating units, in contrast to a hand-coded multimodal fusion architecture, will be able to learn an input-dependent gate-activation pattern that determines how each modality contribute to the output of hidden units.\nThe rest of the paper is organized as follows: Section 2 presents a literature review and some considerations of the previous work. Section 3 describes the methods used as baseline as well as our representation-leaning-based model proposed. Section 4 presents the experimental evaluation setup along with the details of the MM-IMDb dataset. Section 5 shows and discusses the results for movie genre classification. Finally, Section 6 draws the conclusions and future work.\n\n2.1 MULTIMODAL FUSION\nDifferent reviews (Atrey et al., 2010; Bhatt & Kankanhalli, 2011; Li Deng, 2014; Deng, 2014) have summarized strategies that addressed multimodal analysis. Most of the collected works claimed the superiority of multimodal over unimodal approaches for automatic analysis tasks. A conventional multimodal analysis system receives as input two or more modalities that describe a particular concept. The most common multimodal sources are video, audio, images and text. In recent years there has been a consensus with respect to the use of representation learning models to characterize the information of this kind of sources (LeCun et al., 2015). However, the way that such extracted features are combined is still in exploration.\nMultimodal combination seeks to generate a single representation that makes easier automatic analysis tasks when building classifiers or other predictors. A simple approach is to concatenate features to get a final representation (Kiela & Bottou, 2014; Pei et al., 2013; Suk & Shen, 2013). Although it is a straightforward strategy, it ignores inherent correlations between different modalities.\nMore complex fusion strategies include Restricted Boltzmann Machines (RBMs) and autoencoders. Ngiam et al. (2011) concatenated higher level representations and train two RBMs to reconstruct the original audio and video representations respectively. Additionally, they trained a model to recon-\nstruct both modalities given only one of them as input. In an interesting result, Ngiam et al. (2011) were able to mimic a perceptual phenomenon that demonstrates an interaction between hearing and vision in speech perception known as McGurk effect. A similar approach was proposed by Srivastava & Salakhutdinov (2012). They modified feature learning and reconstruction phases with Deep Boltzmann Machines. Authors claimed that such strategy is able to exploit large amounts of unlabeled data by improving the performance in retrieval and annotation tasks. Other similar strategies propose to fusion modalities using neural network architectures (Andrew et al., 2013; Feng et al., 2013; Kang et al., 2012; Kiros et al., 2014a; Lu et al., 2014; Mao et al., 2014; Tu et al., 2014; Wu et al., 2013) with two input layers separately and including a final supervised layer such as softmax regression classifier.\nAn alternative approach involves an objective or loss function suited for the target task (Akata et al., 2014; Frome et al., 2013; Kiros et al., 2014b; Mao et al., 2014; Socher et al., 2013; 2014; Zheng et al., 2014). These strategies usually assume that there exists a common latent space where modalities can express the same semantic concept through a set of transformations of the raw data. The semantic embedding representations are such that two concepts are similar if and only if their semantic embeddings are close (Norouzi et al., 2014). In (Socher et al., 2013) a multimodal strategy to perform zero-shot classification was proposed. They trained a word-based neural network model (Huang et al., 2012) to represent textual information, whilst use unsupervised feature learning models proposed in (Coates & Ng, 2011) to get image representation. The fusion was done by learning an image linear mapping to project images into the semantic word space learned in the neural network model. Additionally a Bayesian framework was included to decide whether an image is of a seen or unseen class. Frome et al. (2013) learn the image representation using a CNN trained with the Imagenet dataset and a word-based neural language model (Mikolov et al., 2013b) to represent the textual modality. To perform the fusion they re-train CNN using text representation as targets. This work outperforms scalability with respect to (Socher et al., 2013) from 2 to 20,000 unknown classes in the zero-shot learning task. A modified strategy of Frome et al. (2013) was presented by Norouzi et al. (2014). Instead of re-train the CNN network, they built a convex combination with probabilities estimated by the classifier and semantic embedding vector of the unseen label. This simple strategy outperforms state-of-the-art results. Because the cost function involves both multimodal combination and supervision, these family of models are tied to the task of interest. Thus, if the domain or task conditions changes, adaptations are required.\nThe proposed model is closely related to the mixture of experts (MoE) approach (Jacobs et al., 1991). However, the common usage of MoE is focused on performing decision fusion, i.e. combining predictors to address a supervised learning problem (Yuksel et al., 2012). Our model is devised as a new component in the representation learning scheme, making it independent from the final task (e.g. classification, regression, unsupervised learning, etc) provided that the defined cost function be differentiable.\n\n2.2 MOVIE GENRE CLASSIFICATION\nWith respect to movie genre classification, several strategies also have been proposed. These strategies have used different modalities to characterize each movie, such as textual features, image features and multimedia features (audio and/or video). Huang et al. (2007) were one of the first teams exploring this task. They classified movie previews into 3 genres by extracting handcrafted features from the video and training a decision tree classifier. They evaluated the model using 44 films. Using only textual modality, Shah et al. (2013) performed single-label genre classification of movie scripts using clustering algorithms with 260 movies. Later, combining two modalities, Pais et al. (2012) classified movies between drama and non-drama using visual and textual features with 107 samples. Hong & Hwang (2015) explored different PLSA models to combine 3 modalities: audio, image and text to predict genre of movie previews. It was single label classification with 4 genres for 140 movies taken from IMDb.\nRecently, Fu et al. (2015) used a set of handcrafted visual features for poster characterization and bag-of-words for synopsis. Then, they trained one SVM per each modality to combine their predictions. The dataset contained 2, 400 movies with one genre (out of 4) each.\nThe previous mentioned works present this problem in a single label setup. However, a more realistic scenario would be multilabel, since most of the movies belong to more than one genre, (e.g.\nMatrix(2000) is a Sci-fi/Action movie). In this setup, Anand (2014) explores the efficiency of using keywords and users’ tags to perform multilabeling using the movies from MovieLens 1M dataset which contains 1, 700 movies. Also Ivasic-Kos et al. (2014; 2015) performed multilabel classification using handcrafted features from posters, with 1, 500 samples for 6 genres. Makita & Lenskiy (2016a;b) use movie ratings matrix and genre correlation matrix to predict the genre. It used a smaller version of the Movielens dataset with 18 movie genres.\nMost of the above works have used the publicly available MovieLens datasets. However, there is not a single experimental setup defined so that all methods can be systematically compared. Also, to the best of our knowledge, none of the previous works contain more than 10, 000 samples. With this work we will release a dataset created with the movies of the MovieLens 20M dataset. We include not only genre, poster and plot information used in this work, but also the poster of the movie as well as more than 50 characteristics taken from the IMDb website. We will also release the source code to automatically add more movies and genres.\n\n3 METHODS\nThis paper presents a neural-network-based strategy for multilabel classification of multimodal data. The key component of the strategy is a novel type of hidden unit, the Gated Multimodal Unit (GMU), which learns to decide how modalities influence the activation of the unit using gates. The details of the GMU are presented in Subsection 3.1.\nStatistical properties usually are not shared across modalities (Srivastava & Salakhutdinov, 2012). And thus, they require different representation strategies according to the nature of data. This work explored several strategies to address text and visual representation. For text information we evaluated word2vec models, n-grams models and RNN models. The details are discussed in Subsection 3.2. On the other hand, two different convolutional neural networks were evaluated for processing visual data and are presented in Subsection 3.3.\n\n3.1 GATED MULTIMODAL UNIT FOR MULTIMODAL FUSION\nMultimodal learning is closely related to data fusion. Data fusion looks for optimal ways of combining different information sources into an integrated representation that provides more information than the individual sources (Bhatt & Kankanhalli, 2011). This fusion can be performed at different levels, that can be categorized into two broad categories: feature fusion and decision fusion. Feature fusion, also called early fusion, looks for a subset of features from different modalities, or combinations of them, that better represent the information needed to solve a particular problem. On the other hand, decision fusion, or late fusion, combines decisions from different systems, e.g. classifiers, to produce consensus. This consensus may be reached by a simple average, a voting system or a more complex Bayesian framework.\nIn this work we present a model, based on gated neural networks, for data fusion that combines ideas from both feature and decision fusion. The model, called Gated Multimodal Unit (GMU), is inspired by the flow control in recurrent architectures like GRU or LSTM. A GMU is intended to be used as an internal unit in a neural network architecture whose purpose is to find an intermediate representation based on a combination of data from different modalities. Figure 2.a depicts the structure of a GMU. Each xi corresponds to a feature vector associated with modality i. Each feature vector feeds a neuron with a tanh activation function, which is intended to encode an internal representation feature based on the particular modality. For each input modality, xi, there is a gate neuron (represented by σ nodes in the diagram), which controls the contribution of the feature calculated from xi to the overall output of the unit. When a new sample is fed to the network, a gate neuron associated to modality i receives as input the feature vectors from all the modalities and uses them to decide whether the modality i may contribute, or not, to the internal encoding of the particular input sample.\nFigure 2.b shows a simplified version of the GMU for two input modalities, xv (visual modality) and xt (textual modality), that will be used in the remaining of the paper. It should be noted that both models are not completely equivalent, since in the bimodal case the gates are tied. Such weight tying constraints the model, so that the units trade off between both modalities while they use less\nparameters than the multimodal case. The equations governing this GMU are as follows:\nhv = tanh (Wv · xv) ht = tanh (Wt · xt) z = σ (Wz · [xv, xt]) h = z ∗ hv + (1− z) ∗ ht Θ = {Wv,Wt,Wz}\nwith Θ the parameters to be learned and [·, ·] the concatenation operator. Since all are differentiable operations, this model can be easily coupled with other neural network architectures and trained with stochastic gradient descent.\n\n3.2 TEXT REPRESENTATION\nText representation is a critical step when classification tasks are addressed using machine learning methods. Traditional approaches are based on counting frequencies of n-gram occurrences such as words or sequences of characters (e.g. bag-of-words models). The main drawback of such approaches is the difficulty to model relationships between words and their context. An alternative approach was initially proposed by Bengio et al. (2003), by building a language model based on a neural network architecture (NNLM). The NNLM was able to learn distributed representations of words that capture contextual information. Later, this model was simplified to deal with large corpora by removing hidden layers in the neural network architecture (word2vec) (Mikolov et al., 2013a). This is a fully unsupervised model that takes advantage of large sets of unlabeled documents. Herein, three text representations were evaluated:\nn-gram Following the strategy proposed by Kanaris & Stamatatos (2009), we used the n-gram strategy for representing text. Despite their simplicity, n-gram models have shown to be a competitive baseline.\nWord2Vec Word2vec is an unsupervised learning algorithm that finds a vector representation for each word based on its context (Mikolov et al., 2013a). It has been shown that this model is able to find semantic and syntactic relationships using arithmetic operations between the vectors. Based on this property, we represent a movie as the average of the vectors of words in the plot outline. The main motivation to aggregate word2vec vectors is the property of additive compositionality that this representation has exposed over different set of tasks such as word analogies. The usual way to aggregate is to sum vectors. We instead take the average to avoid large input values to the neural network.\nRecurrent neural network Here we take the plot outline as a sequence of words and train a supervised recurrent neural network. We evaluated two variants. The first one (RNN w2v) is a transfer learning model that takes as input the word vectors of word2vec as representations. The second one learns the word vectors from scratch (RNN end2end).\n\n3.3 VISUAL REPRESENTATION\nIn computer vision tasks, Convolutional neural networks have become the de facto standard. It has been shown that CNN models trained with a huge amount of data are able to learn common features shared across different domains. This characteristic is usually exploited by transfer learning approaches. For visual representation we explored 2 strategies: transfer learning and end-to-end training.\nVGG Transfer In this approach, the VGG Network (Simonyan & Zisserman, 2014) trained with the ImageNet dataset is used as feature extractor by taking the last hidden activations as the visual representation.\nEnd2End CNN Here, a CNN with 5 convolutional layers and an MLP (see Section 3.4) on top was trained from scratch.\n\n3.4 CLASSIFICATION MODEL\nBased on the defined representation, we explored two methods to map from feature vectors to genre classification. In particular we explored a simple Logistic regression and a neural network architecture. This is a multilayer perceptron (MLP) with two fully connected layers and maxout activation function. In particular, the maxout activation function hi : Rn → R is a defined as:\nhi (s) = max j∈[1,k] zi,j (1)\nwhere s ∈ Rn is the input vector, zi,j = sTW···ij+bij is the output of the j-th linear transformation of the i-th hidden unit, and W ∈ Rd×m×k and b ∈ Rm×k are learned parameters. It has been shown that maxout models with just 2 hidden units behave as universal approximators, while are less prone to saturate units (Goodfellow et al., 2013).\n0 100 200 300 400 500 600 size (pixels)\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n# o\nf sa\nm p le\ns\nwidth height\nFigure 4: Size distribution of movie posters. 0 100 200 300 400 500 600 # of words\n0\n500\n1000\n1500\n2000\n2500\n3000\n# o\nf sa\nm p le\ns\nFigure 5: Length distribution of movie plots.\n\n4.1 MULTIMODAL IMDB DATASET\nWith this work we will make publicly available the Multimodal IMDb (MM-IMDb)1 dataset. MMIMDb dataset is built with the IMDb id’s provided by the Movielens 20M dataset 2 that contains ratings of 27, 000 movies. Using the IMDbPY 3 library, movies which do not contain their poster image were filtered out. As the final result, the MM-IMDb dataset comprises 25, 959 movies along with their plot, poster, genres and other 50 additional metadata fields such as year, language, writer, director, aspect ratio, etc.\nNotice that one movie may belong to more than one genre. Figure 3 shows the co-occurrence matrix, where the color bar indicates the representative co-occurrence per row, while Figure 4 and Figure 5 depict the distribution of the movie poster sizes and length of movie plots respectively. Each plot contains on average 92.5 words, while the longest one contains 1, 431 words and the average of genres per movie is 2.48. In this work, we defined the task of movie genre prediction based on its plot and image poster. Nevertheless, the additional metadata information encourages other interesting tasks such as rating prediction and content-based retrieval, among others.\n\n4.2 EXPERIMENTAL SETUP\nThe MM-IMDb dataset has been split in three subsets. Train, development and test subsets contain 15552, 2608 and 7799 respectively. The distribution of samples is listed in Table 1. The sample was stratified so that training, dev and test sets comprises 60%, 10%, 30% samples of each genre respectively.\nIn the multilabel classification the performance evaluation can be more complex than traditional multi-class classification and the differences can be significant among several measures (Madjarov et al., 2012). Herein, four averages of the f-score (f1) are reported: samples computes the f-score per sample and then averages the results, micro computes the f-score using all predictions at once, macro computes the f-score per genre and then averages the results. weighted is the same as macro with a weighted average based on the number of positive samples per genre. Concretely, we calculate them as follows (Madjarov et al., 2012):\nfsample1 = 1\nN N∑ i=1 2× |ŷi ∩ yi| |ŷi|+ |yi| fmacro1 = 1 Q ∑Q j=1 2×pj×rj pj+rj fweighted1 = 1 Q2 Q∑ j=1 Qj 2× pj × rj pj + rj\n1http://lisi1.unal.edu.co/mmimdb/ 2http://grouplens.org/datasets/movielens/ 3http://imdbpy.sourceforge.net/\nTable 1: Genre distribution per subset\nGenre Train Dev Test Genre Train Dev Test\nDrama 8424 1401 4142 Family 978 172 518 Comedy 5108 873 2611 Biography 788 144 411 Romance 3226 548 1590 War 806 128 401 Thriller 3113 512 1567 History 680 118 345 Crime 2293 382 1163 Music 634 100 311 Action 2155 351 1044 Animation 586 105 306 Adventure 1611 278 821 Musical 503 85 253 Horror 1603 275 825 Western 423 72 210 Documentary 1234 219 629 Sport 379 64 191 Mystery 1231 209 617 Short 281 48 142 Sci-Fi 1212 193 586 Film-Noir 202 34 102 Fantasy 1162 186 585\npmicro = ∑Q j=1 tpj∑Q\nj=1 tpj + ∑Q j=1 fpj rmicro =\n∑Q j=1 tpj∑Q\nj=1 tpj+ ∑Q j=1 fnj fmicro1 = 2× pmicro × rmicro pmicro + rmicro\nWith N the number of examples; Q the number of labels; Qj the number of true instances for the j-th label; p the precision, r the recall; ŷi, yi ∈ (0, 1)Q the prediction and ground truth binary tuples respectively; tpj , fpjandfnj the number of true positives, false positives and false negatives for the j-th label respectively.\nTEXTUAL REPRESENTATION\nThe pretrained Google Word2vec4 embedding space was used. After intersecting the Google word2vec available words with the MM-IMDb plots, the final vocabulary contains 41,612 words. Other than lowercase, no text preprocessing was applied. Since it is our intention to measure how the network’s depth affects the performance of the model, we also evaluate the architecture with a single fully connected layer. In order to compare the performance of this textual representation, we evaluate it using two publicly available datasets: 7genre dataset that comprises 1,400 web pages with 7 disjoint genres and ki-04 dataset that comprises 1,239 samples classified under 8 genres. We compare the model with the state of the art results (Kanaris & Stamatatos, 2009) which used character n-grams with structured information from the HTML tags to predict the genre of web pages.\nVISUAL REPRESENTATION\nSince the first approach was to use VGG as a feature extractor. This model is referred as VGG Transfer. The second approach takes as input the raw images to a CNN. Since all the images do not have the same size, all images were scaled, and cropped when required, to 160 × 256 pixels keeping the aspect ratio. This CNN comprises 5 CNN layers of 5, 3, 3, 3, 3 squared filters and 2 × 2 pool sizes. Each convolutional layer has 16 hidden units. The convolutional layers are connected with the MaxoutMLP on top.\nMULTIMODAL REPRESENTATION\nWe evaluate 4 different ways to combine both modalities as baselines.\nAverage probability This can be seen as a late-fusion strategy. The probabilities obtained by the best model of each modality are averaged and thresholded.\nconcatenation Different works have found that a simple concatenation of representations of different modalities are good for combining the information (Suk & Shen, 2013; Pei et al., 2013; Kiela & Bottou, 2014). Herein, we concatenated both representations to train the MaxoutMLP architecture.\n4https://code.google.com/archive/p/word2vec/\nlinear sum Following the way Vinyals et al. (2015) combine text and images representation into a single space, this model adds a linear transformation for each modality so that both outputs have the same size to be summed up and then followed by the MaxoutMLP architecture.\nMoE The mixture of experts (MoE) (Jacobs et al., 1991) model was adapted for multilabel classification. two gating strategies were explored: tied, where a single gate multiplies all the logistics outputs, and untied where every logistic output has its own gate. Logistic regression and MaxoutMLP were evaluated as experts.\nNEURAL NETWORK TRAINING\nNeural network models were trained using using Batch Normalization scheme (Ioffe & Szegedy, 2015). This strategy applies a normalization step across samples that belong to the same batch, so that each hidden unit in the network receive a zero-mean and unit variance. Stochastic gradient descent with ADAM optimization (Kingma & Ba, 2014) was used to learn the weights of the neural network. Dropout and max-norm regularization were used to control overfitting. Hidden size ({64, 128, 256, 512}), learning rate ( [ 10−3, 10−1 ] ), dropout ([0.3, 0.7]), max-norm ([5, 20]) and\ninitialization ranges ( [ 10−3, 10−1 ] ) parameters were explored by training 25 models with random (uniform) hyperparameter initializations and the best was chosen according to validation performance. It has been reported that this strategy is preferable over grid search when training deep models (Bergstra & Bengio, 2012). All the implementation was carried on with the Blocks framework (Van Merriënboer et al., 2015)5.\nDuring the training process, we noticed that batch normalization considerably helped in terms of training time and convergence, resulting in less sensitivity to hyperparameters such as initialization ranges or learning rate. Also, dropout and max-norm regularization strategies helped to increase the performance at test time.\n\n5.1 EVALUATION OVER SYNTHETIC DATA\nIn order to evaluate if the model is able to identify which modality is contributing more information to classify a particular sample, we created a synthetic task based on a generative model, which is depicted in Figure 6. In this model we define the random binary variable C as the target and xv, xt ∈ R2 as the input features. M is a random binary variable that decides which modality will contain the relevant information that determines the class. The input features of each modality can\n5https://github.com/johnarevalo/gmu-mmimdb\nbe generated by a random source, ŷv and ŷt, or by an informed source, yv and yt. The generative model is specified as follows:\nC ∼ Bernoulli(pC) M ∼ Bernoulli(pM ) yv ∼ N (γCv ) ŷv ∼ N (γ̂v)\nxv = Myv + (1−M)ŷv yt ∼ N (γCt ) ŷt ∼ N (γ̂t) xt = Mŷt + (1−M)yt\nWe trained a model with a single GMU and applied a sigmoid function over h, then the binary cross entropy was used as loss function. Using the generative model, 200 samples per class were generated for each experiment. 1000 synthetic experiments with different random seeds were run and the GMU outperformed a logistic regression classifier in 370 of them, while obtaining equal results in the remainder ones. Our goal in these simulations was to show that the model was able to learn a latent variable that determines which modality carries the useful information for the classification. An interesting result is that between M and the activations of the gate z there is a correlation of 1. This means the model was capable of learning such latent variable by only observing the xv and xt input features.\nWe also wanted to project back the z activations to the feature space in order to visualize regions depending on the modality. Figure 7 shows the activations in a synthetic experiment generated by the setup of Figure 6 for xv, xt ∈ R1. Each axis represents a modality, red and blue dots are the samples generated for the two classes and black Gaussian curves represent the γ̂v and γ̂t noises. The contour of the left figure (gray) represents the activation of z. Notice that in white regions (z = 1), the model gives more importance to the xv modality while in gray regions (z = 0) the xt modality is more relevant; i.e. the z gate is isolating the noise. The contour of the right figure (blue-red) represents the model prediction. It is noteworthy that the boundary defined by the gates still holds when the model solves the task. This also encourages the inclusion of non-linearities to the z gate so that it is able to discriminate more complex interactions between modalities.\n\n5.2 GENRE CLASSIFICATION RESULTS\nBefore using our text representation in the multimodal task, we wanted to be sure such representation was good enough to address the genre classification task. Thus, we evaluated it on 2 public datasets. We found MaxoutMLP w2v achieves the state of the art results on the ki-04 dataset and increases the performance in the 7Genre dataset from 0.841 to 0.854 (Kanaris & Stamatatos, 2009). Notice\nthat the baseline uses additional information from the HTML structure from the web page, while this representation uses only the text data.\nTable 2 shows the results in the proposed dataset. For the textual modality, the best performance is obtained by the combination of word2vec representation with an MLP classifier. The behavior of all representation methods are consistent across the performance measures. Learning from scratch the RNN model performed the worst. We hypothesize this has to do with the lack of data to learn meaningful relations among words. It has been shown that millions of words are required to train a model such as word2vec that is able to exploit common regularities between word co-occurrences.\nFor the visual modality, the usage of pretrained models works better than training the model from scratch. It seems it is still a small dataset to learn all the complexities of the posters. Now, comparing the performance independently per genre, as in Table 3, it is interesting to notice that in Animation the visual modality outperforms the textual one.\nIn the multimodal scenario, by adding the GMU as building block to learn the fusion we obtained the best performance, improving independent modalities in the averaged measures and in 16 of out 23 genres and outperforming all other evaluated fusion strategies. The concatenation or the linear\ncombination approaches were not enough to model the correlation between the modalities and MoE models did not perform better than simpler approaches. This is an expected behavior for MoE in a relatively small dataset because the data is fractionated over different experts, and thus it doesn’t make an efficient use of the training samples.\nIn order to evaluate which modality influences more the model when assigning a particular label, we averaged the activations of a subset of z gates of the test samples to which the model assigned them such label. We counted the number of samples that pays more attention to the textual modality (z <= 0.5) or to the visual modality (z > 0.5). The units were chosen taking into account the mutual information between the predictions and the z activations. The result of this analysis is depicted in Figure 8. As expected, the model is generally more influenced by the textual modality. But, in particular cases such as Animation or Family genres, the visual modality affects more the model. This is also consistent with results of Table 3 which reports better performances for visual modality.\nWe wanted to qualitative explore test examples in which performance was improved by a relative large margin. Table 4 illustrates cases where the model takes advantage of the most accurate modality, and in some cases removes false positives. It is noteworthy that some of these examples can be confusing for a human if one modality is missing, or additional context information is not given.\n\n6 CONCLUSIONS\nThis work presented a strategy to learn fusion transformations from multimodal sources. Similarly to the way recurrent models control the information flow, the proposed model is based on multiplicative gates. The Gated Multimodal Unit (GMU) receives two or more input sources and learns to determine how much each input modality affects the unit activation. In synthetic experiments the GMU was able to learn hidden latent variables, and in a real scenario it outperformed the singlemodality approaches. An interesting property of GMU is that, being a differentiable operation, it\nis easily coupled in any other neural network architecture and trained with standard gradient-based optimization algorithms. With this work we will also release a new dataset that contains around 27, 000 movie plots, images and other metadata. To the best of our knowledge, this is the biggest dataset used to perform movie genre classification based on multimodal information and the first one to be publicly available. In our future work we expect to explore deep architectures of GMU layers as well as integration with attention mechanism over the input modalities. Also, It will be interesting to explore in more depth the interpretability of the learned features.\n\nACKNOWLEDGMENTS\nArevalo thanks Colciencias for its support through a doctoral grant in call 617/2013. The authors also thank for K40 Tesla GPU donated by NVIDIA and which was used for some representation learning experiments.", "ground_truth": "ICLR 2017 conference submission\n\n---\n\nGated Multimodal Units for Information Fusion\n\n---\n\nThis paper presents a novel model for multimodal learning based on gated neural networks. The Gated Multimodal Unit (GMU) model is intended to be used as an internal unit in a neural network architecture whose purpose is to find an intermediate representation based on a combination of data from different modalities. The GMU learns to decide how modalities influence the activation of the unit using multiplicative gates. It was evaluated on a multilabel scenario for genre classification of movies using the plot and the poster. The GMU improved the macro f-score performance of single-modality approaches and outperformed other fusion strategies, including mixture of experts models. Along with this work, the MM-IMDb dataset is released which, to the best of our knowledge, is the largest publicly available multimodal dataset for genre prediction on movies.\n\n---\n\nICLR committee final decision\n\n---\n\nICLR 2017 pcs\n\n---\n\nThe authors propose a Gated Muiltimodal Unit to combine multi-modal information (visual and textual). They also collect a large dataset of movie summers and posters. Overall, the reviewers were quite positive, while AR4 points to related models and feels that the contribution in the current version is too weak for ICLR. The AC read the paper and the authors responses but tends to agree with AR4. The authors are encouraged to strengthen their work and resubmit to a future conference.\n\n---\n\n06 Feb 2017\n\n---\n\nNew revision submitted\n\n---\n\nJohn Arevalo\n\n---\n\nWe have added a new version which includes the mixture of experts evaluation. Since this is a multilabel scenario, we implement tied and untied gates for the outputs. We also evaluate Logistic regression and MaxoutMLP as experts. \n\nOverall, the MoE models did not work very well in comparison with other fusion strategies. We believe this has to do with the amount of training samples. It is known that MoE models require large datasets because the data is fractionated over different experts.\n\n---\n\n13 Jan 2017\n\n---\n\nICLR 2017 conference AnonReviewer3\n\n---\n\nreview\n\n---\n\nPaper proposes Gated Muiltimodal Unit, a building block for connectionist models capable of handling multiple modalities.\n\n(Figure 2) The bimodal case returns weighted activation by gains of gating units, do you do anything special to keep multi-modal case weighted as well? I.e. how the equation for h in section 3.1 would look like for multi-modal case. Also what’s the rationale for using tanh nonlinearity (over, say RELU), is it somehow experimentally optimised choice?\n\nI would find interesting a discussion on a possibility of handling missing data in case one or more modalities are unavailable at test time. Is this possible in the current model to back-off to fewer modalities? Synthetic example may suggest that’s in fact possible. Those numbers, perhaps, could be added to table 2.\n\nIn the synthetic experiment, you should compare MGU with the fully-connected MLP model really, with similar complexity - that is - at least two hidden units (as GMU has two such for each modality) followed by logistic regression. At least in terms of capability of drawing decision boundary, those should be comparable.\n\nI think, broader discussion shall be written on the related work associated with mixture of experts models (which is fact are very similar conceptually) as well as multiplicative RNN models [1]. Also, gating unit in LSTM can, in principle, play very similar role when multiple modalities are spliced in the input.\n\nOverall, the paper is interesting, so is the associated (and to be released) dataset.\n\nMinor comments/typos:\n\nSec. 3.3: layers and a MLP (see Section 3.4) -> layers and an MLP\n\nApologies for unacceptably late review.\n\n[1] Multiplicative LSTM for sequence modelling B Krause, L Lu, I Murray, S Renals\n\n---\n\n21 Dec 2016 (modified: 19 Jan 2017)\n\n---\n\nICLR 2017 conference AnonReviewer4\n\n---\n\nPromising work, but too preliminary for a major conference\n\n---\n\nThe paper introduces Gated Multimodal Units GMUs, which use multiplicative weights to select the degree to which a hidden unit will consider different modalities in determining its activation. The paper also introduces a new dataset, \"Multimodal IMDb,\" consisting of over 25k movie summaries, with their posters, and labeled genres.\n\nGMUs are related to \"mixture of experts\" in that different examples will be classified by different parts of the model, (but rather than routing/gating entire examples, individual hidden units are gated separately). They are related to attention models in that different parts of the input are weighted differently; there the emphasis is on gating modalities of input.\n\nThe dataset is a very nice contribution, and there are many experiments varying text representation and single-modality vs two-modality. What the paper is lacking is a careful discussion, experimentation and analysis in comparison to other multiplicative gate models---which is the core intellectual contribution of the paper. For example, I could imagine that a mixture of experts or attention models or other gated models might perform very well, and at the very least provide interesting scientific comparative analysis. I encourage the authors to continue the work, and submit a revised paper when ready.\n\nAs is, I consider the paper to be a good workshop paper, but not ready for a major conference.\n\n---\n\n19 Dec 2016\n\n---\n\nICLR 2017 conference AnonReviewer2\n\n---\n\nGMU is an interesting model, but still it is not straighforward to understand how much better than baselines\n\n---\n\nThis paper proposed The Gated Multimodal Unit (GMU) model for information fusion. The GMU learns to decide how modalities influence the activation of the unit using multiplicative gates. The paper collected a large genre dataset from IMDB and showed that GMU gets good performance.\n\nThe proposed approach seems quite interesting, and the audience may expect it can be used in general scenarios beyond movie genre prediction. So it is quite straightforward that the paper should test the algorithm in other applications, which was not done yet. That is the biggest shortcoming of this paper in my opinions. \n\nAnother concern lies in how to evaluate the performance of information fusion. The abstract claims \"The model improves the macro f-score performance of single-modality models by 30% and 4% with respect to visual and textual information respectively\", however, such an improvement is off the key. If two modals are complementary to each other, the fusion results will always be higher. The key fact is how much better than baselines the proposed GMU is. There is a long list of techniques for fusions, so it is difficult to conduct an impressive comparison on only one real dataset. I think GMU did a nice work on movie dataset, but I would also expect other techniques, including fine-tuning, dropout, distillation may help too. It would be nice if the author could compare these techniques. \n\nI also hope this paper could talk in more details the connection with mixture-of-expert (MoE) model. Both models are based on the nonlinear gated functions, while both method may suffer from local minimum for optimization on small datasets. I would like more in-depth discussion in their similarity and difference.\n\nTo gain more attention for GMU, I would encourage the author to open-source their code and try more datasets.\n\n---\n\n16 Dec 2016 (modified: 24 Jan 2017)\n\n---\n\nNew revision submitted\n\n---\n\nJohn Arevalo\n\n---\n\nFollowing your comments, we've added a new revision which includes:\n - More details on parameter exploration and training procedure.\n - Updates on multimodal baseline and synthetic results.\n - Histogram for image sizes.\n - Link to the dataset webpage.\n - Plot depicting the influence of each modality in the label assignment.\n\nThanks again for your feedback.\n\n---\n\n13 Dec 2016\n\n---\n\nQuestions\n\n---\n\nICLR 2017 conference AnonReviewer4\n\n---\n\n04 Dec 2016\n\n---\n\nseveral questions on experiments\n\n---\n\nICLR 2017 conference AnonReviewer2\n\n---\n\n22 Nov 2016\n\n---\n\nJohn Arevalo, Thamar Solorio, Manuel Montes-y-Gómez, Fabio A. González\n\n---\n\nGated Multimodal Units: a novel unit that learns to combine multiple modalities using multiplicative gates"}
{"paper_id": "476", "paper_text": "ABSTRACT: Yes, they do. This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained. Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution. Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher.\n\n1 INTRODUCTION\nCybenko (1989) proved that a network with a large enough single hidden layer of sigmoid units can approximate any decision boundary. Empirical work, however, suggests that it can be difficult to train shallow nets to be as accurate as deep nets. Dauphin and Bengio (2013) trained shallow nets on SIFT features to classify a large-scale ImageNet dataset and found that it was difficult to train large, high-accuracy, shallow nets. A study of deep convolutional nets suggests that for vision tasks deeper models are preferred under a parameter budget (e.g. Eigen et al. (2014); He et al. (2015); Simonyan and Zisserman (2014); Srivastava et al. (2015)). Similarly, Seide et al. (2011) and Geras et al. (2015) show that deeper models are more accurate than shallow models in speech acoustic modeling. More recently, Romero et al. (2015) showed that it is possible to gain increases in accuracy in models with few parameters by training deeper, thinner nets (FitNets) to mimic much wider nets. Cohen and Shashua (2016); Liang and Srikant (2016) suggest that the representational efficiency of deep networks scales exponentially with depth, but it is unclear if this applies only to pathological problems, or is encountered in practice on data sets such as TIMIT and CIFAR.\nBa and Caruana (2014), however, demonstrated that shallow nets sometimes can learn the functions learned by deep nets, even when restricted to the same number of parameters as the deep nets. They did this by first training state-of-the-art deep models, and then training shallow models to mimic the deep models. Surprisingly, and for reasons that are not well understood, the shallow models learned more accurate functions when trained to mimic the deep models than when trained on the original data used to train the deep models. In some cases shallow models trained this way were as accurate as state-of-the-art deep models. But this demonstration was made on the TIMIT speech recognition benchmark. Although their deep teacher models used a convolutional layer, convolution is less important for TIMIT than it is for other domains such as image classification.\nBa and Caruana (2014) also presented results on CIFAR-10 which showed that a shallow model could learn functions almost as accurate as deep convolutional nets. Unfortunately, the results on CIFAR-10 are less convincing than those for TIMIT. To train accurate shallow models on CIFAR-10\nthey had to include at least one convolutional layer in the shallow model, and increased the number of parameters in the shallow model until it was 30 times larger than the deep teacher model. Despite this, the shallow convolutional student model was several points less accurate than a teacher model that was itself several points less accurate than state-of-the-art models on CIFAR-10.\nIn this paper we show that the methods Ba and Caruana used to train shallow students to mimic deep teacher models on TIMIT do not work as well on problems such as CIFAR-10 where multiple layers of convolution are required to train accurate teacher models. If the student models have a similar number of parameters as the deep teacher models, high accuracy can not be achieved without multiple layers of convolution even when the student models are trained via distillation.\nTo ensure that the shallow student models are trained as accurately as possible, we use Bayesian optimization to thoroughly explore the space of architectures and learning hyperparameters. Although this combination of distillation and hyperparameter optimization allows us to train the most accurate shallow models ever trained on CIFAR-10, the shallow models still are not as accurate as deep models. Our results clearly suggest that deep convolutional nets do, in fact, need to be both deep and convolutional, even when trained to mimic very accurate models via distillation (Hinton et al., 2015).\n\n2 TRAINING SHALLOW NETS TO MIMIC DEEPER CONVOLUTIONAL NETS\nIn this paper, we revisit the CIFAR-10 experiments in Ba and Caruana (2014). Unlike in that work, here we compare shallow models to state-of-the-art deep convolutional models, and restrict the number of parameters in the shallow student models to be comparable to the number of parameters in the deep convolutional teacher models. Because we anticipated that our results might be different, we follow their approach closely to eliminate the possibility that the results differ merely because of changes in methodology. Note that the goal of this paper is not to train models that are small or fast as in Bucila et al. (2006), Hinton et al. (2015), and Romero et al. (2015), but to examine if shallow models can be as accurate as deep convolutional models given the same parameter budget.\nThere are many steps required to train shallow student models to be as accurate as possible: train state-of-the-art deep convolutional teacher models, form an ensemble of the best deep models, collect and combine their predictions on a large transfer set, and then train carefully optimized shallow student models to mimic the teacher ensemble. For negative results to be informative, it is important that each of these steps be performed as well as possible. In this section we describe the experimental methodology in detail. Readers familiar with distillation (model compression), training deep models on CIFAR-10, data augmentation, and Bayesian hyperparameter optimization may wish to skip to the empirical results in Section 3.\n\n2.1 MODEL COMPRESSION AND DISTILLATION\nThe key idea behind model compression is to train a compact model to approximate the function learned by another larger, more complex model. Bucila et al. (2006) showed how a single neural net of modest size could be trained to mimic a much larger ensemble. Although the small neural nets contained 1000× fewer parameters, often they were as accurate as the large ensembles they were trained to mimic.\nModel compression works by passing unlabeled data through the large, accurate teacher model to collect the real-valued scores it predicts, and then training a student model to mimic these scores. Hinton et al. (2015) generalized the methods of Bucila et al. (2006) and Ba and Caruana (2014) by incorporating a parameter to control the relative importance of the soft targets provided by the teacher model to the hard targets in the original training data, as well as a temperature parameter that regularizes learning by pushing targets towards the uniform distribution. Hinton et al. (2015) also demonstrated that much of the knowledge passed from the teacher to the student is conveyed as dark knowledge contained in the relative scores (probabilities) of outputs corresponding to other classes, as opposed to the scores given to just the output for the one correct class.\nSurprisingly, distillation often allows smaller and/or shallower models to be trained that are nearly as accurate as the larger, deeper models they are trained to mimic, yet these same small models are not as accurate when trained on the 1-hot hard targets in the original training set. The reason for this is not yet well understood. Similar compression and distillation methods have also successfully\nbeen used in speech recognition (e.g. Chan et al. (2015); Geras et al. (2015); Li et al. (2014)) and reinforcement learning Parisotto et al. (2016); Rusu et al. (2016). Romero et al. (2015) showed that distillation methods can be used to train small students that are more accurate than the teacher models by making the student models deeper, but thinner, than the teacher model.\n\n2.2 MIMIC LEARNING VIA L2 REGRESSION ON LOGITS\nWe train shallow mimic nets using data labeled by an ensemble of deep teacher nets trained on the original 1-hot CIFAR-10 training data. The deep teacher models are trained in the usual way using softmax outputs and cross-entropy cost function. Following Ba and Caruana (2014), the student mimic models are not trained with cross-entropy on the ten p values where pk = ezk/ ∑ j e\nzj output by the softmax layer from the deep teacher model, but instead are trained on the un-normalized log probability values z (the logits) before the softmax activation. Training on the logarithms of predicted probabilities (logits) helps provide the dark knowledge that regularizes students by placing emphasis on the relationships learned by the teacher model across all of the outputs.\nAs in Ba and Caruana (2014), the student is trained as a regression problem given training data {(x(1), z(1)),...,(x(T ), z(T ))}:\nL(W ) = 1 T ∑ t ||g(x(t);W )− z(t)||22, (1)\nwhereW represents all of the weights in the network, and g(x(t);W ) is the model prediction on the tth training data sample.\n\n2.3 USING A LINEAR BOTTLENECK TO SPEED UP TRAINING\nA shallow net has to have more hidden units in each layer to match the number of parameters in a deep net. Ba and Caruana (2014) found that training these wide, shallow mimic models with backpropagation was slow, and introduced a linear bottleneck layer between the input and non-linear layers to speed learning. The bottleneck layer speeds learning by reducing the number of parameters that must be learned, but does not make the model deeper because the linear terms can be absorbed back into the non-linear weight matrix after learning. See Ba and Caruana (2014) for details. To match their experiments we use linear bottlenecks when training student models with 0 or 1 convolutional layers, but did not find the linear bottlenecks necessary when training student models with more than 1 convolutional layer.\n\n2.4 BAYESIAN HYPERPARAMETER OPTIMIZATION\nThe goal of this work is to determine empirically if shallow nets can be trained to be as accurate as deep convolutional models using a similar number of parameters in the deep and shallow models. If we succeed in training a shallow model to be as accurate as a deep convolutional model, this provides an existence proof that shallow models can represent and learn the complex functions learned by deep convolutional models. If, however, we are unable to train shallow models to be as accurate as deep convolutional nets, we might fail only because we did not train the shallow nets well enough.\nIn all our experiments we employ Bayesian hyperparameter optimization using Gaussian process regression to ensure that we thoroughly and objectively explore the hyperparameters that govern learning. The implementation we use is Spearmint (Snoek et al., 2012). The hyperparameters we optimize with Bayesian optimization include the initial learning rate, momentum, scaling of the initial random weights, scaling of the inputs, and terms that determine the width of each of the network’s layers (i.e. number of convolutional filters and neurons). More details of the hyperparameter optimization can be found in Sections 2.5, 2.7, 2.8 and in the Appendix.\n\n2.5 TRAINING DATA AND DATA AUGMENTATION\nThe CIFAR-10 (Krizhevsky, 2009) data set consists of a set of natural images from 10 different object classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. The dataset is a labeled subset of the 80 million tiny images dataset (Torralba et al., 2008) and is divided into 50,000 train and\n10,000 test images. Each image is 32×32 pixels in 3 color channels, yielding input vectors with 3072 dimensions. We prepared the data by subtracting the mean and dividing by the standard deviation of each image vector. We train all models on a subset of 40,000 images and use the remaining 10,000 images as the validation set for the Bayesian optimization. The final trained models only used 80% of the theoretically available training data (as opposed to retraining on all of the data after hyperparameter optimization).\nWe employ the HSV-data augmentation technique as described by Snoek et al. (2015). Thus we shift hue, saturation and value by uniform random values: ∆h ∼ U(−Dh, Dh), ∆s ∼ U(−Ds, Ds), ∆v ∼ U(−Dv, Dv). Saturation and value values are scaled globally: as ∼ U( 11+As , 1 + As), av ∼ U( 1 1+Av\n, 1 + Av). The five constants Dh, Ds, Dv, As, Av are treated as additional hyperparameters in the Bayesian hyperparameter optimization.\nAll training images are mirrored left-right randomly with a probability of 0.5. The input images are further scaled and jittered randomly by cropping windows of size 24×24 up to 32×32 at random locations and then scaling them back to 32×32. The procedure is as follows: we sample an integer value S ∼ U(24, 32) and then a pair of integers x, y ∼ U(0, 32 − S). The transformed resulting image is R = fspline,3(I[x : x + S, y : y + S]) with I denoting the original image and fspline,3 denoting the 3rd order spline interpolation function that maps the 2D array back to 32×32 (applied to the three color channels separately).\nAll data augmentations for the teacher models are computed on the fly using different random seeds. For student models trained to mimic the ensemble (see Section 2.7 for details of the ensemble teacher model), we pre-generated 160 epochs worth of randomly augmented training data, evaluated the ensemble’s predictions (logits) on these samples, and saved all data and predictions to disk. All student models thus see the same training data in the same order. The parameters for HSV-augmentation in this case had to be selected beforehand; we chose to use the settings found with the best single model (Dh = 0.06, Ds = 0.26, Dv = 0.20, As = 0.21, Av = 0.13). Pre-saving the logits and augmented data is important to reduce the computational cost at training time, and to ensure that all student models see the same training data\nBecause augmentation allows us to generate large training sets from the original 50,000 images, we use augmented data as the transfer set for model compression. No extra unlabeled data is required.\n\n2.6 LEARNING-RATE SCHEDULE\nWe train all models using SGD with Nesterov momentum. The initial learning rate and momentum are chosen by Bayesian optimization. The learning rate is reduced according to the evolution of the model’s validation error: it is halved if the validation error does not drop for ten epochs in a row. It is not reduced within the next eight epochs following a reduction step. Training ends if the error did not drop for 30 epochs in a row or if the learning rate was reduced by a factor of more than 2000 in total.\nThis schedule provides a way to train the highly varying models in a fair manner (it is not feasible to optimize all of the parameters that define the learning schedule). It also decreases the time spent to train each model compared to using a hand-selected overestimate of the number of epochs to train, thus allowing us to train more models in the hyperparameter search.\n\n2.7 SUPER TEACHER: AN ENSEMBLE OF 16 DEEP CONVOLUTIONAL CIFAR-10 MODELS\nOne limitation of the CIFAR-10 experiments performed in Ba and Caruana (2014) is that the teacher models were not state-of-the-art. The best deep models they trained on CIFAR-10 had only 88% accuracy, and the ensemble of deep models they used as a teacher had only 89% accuracy. The accuracies were not state-of-the-art because they did not use augmentation and because their deepest models had only three convolutional layers. Because our goal is to determine if shallow models can be as accurate as deep convolutional models, it is important that the deep models we compare to (and use as teachers) are as accurate as possible.\nWe train deep neural networks with eight convolutional layers, three intermittent max-pooling layers and two fully-connected hidden layers. We include the size of these layers in the hyperparameter optimization, by allowing the first two convolutional layers to contain from 32 to 96 filters each, the next two layers to contain from 64 to 192 filters, and the last four convolutional layers to contain\nfrom 128 to 384 filters. The two fully-connected hidden layers can contain from 512 to 1536 neurons. We parametrize these model-sizes by four scalars (the layers are grouped as 2-2-4) and include the scalars in the hyperparameter optimization. All models are trained using Theano (Bastien et al., 2012; Bergstra et al., 2010).\nWe optimize eighteen hyperparameters overall: initial learning rate on [0.01, 0.05], momentum on [0.80, 0.91], l2 weight decay on [5 · 10−5,4 · 10−4], initialization coefficient on [0.8, 1.35] which scales the initial weights of the CNN, four separate dropout rates, five constants controlling the HSV data augmentation, and the four scaling constants controlling the networks’ layer widths. The learning rate and momentum are optimized on a log-scale (as opposed to linear scale) by optimizing the exponent with appropriate bounds, e.g. LR = e−x optimized over x on [3.0, 4.6]. See the Appendix for more details about hyperparameter optimization.\nWe trained 129 deep CNN models with Spearmint. The best model obtained an accuracy of 92.78%; the fifth best achieved 92.67%. See Table 1 for the sizes and architectures of the three best models.\nWe are able to construct a more accurate model on CIFAR-10 by forming an ensemble of multiple deep convolutional neural nets, each trained with different hyperparameters, and each seeing slightly different training data (as the augmentation parameters vary). We experimented with a number of ensembles of the many deep convnets we trained, using accuracy on the validation set to select the best combination. The final ensemble contained 16 deep convnets and had an accuracy of 94.0% on the validation set, and 93.8% on the final test set. We believe this is among the top published results for deep learning on CIFAR-10. The ensemble averages the logits predicted by each model before the softmax layers.\nWe used this very accurate ensemble model as the teacher model to label the data used to train the shallower student nets. As described in Section 2.2, the logits (the scores just prior to the final softmax layer) from each of the CNN teachers in the ensemble model are averaged for each class, and the average logits are used as final regression targets to train the shallower student neural nets.\n\n2.8 TRAINING SHALLOW STUDENT MODELS TO MIMIC AN ENSEMBLE OF DEEP CONVOLUTIONAL MODELS\nWe trained student mimic nets with 1, 3.161, 10 and 31.6 million trainable parameters on the pre-computed augmented training data (Section 2.5) that was re-labeled by the teacher ensemble (Section 2.7). For each of the four student sizes we trained shallow fully-connected student MLPs containing 1, 2, 3, 4, or 5 layers of non-linear units (ReLU), and student CNNs with 1, 2, 3 or 4 convolutional layers. The convolutional student models also contain one fully-connected ReLU layer. Models with zero or only one convolutional layer contain an additional linear bottleneck layer to speed up learning (cf. Section 2.3). We did not need to use a bottleneck to speed up learning for the deeper models as the number of learnable parameters is naturally reduced by the max-pooling layers.\nThe student CNNs use max-pooling and Bayesian optimization controls the number of convolutional filters and hidden units in each layer. The hyperparameters we optimized in the student models are: initial learning rate, momentum, scaling of the initially randomly distributed learnable parameters, scaling of all pixel values of the input, and the scale factors that control the width of all hidden and convolutional layers in the model. Weights are initialized as in Glorot and Bengio (2010). We intentionally do not optimize and do not make use of weight decay and dropout when training student models because preliminary experiments showed that these consistently reduced the accuracy of student models by several percent. Please refer to the Appendix for more details on the individual architectures and hyperparameter ranges.\n\n3 EMPIRICAL RESULTS\nTable 1 summarizes results after Bayesian hyperparameter optimization for models trained on the original 0/1 hard CIFAR-10 labels. All of these models use weight decay and are trained with the dropout hyperparameters included in the Bayesian optimization. The table shows the accuracy of the best three deep convolutional models we could train on CIFAR-10, as well as the accuracy of\n13.16 ≈ Sqrt(10) falls halfway between 1 and 10 on log scale.\nthe ensemble of 16 deep CNNs. For comparison, the accuracy of the ensemble trained by Ba and Caruana (2014)) is included at the bottom of the table.\nTable 2 summarizes the results after Bayesian hyperparameter optimization for student models of different depths and number of parameters trained on soft targets (average logits) to mimic the teacher ensemble of 16 deep CNNs. For comparison, the student model trained by Ba and Caruana (2014) also is shown.\nThe first four rows in Table 1 show the accuracy of convolutional models with 10 million parameters and 1, 2, 3, and 4 convolutional layers. The accuracies of these same architectures with 1M, 3.16M, 10M, and 31.6M parameters when trained as students on the soft targets predicted by the teacher ensemble are shown in Table 2. Comparing the accuracies of the models with 10 million parameters in both tables, we see that training student models to mimic the ensemble leads to significantly better accuracy in every case. The gains are more pronounced for shallower models, most likely because their learnable internal representations do not naturally lead to good generalization in this task when trained on the 0/1 hard targets: the difference in accuracy for models with one convolutional layer is 2.7% (87.3% vs. 84.6%) and only 0.8% (92.6% vs. 91.8%) for models with four convolutional layers.\nFigure 1 summarizes the results in Table 2 for student models of different depth, number of convolutional layers, and number of parameters when trained to mimic the ensemble teacher model. Student models trained on the ensemble logits are able to achieve accuracies previously unseen on CIFAR-10 for models with so few layers. Also, it is clear that there is a huge gap between the convolutional student models at the top of the figure, and the non-convolutional student models at the bottom of the figure: the most accurate student MLP has accuracy less than 75%, while the least accurate convolutional student model with the same number of parameters but only one convolutional layer has accuracy above 87%. And the accuracy of the convolutional student models increases further as more layers of convolution are added. Interestingly, the most accurate student MLPs with no convolutional layers have only 2 or 3 hidden layers; the student MLPs with 4 or 5 hidden layers are not as accurate.\nComparing the student MLP with only one hidden layer (bottom of the graph) to the student CNN with 1 convolutional layer clearly suggests that convolution is critical for this problem even when models are trained via distillation, and that it is very unlikely that a shallow non-convolutional model with 100 million parameters or less could ever achieve accuracy comparable to a convolutional model. It appears that if convolution is critical for teacher models trained on the original 0/1 hard targets, it\nis likely to be critical for student models trained to mimic these teacher models. Adding depth to the student MLPs without adding convolution does not significantly close this “convolutional gap”.\nFurthermore, comparing student CNNs with 1, 2, 3, and 4 convolutional layers, it is clear that CNN students benefit from multiple convolutional layers. Although the students do not need as many layers as teacher models trained on the original 0/1 hard targets, accuracy increases significantly as multiple convolutional layers are added to the model. For example, the best student with only one convolutional layer has 87.7% accuracy, while the student with the same number of parameters (31M) and 4 convolutional layers has 92.6% accuracy.\nFigure 1 includes short horizontal lines at 10M parameters indicating the accuracy of non-student models trained on the original 0/1 hard targets instead of on the soft targets. This “compression gap” is largest for shallower models, and as expected disappears as the student models become architecturally more similar to the teacher models with multiple layers of convolution. The benefits of distillation are most significant for shallow models, yielding an increase in accuracy of 3% or more.\nOne pattern that is clear in the graph is that all student models benefit when the number of parameters increases from 1 million to 31 million parameters. It is interesting to note, however, that the largest student (31M) with a one convolutional layer is less accurate than the smallest student (1M) with two convolutional layers, further demonstrating the value of depth in convolutional models.\nIn summary, depth-constrained student models trained to mimic a high-accuracy ensemble of deep convolutional models perform better than similar models trained on the original hard targets (the “compression” gaps in Figure 1), student models need at least 3-4 convolutional layers to have high accuracy on CIFAR-10, shallow students with no convolutional layers perform poorly on CIFAR-10, and student models need at least 3-10M parameters to perform well. We are not able to compress deep convolutional models to shallow student models without significant loss of accuracy.\nWe are currently running a reduced set of experiments on ImageNet, though the chances of shallow models performing well on a more challenging problem such as ImageNet appear to be slim.\n\n4 DISCUSSION\nAlthough we are not able to train shallow models to be as accurate as deep models, the models trained via distillation are the most accurate models of their architecture ever trained on CIFAR-10. For example, the best single-layer fully-connected MLP (no convolution) we trained achieved an accuracy of 70.2%. We believe this to be the most accurate shallow MLP ever reported for CIFAR-10 (in comparison to 63.1% achieved by Le et al. (2013), 63.9% by Memisevic et al. (2015) and 64.3% by Geras and Sutton (2015)). Although this model cannot compete with convolutional models, clearly distillation helps when training models that are limited by architecture and/or number of parameters. Similarly, the student models we trained with 1, 2, 3, and 4 convolutional layers are, we believe, the most accurate convnets of those depths reported in the literature. For example, the ensemble teacher model in Ba and Caruana (2014) was an ensemble of four CNNs, each of which had 3 convolutional layers, but only achieved 89% accuracy, whereas the single student CNNs we train via distillation achieve accuracies above 90% with only 2 convolutional layers, and above 92% with 3 convolutional layers. The only other work we are aware of that achieves comparable high accuracy with non-convolutional MLPs is recent work by Lin et al. (2016). They train multi-layer Z-Lin networks, and use a powerful form of data augmentation based on deformations that we did not use.\nInterestingly, we noticed that mimic networks perform consistently worse when trained using dropout. This surprised us, and suggests that training student models on the soft-targets from a teacher provides significant regularization for the student models obviating the need for extra regularization methods such as dropout. This is consistent with the observation made by Ba and Caruana (2014) that student mimic models did not seem to overfit. Hinton et al. (2015) claim that soft targets convey more information per sample than Boolean hard targets. The also suggest that the dark knowledge in the soft targets for other classes further helped regularization, and that early stopping was unnecessary. Romero et al. (2015) extend distillation by using the intermediate representations learned by the teacher as hints to guide training deep students, and teacher confidences further help regularization by providing a measure of sample simplicity to the student, akin to curriculum learning. In other work, Pereyra et al. (2017) suggest that the soft targets provided by a teacher provide a form of confidence penalty that penalizes low entropy distributions and label smoothing, both of which improve regularization by maintaining a reasonable ratio between the logits of incorrect classes.\nZhang et al. (2016) question the traditional view of regularization in deep models. Although they do not discuss distillation, they suggest that in deep learning traditional function approximation appears to be deeply intertwined with massive memorization. The multiple soft targets used to train student models have a high information density (Hinton et al., 2015) and thus provide regularization by reducing the impact of brute-force memorization.\n\n5 CONCLUSIONS\nWe train shallow nets with and without convolution to mimic state-of-the-art deep convolutional nets. If one controls for the number of learnable parameters, nets containing a single fully-connected non-linear layer and no convolutional layers are not able to learn functions as accurate as deeper convolutional models. This result is consistent with those reported in Ba and Caruana (2014). However, we also find that shallow nets that contain only 1-2 convolutional layers also are unable to achieve accuracy comparable to deeper models if the same number of parameters are used in the shallow and deep models. Deep convolutional nets are significantly more accurate than shallow convolutional models, given the same parameter budget. We do, however, see evidence that model compression allows accurate models to be trained that are shallower and have fewer convolutional layers than the deep convolutional architectures needed to learn high-accuracy models from the original 1-hot hard-target training data. The question remains why extra layers are required to train accurate models from the original training data.\n\n6.1 DETAILS OF TRAINING THE TEACHER MODELS\nWeights of trained nets are initialized as in Glorot and Bengio (2010). The models trained in Section 2.7 contain eight convolutional layers organized into three groups (2-2-4) and two fully-connected hidden layers. The Bayesian hyperparameter optimization controls four constants C1, C2, C3, H1 all in the range [0, 1] that are then linearly transformed to the number of filters/neurons in each layer. The hyperparameters for which ranges were not shown in Section 2.7 are: the four separate dropout rates (DOc1,DOc2,DOc3,DOf) and the five constants Dh, Ds, Dv, As, Av controlling the HSV data augmentation. The ranges we selected are DOc1 ∈ [0.1, 0.3],DOc2 ∈ [0.25, 0.35],DOc3 ∈ [0.3, 0.44],DOf1 ∈ [0.2, 0.65],DOf2 ∈ [0.2, 0.65], Dh ∈ [0.03, 0.11], Ds ∈ [0.2, 0.3], Dv ∈ [0.0, 0.2], As ∈ [0.2, 0.3], Av ∈ [0.03, 0.2], partly guided by Snoek et al. (2015) and visual inspection of the resulting augmentations.\nThe number of filters and hidden units for the models have the following bounds: 1 conv. layer: 50 - 500 filters, 200 - 2000 hidden units, number of units in bottleneck is the dependent variable. 2 conv. layers: 50 - 500 filters, 100 - 400 filters, number of hidden units is the dependent variable. 3 conv. layers: 50 - 500 filters (layer 1), 100 - 300 filters (layers 2-3), # of hidden units is dependent the variable. 4 conv. layers: 50 - 300 filters (layers 1-2), 100 - 300 filters (layers 3-4), # of hidden units is the dependent variable.\nAll convolutional filters in the model are sized 3×3, max-pooling is applied over windows of 2×2 and we use ReLU units throughout all our models. We apply dropout after each max-pooling layer with the three rates DOc1,DOc2,DOc3 and after each of the two fully-connected layers with the same rate DOf .\n\n6.2 DETAILS OF TRAINING MODELS OF VARIOUS DEPTHS ON CIFAR-10 HARD 0/1 LABELS\nModels in the first four rows in Table 1 are trained similarly to those in Section 6.1, and are architecturally equivalent to the four convolutional student models shown in Table 2 with 10 million parameters. The following hyperparameters are optimized: initial learning rate [0.0015, 0.025] (optimized on a log scale), momentum [0.68, 0.97] (optimized on a log scale), constants C1, C2 ∈ [0, 1] that control the number of filters or neurons in different layers, and up to four different dropout rates DOc1 ∈ [0.05, 0.4],DOc2 ∈ [0.1, 0.6],DOc3 ∈ [0.1, 0.7],DOf1 ∈ [0.1, 0.7] for the different layers. Weight decay was set to 2 · 10−4 and we used the same data augmentation settings as for the student models. We use 5×5 convolutional filters, one nonlinear hidden layer in each model and each max-pooling operation is followed by dropout with a separately optimized rate. We use 2×2 max-pooling except in the model with only one convolutional layer where we apply 3×3 pooling as this seemed to boost performance and reduces the number of parameters.\n\n6.3 DETAILS OF TRAINING STUDENT MODELS OF VARIOUS DEPTHS ON ENSEMBLE LABELS\nOur student models have the same architecture as models in Section 6.2. The model without convolutional layers consists of one linear layer that acts as a bottleneck followed by a hidden layer of ReLU units. The following hyperparameters are optimized: initial learning rate [0.0013, 0.016] (optimized on a log scale), momentum [0.68, 0.97] (optimized on a log scale), input-scale ∈ [0.8, 1.25], global initialization scale (after initialization) ∈ [0.4, 2.0], layer-width constants C1, C2 ∈ [0, 1] that control the number of filters or neurons. The exact ranges for the number of filters and implicitly resulting number of hidden units was chosen for all twenty optimization experiments independently, as architectures, number of units and number of parameters strongly interact.\nFor the non-convolutional models we chose a slightly different hyper-parameterization. Given that all layers (in models with “two layers” or more) are nonlinear and fully connected we treat all of them similarly from the hyperparameter-optimizer’s point of view. In order to smoothly enforce the parameter budgets without rejecting any samples from the Bayesian optimizer we instead optimize the ratios of hidden units in each layer (numbers between 0 and 1), and then re-normalize and scale them to the final number of neurons in each layer to match the target parameter budget.\nFigure 2 is similar to 1 but includes preliminary results from experiments for models with 100M parameters. We are also running experiments with 300M parameters. Unfortunately, Bayesian optimization on models with 100M and 300M parameters is even more expensive than for the other points in the graph.\nAs expected, adding capacity to the convolutional students (top of the figure) modestly increases their accuracy. Preliminary results for the MLPs however (too preliminary to include in the graph) may not show the same increase in accuracy with increasing model size. Models with two or three hidden layers may benefit from adding capacity to each layer, but we have yet to see any benefit from adding capacity to the MLPs with four or five hidden layers.", "ground_truth": "ICLR 2017 conference submission\n\n---\n\nDo Deep Convolutional Nets Really Need to be Deep and Convolutional?\n\n---\n\nYes, they do. This paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained. Although previous research showed that shallow feed-forward nets sometimes can learn the complex functions previously learned by deep nets while using the same number of parameters as the deep models they mimic, in this paper we demonstrate that the same methods cannot be used to train accurate models on CIFAR-10 unless the student models contain multiple layers of convolution. Although the student models do not have to be as deep as the teacher model they mimic, the students need multiple convolutional layers to learn functions of comparable accuracy as the deep convolutional teacher.\n\n---\n\nThis paper describes a careful experimental study on the CIFAR-10 task that uses data augmentation and Bayesian hyperparameter optimization to train a large number of high-quality, deep convolutional network classification models from hard (0-1) targets. An ensemble of the 16 best models is then used as a teacher model in the distillation framework, where student models are trained to match the averaged logits from the teacher ensemble. Data augmentation and Bayesian hyperparameter optimization is also applied in the training of the student models. Both non-convolutional (MLP) and convolutional student models of varying depths and parameter counts are trained. Convolutional models with the same architecture and parameter count as some of the convolutional students are also trained using hard targets and cross-entropy loss. The experimental results show that convolutional students with only one or two convolutional layers are unable to match the results of students having more convolutional layers under the constraint that the number of parameters in all students is kept constant.\n\nPros\n+ This is a very thorough and well designed study that make use of the best existing tools to try to answer the question of whether or not deep convolutional models need both depth and convolution.\n+ It builds nicely on the preliminary results in Ba & Caruana, 2014.\n\nCons\n- It is difficult to prove a negative, as the authors admit. That said, this study is as convincing as possible given current theory and practice in deep learning.\n\nSection 2.2 should state that the logits are unnormalized log-probabilities (they don't include the log partition function).\n\nThe paper does not follow the ICLR citation style. Quoting from the template: \"When the authors or the publication are included in the sentence, the citation should not be in parenthesis (as in “See Hinton et al. (2006) for more information.”). Otherwise, the citation should be in parenthesis (as in “Deep learning shows promise to make progress towards AI (Bengio & LeCun, 2007).”).\"\n\nThere are a few minor issues with English usage and typos that should be cleaned up in the final manuscript.\n\nnecessary when training student models with more than 1 convolutional layers → necessary when training student models with more than 1 convolutional layer\n\nremaining 10,000 images as validation set → remaining 10,000 images as the validation set\n\nevaluate the ensemble’s predictions (logits) on these samples, and save all data → evaluated the ensemble’s predictions (logits) on these samples, and saved all data\n\nmore detail about hyperparamter optimization → more detail about hyperparameter optimization\n\nWe trained 129 deep CNN models with spearmint → We trained 129 deep CNN models with Spearmint\n\nThe best model obtained an accuracy of 92.78%, the fifth best achieved 92.67%. → The best model obtained an accuracy of 92.78%; the fifth best achieved 92.67%.\n\nthe sizes and architectures of three best models → the sizes and architectures of the three best models\n\nclearly suggests that convolutional is critical → clearly suggests that convolution is critical\n\nsimilarly from the hyperparameter-opimizer’s point of view → similarly from the hyperparameter-optimizer’s point of view\n\n---\n\nICLR committee final decision\n\n---\n\nICLR 2017 pcs\n\n---\n\nThe reviewers unanimously recommend accepting this paper.\n\n---\n\n06 Feb 2017\n\n---\n\nICLR 2017 conference AnonReviewer3\n\n---\n\nExperimental paper with interesting results. Well written. Solid experiments.\n\n---\n\nDescription.\nThis paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset where deep convolutional teacher networks are used to train shallow student networks using L2 regression on logit outputs. The results show that similar accuracy on the same parameter budget can be only obtained when multiple layers of convolution are used. \n\nStrong points.\n- The experiments are carefully done with thorough selection of hyperparameters. \n- The paper shows interesting results that go partially against conclusions from the previous work in this area (Ba and Caruana 2014).\n- The paper is well and clearly written.\n\nWeak points:\n- CIFAR is still somewhat toy dataset with only 10 classes. It would be interesting to see some results on a more challenging problem such as ImageNet. Would the results for a large number of classes be similar?\n\nOriginality:\n- This is mainly an experimental paper, but the question it asks is interesting and worth investigation. The experimental results are solid and provide new insights.\n\nQuality:\n- The experiments are well done.\n\nClarity:\n- The paper is well written and clear.\n\nSignificance:\n- The results go against some of the conclusions from previous work, so should be published and discussed.\n\nOverall:\nExperimental paper with interesting results. Well written. Solid experiments.\n\n---\n\n16 Dec 2016\n\n---\n\nICLR 2017 conference AnonReviewer4\n\n---\n\nExperimental comparison of shallow, deep, and (non)-convolutional architectures with a fixed parameter budget\n\n---\n\nThis paper aims to investigate the question if shallow non-convolutional networks can be as affective as deep convolutional ones for image classification, given that both architectures use the same number of parameters. \nTo this end the authors conducted a series of experiments on the CIFAR10 dataset.\nThey find that there is a significant performance gap between the two approaches, in favour of deep CNNs. \nThe experiments are well designed and involve a distillation training approach, and the results are presented in a comprehensive manner.\nThey also observe (as others have before) that student models can be shallower than the teacher model from which they are trained for comparable performance.\n\nMy take on these results is that they suggest that using (deep) conv nets is more effective, since this model class encodes a form of a-prori or domain knowledge that images exhibit a certain degree of translation invariance in the way they should be processed for high-level recognition tasks. The results are therefore perhaps not quite surprising, but not completely obvious either.\n\nAn interesting point on which the authors comment only very briefly is that among the non-convolutional architectures the ones using 2 or 3 hidden layers outperform those with 1, 4 or 5 hidden layers. Do you have an interpretation / hypothesis of why this is the case? It would be interesting to discuss the point a bit more in the paper.\n\nIt was not quite clear to me why were the experiments were limited to use 30M parameters at most. None of the experiments in Figure 1 seem to be saturated. Although the performance gap between CNN and MLP is large, I think it would be worthwhile to push the experiment further for the final version of the paper.\n\nThe authors state in the last paragraph that they expect shallow nets to be relatively worse in an ImageNet classification experiment. \nCould the authors argue why they think this to be the case? \nOne could argue that the much larger training dataset size could compensate for shallow and/or non-convolutional choices of the architecture. \nSince MLPs are universal function approximators, one could understand architecture choices as expressions of certain priors over the function space, and in a large-data regimes such priors could be expected to be of lesser importance.\nThis issue could for example be examined on ImageNet when varying the amount of training data.\nAlso, the much higher resolution of ImageNet images might have a non-trivial impact on the CNN-MLP comparison as compared to the results established on the CIFAR10 dataset.\n\nExperiments on a second data set would also help to corroborate the findings, demonstrating to what extent such findings are variable across datasets.\n\n---\n\n16 Dec 2016\n\n---\n\nICLR 2017 conference AnonReviewer1\n\n---\n\nCareful study proving, to the extent possible, a fascinating point\n\n---\n\nThis paper describes a careful experimental study on the CIFAR-10 task that uses data augmentation and Bayesian hyperparameter optimization to train a large number of high-quality, deep convolutional network classification models from hard (0-1) targets. An ensemble of the 16 best models is then used as a teacher model in the distillation framework, where student models are trained to match the averaged logits from the teacher ensemble. Data augmentation and Bayesian hyperparameter optimization is also applied in the training of the student models. Both non-convolutional (MLP) and convolutional student models of varying depths and parameter counts are trained. Convolutional models with the same architecture and parameter count as some of the convolutional students are also trained using hard targets and cross-entropy loss. The experimental results show that convolutional students with only one or two convolutional layers are unable to match the results of students having more convolutional layers under the constraint that the number of parameters in all students is kept constant.\n\nPros\n+ This is a very thorough and well designed study that make use of the best existing tools to try to answer the question of whether or not deep convolutional models need both depth and convolution.\n+ It builds nicely on the preliminary results in Ba & Caruana, 2014.\n\nCons\n- It is difficult to prove a negative, as the authors admit. That said, this study is as convincing as possible given current theory and practice in deep learning.\n\nSection 2.2 should state that the logits are unnormalized log-probabilities (they don't include the log partition function).\n\nThe paper does not follow the ICLR citation style. Quoting from the template: \"When the authors or the publication are included in the sentence, the citation should not be in parenthesis (as in “See Hinton et al. (2006) for more information.”). Otherwise, the citation should be in parenthesis (as in “Deep learning shows promise to make progress towards AI (Bengio & LeCun, 2007).”).\"\n\nThere are a few minor issues with English usage and typos that should be cleaned up in the final manuscript.\n\nnecessary when training student models with more than 1 convolutional layers → necessary when training student models with more than 1 convolutional layer\n\nremaining 10,000 images as validation set → remaining 10,000 images as the validation set\n\nevaluate the ensemble’s predictions (logits) on these samples, and save all data → evaluated the ensemble’s predictions (logits) on these samples, and saved all data\n\nmore detail about hyperparamter optimization → more detail about hyperparameter optimization\n\nWe trained 129 deep CNN models with spearmint → We trained 129 deep CNN models with Spearmint\n\nThe best model obtained an accuracy of 92.78%, the fifth best achieved 92.67%. → The best model obtained an accuracy of 92.78%; the fifth best achieved 92.67%.\n\nthe sizes and architectures of three best models → the sizes and architectures of the three best models\n\nclearly suggests that convolutional is critical → clearly suggests that convolution is critical\n\nsimilarly from the hyperparameter-opimizer’s point of view → similarly from the hyperparameter-optimizer’s point of view\n\n---\n\n14 Dec 2016\n\n---\n\ndeep, but not too deep ?\n\n---\n\nICLR 2017 conference AnonReviewer4\n\n---\n\n11 Dec 2016\n\n---\n\nRegularization effect of distillation\n\n---\n\nICLR 2017 conference AnonReviewer1\n\n---\n\n01 Dec 2016\n\n---\n\nGregor Urban, Krzysztof J. Geras, Samira Ebrahimi Kahou, Ozlem Aslan, Shengjie Wang, Abdelrahman Mohamed, Matthai Philipose, Matt Richardson, Rich Caruana\n\n---\n\nThis paper provides the first empirical demonstration that deep convolutional models really need to be both deep and convolutional, even when trained with model distillation and heavy hyperparameter optimization."}
{"paper_id": "464", "paper_text": "ABSTRACT: We use reinforcement learning to learn tree-structured neural networks for computing representations of natural language sentences. In contrast with prior work on tree-structured models, in which the trees are either provided as input or predicted using supervision from explicit treebank annotations, the tree structures in this work are optimized to improve performance on a downstream task. Experiments demonstrate the benefit of learning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations. We analyze the induced trees and show that while they discover some linguistically intuitive structures (e.g., noun phrases, simple verb phrases), they are different than conventional English syntactic structures.\n\n1 INTRODUCTION\nLanguages encode meaning in terms of hierarchical, nested structures on sequences of words (Chomsky, 1957). However, the degree to which neural network architectures that compute representations of the meaning of sentences for practical applications should explicitly reflect such structures is a matter for debate. In this work, we use reinforcement learning to learn to construct trees for computing sentence representations, guided by feedback from downstream tasks that depend on these representations. The space of structures that are considered by the learner includes both fully sequential structures (corresponding to traditional recurrent neural network “encoders”), as well as all projective binary trees. Thus, although we take seriously the notion that good compositional architectures might be tree-structured, we specify neither the form of the tree nor whether a tree is necessary at all, and instead leave those decisions up to the learner (and the data).\nTo place this work in context, there are three predominant approaches for constructing vector representations of sentences from a sequence of words. The first composes words sequentially using a recurrent neural network, treating the RNN’s final hidden state as the representation of the sentence (Cho et al., 2014; Sutskever et al., 2014; Kiros et al., 2015). In such models, there is no explicit hierarchical organization imposed on the words, and the RNN’s dynamics must learn to simulate it. The second approach uses tree-structured networks to recursively compose representations of words and phrases to form representations of larger phrases and, finally, the complete sentence. In contrast to sequential models, these models’ architectures are organized according to each sentence’s syntactic structure, that is, the hierarchical organization of words into nested phrases that characterizes human intuitions about how words combine to form grammatical sentences. Prior work on tree-structured models has assumed that trees are either provided together with the input sentences (Clark et al., 2008; Grefenstette & Sadrzadeh, 2011; Socher et al., 2012; 2013; Tai et al., 2015) or that they are predicted based on explicit treebank annotations jointly with the downstream task (Bowman et al., 2016; Dyer et al., 2016). The last approach for constructing sentence representations uses convolutional neural networks to produce the representation in a bottom up manner, either with syntactic information (Ma et al., 2015) or without (Kim, 2014; Kalchbrenner et al., 2014).\nOur work can be understood as a compromise between the first two approaches. Rather than using explicit supervision of tree structure, we use reinforcement learning to learn tree structures (and thus, sentence-specific compositional architectures), taking performance on a downstream task that uses the computed sentence representation as the reward signal. In contrast to sequential RNNs, which ignore tree structure, our model still generates a latent tree for each sentence and uses it to\nstructure the composition. Our hypothesis is that encouraging the model to learn tree-structured compositions will bias the model toward better generalizations about how words compose to form sentence meanings, leading to better performance on downstream tasks.\nThis work is related to unsupervised grammar induction (Klein & Manning, 2004; Blunsom & Cohn, 2010; Spitkovsky et al., 2011, inter alia), which seeks to infer a generative grammar of an infinite language from a finite sample of strings from the language—but without any semantic feedback. Previous work on unsupervised grammar induction that incorporates semantic supervision involves designing complex models for Combinatory Categorial Grammars (Zettlemoyer & Collins, 2005) or marginalizing over latent syntactic structures (Naradowsky et al., 2012). Since semantic feedback has been proposed as crucial for the acquisition of syntax (Pinker, 1984), our model offers a simpler alternative.1 However, our primary focus is on improving performance on the downstream model, so the learner may settle on a different solution than conventional English syntax. We thus also explore what kind of syntactic structures are derivable from shallow semantics.\nExperiments on various tasks (i.e., sentiment analysis, semantic relatedness, natural language inference, and sentence generation) show that reinforcement learning is a promising direction to discover hierarchical structures of sentences. Notably, representations learned this way outperformed both conventional left-to-right models and tree-structured models based on linguistic syntax in downstream applications. This is in line with prior work showing the value of learning tree structures in statistical machine translation models (Chiang, 2007). Although the induced tree structures manifested a number of linguistically intuitive structures (e.g., noun phrases, simple verb phrases), there are a number of marked differences to conventional analyses of English sentences (e.g., an overall left-branching structure).\n\n2 MODEL\nOur model consists of two components: a sentence representation model and a reinforcement learning algorithm to learn the tree structure that is used by the sentence representation model.\n\n2.1 TREE LSTM\nOur sentence representation model follows the Stack-augmented Parser-Interpreter Neural Network (SPINN; Bowman et al., 2016), SPINN is a shift-reduce parser that uses Long Short-Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) as its composition function. Given an input sentence of N words x = {x1, x2, . . . , xN}, we represent each word by its embedding vector xi ∈ RD. The parser maintains an index pointer p starting from the leftmost word (p = 1) and a stack. To parse the sentence, it performs a sequence of operations a = {a1, a2, . . . , a2N−1}, where at ∈ {SHIFT, REDUCE}. A SHIFT operation pushes xp to the stack and moves the pointer to the next word (p++); while a REDUCE operation pops two elements from the stack, composes them to a single element, and pushes it back to the stack. SPINN uses Tree LSTM (Tai et al., 2015; Zhu et al., 2015) as the REDUCE composition function, which we follow. In Tree LSTM, each element of the stack is represented by two vectors, a hidden state representation h and a memory representation c. Two elements of the stack (hi, ci) and (hj , cj) are composed as:\ni = σ(WI [hi,hj ] + bI) o = σ(WO[hi,hj ] + bI)\nfL = σ(WFL [hi,hj ] + bFL) fR = σ(WFR [hi,hj ] + bFR)\ng = tanh(WG[hi,hj ] + bG) c = fL ci + fR cj + i g h = o c\n(1)\nwhere [hi,hj ] denotes concatenation of hi and hj , and σ is the sigmoid activation function.\nA unique sequence of {SHIFT, REDUCE} operations corresponds to a unique binary parse tree of the sentence. A SHIFT operation introduces a new leaf node in the parse tree, while a REDUCE operation combines two nodes by merging them into a constituent. See Figure 1 for an example. We note that for a sentence of lengthN , there are exactlyN SHIFT operations andN−1 REDUCE operations that are needed to produce a binary parse tree of the sentence. The final sentence representation produced\n1Our model only produces an interpretation grammar that parses language instead of a generative grammar.\nby the Tree LSTM is the hidden state of the final element of the stack hN−1 (i.e., the topmost node of the tree).\nTracking LSTM. SPINN optionally augments Tree LSTM with another LSTM that incorporates contextual information in sequential order called tracking LSTM, which has been shown to improve performance for textual entailment. It is a standard recurrent LSTM network that takes as input the hidden states of the top two elements of the stack and the embedding vector of the word indexed by the pointer at timestep t. Every time a REDUCE operation is performed, the output of the tracking LSTM e is included as an additional input in Eq. 1 (i.e., the input to the REDUCE composition function is [hi,hj , e] instead of [hi,hj ]).\n\n2.2 REINFORCEMENT LEARNING\nIn previous work (Tai et al., 2015; Bowman et al., 2016), the tree structures that guided composition orders of Tree LSTM models are given directly as input (i.e., a is observed and provided as an input). Formally, each training data is a triplet {x,a,y}. Tai et al. (2015) consider models where a is also given at test time, whereas Bowman et al. (2016) explore models where a can be either observed or not at test time. When it is only observed during training, a policy is trained to predict a at test time. Note that in this case the policy is trained to match explicit human annotations (i.e., Penn TreeBank annotations), so the model learns to optimize representations according to structures that follows human intuitions. They found that models that observe a at both training and test time are better than models that only observe a during training.\nOur main idea is to use reinforcement learning (policy gradient methods) to discover the best tree structures for the task that we are interested in. We do not place any kind of restrictions when learning these structures other than that they have to be valid binary parse trees, so it may result in tree structures that match human linguistic intuition, heavily right or left branching, or other solutions if they improve performance on the downstream task.\nWe parameterize each action a ∈ {SHIFT, REDUCE} by a policy network π(a | s;WR), where s is a representation of the current state and WR is the parameter of the network. Specifically, we use a two-layer feedforward network that takes the hidden states of the top two elements of the stack hi and hj and the embedding vector of the word indexed by the pointer xp as its input:\ns = ReLU(W1R[hi,hj,xp] + b 1 R) such that π(a | s;WR) ∝ exp(w2>R s+ b2R)\nwhere [hi,hj ,xp] denotes concatenation of vectors inside the brackets.\nIf a is given as part of the training data, the policy network can be trained—in a supervised training regime—to predict actions that result in trees that match human intuitions. Our training data, on the other hand, is a tuple {x,y}. We use REINFORCE (Williams, 1992), which is an instance of a broader class of algorithms called policy gradient methods, to learn WR such that the sequence of actions a = {a1, . . . , aT } maximizes:\nR(W) = Eπ(a,s;WR) [ T∑ t=1 rtat ] ,\nwhere rt is the reward at timestep t. We use performance on a downstream task as the reward function. For example, if we are interested in using the learned sentence representations in a classification task, our reward function is the probability of predicting the correct label using a sentence representation composed in the order given by the sequence of actions sampled from the policy network, so R(W) = log p(y | T-LSTM(x);W), where we use W to denote all model parameters (Tree LSTM, policy network, and classifier parameters), y is the correct label for input sentence x, and x is represented by the Tree LSTM structure in §2.1. For a natural language generation task where the goal is to predict the next sentence given the current sentence, we can use the probability of predicting words in the next sentence as the reward function, so R(W) = log p(xs+1 | T-LSTM(xs);W). Note that in our setup, we do not immediately receive a reward after performing an action at timestep t. The reward is only observed at the end after we finish creating a representation for the current sentence with Tree LSTM and use the resulting representation for the downstream task. At each timestep t, we sample a valid action according to π(a | s;WR). We add two simple constraints to make the sequence of actions result in a valid tree: REDUCE is forbidden if there are fewer than two elements on the stack, and SHIFT is forbidden if there are no more words to read from the sentence. After reaching timestep 2N − 1, we construct the final representation and receive a reward that is used to update our model parameters.\nWe experiment with two learning methods: unsupervised structures and semi-supervised structures. Suppose that we are interested in a classification task. In the unsupervised case, the objective function that we maximize is log p(y | T-LSTM(x);W). In the semi-supervised case, the objective function for the first E epochs also includes a reward term for predicting the correct SHIFT or REDUCE actions obtained from an external parser—in addition to performance on the downstream task, so we maximize log p(y | T-LSTM(x);W) + log π(a | s;WR). The motivation behind this model is to first guide the model to discover tree structures that match human intuitions, before letting it explore other structures close to these ones. After epochE, we remove the second term from our objective function and continue maximizing the first term. Note that unsupervised and semi-supervised here refer to the tree structures, not the nature of the downstream task.\n\n3.1 BASELINES\nThe goal of our experiments is to evaluate our hypothesis that we can discover useful task-specific tree structures (composition orders) with reinforcement learning. We compare the following composition methods (the last two are unique to our work):\n• Right to left: words are composed from right to left.2\n• Left to right: words are composed from left to right. This is the standard recurrent neural network composition order.\n• Bidirectional: A bidirectional right to left and left to right models, where the final sentence embedding is an average of sentence embeddings produced by each of these models.\n• Balanced binary tree: words are composed according to a balanced binary parse tree of the sentence.\n• Supervised syntax: words are composed according to a predefined parse tree of the sentence. When parse tree information is not included in the dataset, we use Stanford parser (Klein & Manning, 2003) to parse the corpus.\n• Semi-supervised syntax: a variant of our reinforcement learning method, where for the first E epochs we include rewards for predicting predefined parse trees given in the supervised model, before letting the model explore other kind of tree structures at later epochs (i.e., semi-supervised structures in §2.2).\n• Latent syntax: another variant of our reinforcement learning method where there is no predefined structures given to the model at all (i.e., unsupervised structures in §2.2).\n2We choose to include right to left as a baseline since a right-branching tree structure—which is the output of a right to left composition order—has been shown to be a reliable baseline for unsupervised grammar induction (Klein & Manning, 2004).\nFor learning, we use stochastic gradient descent with minibatches of size 1 and `2 regularization constant tune on development data from {10−4, 10−5, 10−6, 0}. We use performance on development data to choose the best model and decide when to stop training.\n\n3.2 TASKS\nWe evaluate our method on four sentence representation tasks: sentiment classification, semantic relatedness, natural language inference (entailment), and sentence generation. We show statistics of the datasets in Table 1 and describe each task in detail in this subsection.\nStanford Sentiment Treebank. We evaluate our model on a sentiment classification task from the Stanford Sentiment Treebank (Socher et al., 2013). We use the binary classification task where the goal is to predict whether a sentence is a positive or a negative movie review.\nWe set the word embedding size to 100 and initialize them with Glove vectors (Pennington et al., 2014)3. For each sentence, we create a 100-dimensional sentence representation s ∈ R100 with Tree LSTM, project it to a 200-dimensional vector and apply ReLU: q = ReLU(Wps + bp), and compute p(ŷ = c | q;wq) ∝ exp(wq,cq+ bq). We run each model 3 times (corresponding to 3 different initialization points) and use the development data to pick the best model. We show the results in Table 2. Our results agree with prior work that have shown the benefits of using syntactic parse tree information on this dataset (i.e., supervised recursive model is generally better than sequential models). The best model is the latent syntax model, which is also competitive with results from other work on this dataset. Both the latent and semi-supervised syntax models outperform models with predefined structures, demonstrating the benefit of learning task-specific composition orders.\nSemantic relatedness. The second task is to predict the degree of relatedness of two sentences from the Sentences Involving Compositional Knowledge corpus (SICK; Marelli et al., 2014) . In this dataset, each pair of sentences are given a relatedness score on a 5-point rating scale. For each sentence, we use Tree LSTM to create its representations. We denote the final representations by {s1, s2} ∈ R100. We construct our prediction by computing: u = (s2 − s1)2, v = s1 s2, q = ReLU(Wp[u,v] + bp), and ŷ = w>q q + bq , where Wp ∈ R200×200,bp ∈ R200,wq ∈ R200, bq ∈ R1 are model parameters, and [u,v] denotes concatenation of vectors inside the brackets. We learn the model to minimize mean squared error.\nWe run each model 5 times and use the development data to pick the best model. Our results are shown in Table 3. Similarly to the previous task, they clearly demonstrate that learning the tree structures yields better performance.\nWe also provide results from other work on this dataset for comparisons. Some of these models (Lai & Hockenmaier, 2014; Jimenez et al., 2014; Bjerva et al., 2014) rely on feature engineering and are designed specifically for this task. Our Tree LSTM implementation performs competitively with most models in terms of mean squared error. Our best model—semi-supervised syntax—is better than most models except LSTM models of Tai et al. (2015) which were trained with a different objective function.4 Nonetheless, we observe the same trends with their results that show the benefit of using syntactic information on this dataset.\nStanford Natural Language Inference. We next evaluate our model for natural language inference (i.e., recognizing textual entailment) using the Stanford Natural Language Inference corpus (SNLI; Bowman et al., 2015) . Natural language inference aims to predict whether two sentences are entailment, contradiction, or neutral, which can be formulated as a three-way classification problem. Given a pair of sentences, similar to the previous task, we use Tree LSTM to create sentence representations {s1, s2} ∈ R100 for each of the sentences. Following Bowman et al. (2016), we construct our prediction by computing: u = (s2−s1)2, v = s1 s2, q = ReLU(Wp[u,v, s1, s2]+bp), and p(ŷ = c | q;wq) ∝ exp(wq,cq+ bq), where Wp ∈ R200×400,bp ∈ R200,wq ∈ R200, bq ∈ R1 are model parameters. The objective function that we maximize is the log likelihood of the correct label under the models.\nWe show the results in Table 4. The latent syntax method performs the best. Interestingly, the sequential left to right model is better than the supervised recursive model in our experiments, which contradicts results from Bowman et al. (2016) that show 300D-LSTM is worse than 300D-SPINN. A possible explanation is that our left to right model has identical number of parameters with the supervised model due to the inclusion of the tracking LSTM even in the left to right model (the only difference is in the composition order), whereas the models in Bowman et al. (2016) have\n4Our experiments with the regularized KL-divergence objective function (Tai et al., 2015) do not result in significant improvements, so we choose to report results with the simpler mean squared error objective function.\ndifferent number of parameters. Due to the poor performance of the supervised model relative to the unsupervised model, semi-supervised training can only mitigate the loss in accuracy, rather than improve over unsupervised learning. Our models underperform state-of-the-art models on this dataset that have almost four times the number of parameters. We only experiment with smaller models since tree-based models with dynamic structures (e.g., our semi-supervised and latent syntax models) take longer to train. See §4 for details and discussions about training time.\nSentence generation. The last task that we consider is natural language generation. Given a sentence, the goal is to maximize the probability of generating words in the following sentence. This is a similar setup to the Skip Thought objective (Kiros et al., 2015), except that we do not generate the previous sentence as well. Given a sentence, we encode it with Tree LSTM to obtain s ∈ R100. We use a bag-of-words model as our decoder, so p(wi | s;V) ∝ exp(v>i s), where V ∈ R100×29,209 and vi ∈ R100 is the i-th column of V. Using a bag-of-words decoder as opposed to a recurrent neural network decoder increases the importance of producing a better representation of the current sentence, since the model cannot rely on a sophisticated decoder with a language model component to predict better. This also greatly speeds up our training time.\nWe use IMDB movie review corpus (Diao et al., 2014) for this experiment, The corpus consists of 280,593, 33,793, and 34,029 reviews in training, development, and test sets respectively. We construct our data using the development and test sets of this corpus. For training, we process 33,793 reviews from the original development set to get 441,617 pairs of sentences. For testing, we use 34,029 reviews in the test set (446,471 pairs of sentences). Half of these pairs is used as our development set to tune hyperparamaters, and the remaining half is used as our final test set. Our results in Table 5 further demonstrate that methods that learn tree structures perform better than methods that have fixed structures.\n\n4 DISCUSSION\nLearned Structures. Our results in §3 show that our proposed method outperforms competing methods with predefined composition order on all tasks. The right to left model tends to perform worse than the left to right model. This suggests that the left to right composition order, similar to how human reads in practice, is better for neural network models. Our latent syntax method is able to discover tree structures that work reasonably well on all tasks, regardless of whether the task is better suited for a left to right or supervised syntax composition order.\nWe inspect what kind of structures the latent syntax model learned and how closely they match human intuitions. We first compute unlabeled bracketing F1 scores5 for the learned structures and parses given by Stanford parser on SNLI and Stanford Sentiment Treebank. In the SNLI dataset, there are 10,000 pairs of test sentences (20,000 sentences in total), while the Stanford Sentiment Treebank test set contains 1,821 test sentences. The F1 scores for the two datasets are 41.73 and 40.51 respectively. For comparisons, F1 scores of a right (left) branching tree are 19.94 (41.37) for SNLI and 12.96 (38.56) for SST.\nWe also manually inspect the learned structures. We observe that in SNLI, the trees exhibit overall left-branching structure, which explains why the F1 scores are closer to a left branching tree structure. Note that in our experiments on this corpus, the supervised syntax model does not perform as well as the left-to-right model, which suggests why the latent syntax model tends to converge towards the left-to-right model. We handpicked two examples of trees learned by our model and show them in Figure 2. We can see that in some cases the model is able to discover concepts such as noun phrases (e.g., a boy, his sleds) and simple verb phrases (e.g., wearing sunglasses, is frowning). Of course, the model sometimes settles on structures that make little sense to humans. We show two such examples in Figure 3, where the model chooses to compose playing frisbee in and outside a as phrases.\nTraining Time. A major limitation of our proposed model is that it takes much longer to train compared to models with predefined structures. We observe that our models only outperforms models with fixed structures after several training epochs; and on some datasets such as SNLI or IMDB, an epoch could take a 5-7 hours (we use batch size 1 since the computation graph needs to be reconstructed for every example at every iteration depending on the samples from the policy network). This is also the main reason that we could only use smaller 100-dimensional Tree LSTM models in\n5We use evalb toolkit from http://nlp.cs.nyu.edu/evalb/.\nall our experiments. While for smaller datasets such as SICK the overall training time is approximately 6 hours, for SNLI or IMDB it takes 3-4 days for the model to reach convergence. In general, the latent syntax model and semi-supervised syntax models take about two or three times longer to converge compared to models with predefined structures.\n\n5 CONCLUSION\nWe presented a reinforcement learning method to learn hierarchical structures of natural language sentences. We demonstrated the benefit of learning task-specific composition order on four tasks: sentiment analysis, semantic relatedness, natural language inference, and sentence generation. We qualitatively and quantitatively analyzed the induced trees and showed that they both incorporate some linguistically intuitive structures (e.g., noun phrases, simple verb phrases) and are different than conventional English syntactic structures.", "ground_truth": "ICLR 2017 conference submission\n\n---\n\nLearning to Compose Words into Sentences with Reinforcement Learning\n\n---\n\nWe use reinforcement learning to learn tree-structured neural networks for computing representations of natural language sentences. In contrast with prior work on tree-structured models, in which the trees are either provided as input or predicted using supervision from explicit treebank annotations, the tree structures in this work are optimized to improve performance on a downstream task. Experiments demonstrate the benefit of learning task-specific composition orders, outperforming both sequential encoders and recursive encoders based on treebank annotations. We analyze the induced trees and show that while they discover some linguistically intuitive structures (e.g., noun phrases, simple verb phrases), they are different than conventional English syntactic structures.\n\n---\n\nIn this paper, the authors propose a new method to learn hierarchical representations of sentences, based on reinforcement learning. They propose to learn a neural shift-reduce parser, such that the induced tree structures lead to good performance on a downstream task. They use reinforcement learning (more specifically, the policy gradient method REINFORCE) to learn their model. The reward of the algorithm is the evaluation metric of the downstream task. The authors compare two settings, (1) no structure information is given (hence, the only supervision comes from the downstream task) and (2) actions from an external parser is used as supervision to train the policy network, in addition to the supervision from the downstream task. The proposed approach is evaluated on four tasks: sentiment analysis, semantic relatedness, textual entailment and sentence generation.\n\nI like the idea of learning tree representations of text which are useful for a downstream task. The paper is clear and well written. However, I am not convinced by the experimental results presented in the paper. Indeed, on most tasks, the proposed model is far from state-of-the-art models:\n - sentiment analysis, 86.5 v.s. 89.7 (accuracy);\n - semantic relatedness, 0.32 v.s. 0.25 (MSE);\n - textual entailment, 80.5 v.s. 84.6 (accuracy).\nFrom the results presented in the paper, it is hard to know if these results are due to the model, or because of the reinforcement learning algorithm.\n\nPROS:\n - interesting idea: learning structures of sentences adapted for a downstream task.\n - well written paper.\nCONS:\n - weak experimental results (do not really support the claim of the authors).\n\nMinor comments:\nIn the second paragraph of the introduction, one might argue that bag-of-words is also a predominant approach to represent sentences.\nParagraph titles (e.g. in section 3.2) should have a period at the end.\n\n----------------------------------------------------------------------------------------------------------------------\nUPDATE\n\nI am still not convinced by the results presented in the paper, and in particular by the fact that one must combine the words in a different way than left-to-right to obtain state of the art results.\nHowever, I do agree that this is an interesting research direction, and that the results presented in the paper are promising. I am thus updating my score from 5 to 6.\n\n---\n\nICLR committee final decision\n\n---\n\nICLR 2017 pcs\n\n---\n\nAll the reviewers agreed that the research direction is very interesting, and generally find the results promising. We could quibble a bit about the results not being really state-of-the-art and the choice of baselines, but I think the main claims are well supported by the experiments (i.e. the induce grammar appears to be useful for the problem in question, within the specific class of models). There are still clearly many issues unresolved, and we are yet to see if this class of methods (RL / implicit structure-based) can lead to state-of-the-art results on any important NLP problem. But it is too much to ask from a submission. I see the paper as a strong contribution to the conference.\n\n---\n\n06 Feb 2017\n\n---\n\nICLR 2017 conference AnonReviewer3\n\n---\n\nAccept\n\n---\n\nI have not much to add to my pre-review comments.\nIt's a very well written paper with an interesting idea.\nLots of people currently want to combine RL with NLP. It is very en vogue.\nNobody has gotten that to work yet in any really groundbreaking or influential way that results in actually superior performance on any highly relevant or competitive NLP task.\nMost people struggle with the fact that NLP requires very efficient methods on very large datasets and RL is super slow.\nHence, I believe this direction hasn't shown much promise yet and it's not yet clear it ever will due to the slowness of RL.\nBut many directions need to be explored and maybe eventually they will reach a point where they become relevant.\n\nIt is interesting to learn the obviously inherent grammatical structure in language though sadly again, the trees here do not yet capture much of what our intuitions are.\n\nRegardless, it's an interesting exploration, worthy of being discussed at the conference.\n\n---\n\n24 Dec 2016\n\n---\n\nICLR 2017 conference AnonReviewer2\n\n---\n\nofficial review\n\n---\n\nThe paper proposes to use reinforcement learning to learn how to compose the words in a sentence, i.e. parse tree, that can be helpful for the downstream tasks. To do that, the shift-reduce framework is employed and RL is used to learn the policy of the two actions SHIFT and REDUCE. The experiments on four datasets (SST, SICK, IMDB, and SNLI) show that the proposed approach outperformed the approach using predefined tree structures (e.g. left-to-right, right-to-left). \n\nThe paper is well written and has two good points. Firstly, the idea of using RL to learn parse trees using downstream tasks is very interesting and novel. And employing the shift-reduce framework is a very smart choice because the set of actions is minimal (shift and reduce). Secondly, what shown in the paper somewhat confirms the need of parse trees. This is indeed interesting because of the current debate on whether syntax is helpful.\n\nI have the following comments:\n- it seems that the authors weren't aware of some recent work using RL to learn structures for composition, e.g. Andreas et al (2016).\n- because different composition functions (e.g. LSTM, GRU, or classical recursive neural net) have different inductive biases, I was wondering if the tree structures found by the model would be independent from the composition function choice.\n- because RNNs in theory are equivalent to Turing machines, I was wondering if restricting the expressiveness of the model (e.g. reducing the dimension) can help the model focus on discovering more helpful tree structures.\n\nRef:\nAndreas et al. Learning to Compose Neural Networks for Question Answering. NAACL 2016\n\n---\n\n19 Dec 2016\n\n---\n\nICLR 2017 conference AnonReviewer1\n\n---\n\nWeak experimental results\n\n---\n\nIn this paper, the authors propose a new method to learn hierarchical representations of sentences, based on reinforcement learning. They propose to learn a neural shift-reduce parser, such that the induced tree structures lead to good performance on a downstream task. They use reinforcement learning (more specifically, the policy gradient method REINFORCE) to learn their model. The reward of the algorithm is the evaluation metric of the downstream task. The authors compare two settings, (1) no structure information is given (hence, the only supervision comes from the downstream task) and (2) actions from an external parser is used as supervision to train the policy network, in addition to the supervision from the downstream task. The proposed approach is evaluated on four tasks: sentiment analysis, semantic relatedness, textual entailment and sentence generation.\n\nI like the idea of learning tree representations of text which are useful for a downstream task. The paper is clear and well written. However, I am not convinced by the experimental results presented in the paper. Indeed, on most tasks, the proposed model is far from state-of-the-art models:\n - sentiment analysis, 86.5 v.s. 89.7 (accuracy);\n - semantic relatedness, 0.32 v.s. 0.25 (MSE);\n - textual entailment, 80.5 v.s. 84.6 (accuracy).\nFrom the results presented in the paper, it is hard to know if these results are due to the model, or because of the reinforcement learning algorithm.\n\nPROS:\n - interesting idea: learning structures of sentences adapted for a downstream task.\n - well written paper.\nCONS:\n - weak experimental results (do not really support the claim of the authors).\n\nMinor comments:\nIn the second paragraph of the introduction, one might argue that bag-of-words is also a predominant approach to represent sentences.\nParagraph titles (e.g. in section 3.2) should have a period at the end.\n\n----------------------------------------------------------------------------------------------------------------------\nUPDATE\n\nI am still not convinced by the results presented in the paper, and in particular by the fact that one must combine the words in a different way than left-to-right to obtain state of the art results.\nHowever, I do agree that this is an interesting research direction, and that the results presented in the paper are promising. I am thus updating my score from 5 to 6.\n\n---\n\n16 Dec 2016 (modified: 21 Jan 2017)\n\n---\n\nimplementation\n\n---\n\nICLR 2017 conference AnonReviewer2\n\n---\n\nIt was slow to train the model because you had to build a different new computational graph for each sentence. I was wondering if the slow speed is due to the way you implemented (e.g. tensorflow, python). Dynet is designed especially for this kind of problem, did you try it?\n\n---\n\n11 Dec 2016\n\n---\n\nExperiments on SST\n\n---\n\nICLR 2017 conference AnonReviewer2\n\n---\n\n11 Dec 2016\n\n---\n\nPrevious work on semi-supervised binary tree constructions\n\n---\n\nKazuma Hashimoto\n\n---\n\nDear authors,\n\nI've read the interesting paper and learned nice ideas.\n\nNow I've found somewhat incorrect mention in your paper.\nIn Introduction, it is said that trees are provided with sentences in Socher et al. (2011), but they jointly learn the binary tree structures according to the target task (sentiment classification) although the approach is different (reinforcement learning or autoencoder).\n\nBest,\n Kazuma\n\n---\n\n09 Dec 2016\n\n---\n\nExperiments\n\n---\n\nICLR 2017 conference AnonReviewer1\n\n---\n\n03 Dec 2016\n\n---\n\nFeedback\n\n---\n\nICLR 2017 conference AnonReviewer3\n\n---\n\n02 Dec 2016\n\n---\n\nWhy is the current \"supervised syntax\" performing worse than older \"supervised syntax\" baselines?\n\n---\n\n(anonymous)\n\n---\n\nIn several of the evaluation, the \"constituency tree LSTM\" and/or \"dependency tree LSTM\" methods perform much better than all of your proposed models, including the \"supervised syntax\" model, and sometimes even with the same number of parameters. What is the difference between your \"supervised syntax\" method and these tree LSTMs? why doesn't the supervised syntax approach perform at least as good as the Tai et al models?\n\n---\n\n21 Nov 2016\n\n---\n\ncomparison vs NTI\n\n---\n\n(anonymous)\n\n---\n\nBig fan of this work. Related to the above comment, have you guys tried a simple baseline where you just assume a binary tree? I imagine it will do better than left-to-right and right-to-left (given the success of NTI), but will probably do worse than the proposed method.\n\n---\n\n11 Nov 2016\n\n---\n\nResults on Stanford Sentiment Treebank\n\n---\n\nTsendsuren Munkhdalai\n\n---\n\nTable 2 is missing some recent results on this task. Please see the NTI and NSE results on the same task [1,2]. NTI is particularly relevant to this work because it encodes a sentence with an n-ary tree (i.e. binary tree) instead of using a parser output or learning to compose.\n\nThanks,\n\nRef:\n\n1. Munkhdalai, Tsendsuren, and Hong Yu. \"Neural Tree Indexers for Text Understanding.\" arXiv preprint arXiv:1607.04492 (2016).\n2. Munkhdalai, Tsendsuren, and Hong Yu. \"Neural Semantic Encoders.\" arXiv preprint arXiv:1607.04315 (2016).\n\n---\n\n09 Nov 2016\n\n---\n\nDani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, Wang Ling"}
{"paper_id": "407", "paper_text": "TITLE: TRANSFER FROM MULTIPLE SOURCES IN THE SAME DOMAIN\n\nABSTRACT: Transferring knowledge from prior source tasks in solving a new target task can be useful in several learning applications. The application of transfer poses two serious challenges which have not been adequately addressed. First, the agent should be able to avoid negative transfer, which happens when the transfer hampers or slows down the learning instead of helping it. Second, the agent should be able to selectively transfer, which is the ability to select and transfer from different and multiple source tasks for different parts of the state space of the target task. We propose A2T (Attend, Adapt and Transfer), an attentive deep architecture which adapts and transfers from these source tasks. Our model is generic enough to effect transfer of either policies or value functions. Empirical evaluations on different learning algorithms show that A2T is an effective architecture for transfer by being able to avoid negative transfer while transferring selectively from multiple source tasks in the same domain.\n\nTransferring knowledge from prior source tasks in solving a new target task can be useful in several learning applications. The application of transfer poses two serious challenges which have not been adequately addressed. First, the agent should be able to avoid negative transfer, which happens when the transfer hampers or slows down the learning instead of helping it. Second, the agent should be able to selectively transfer, which is the ability to select and transfer from different and multiple source tasks for different parts of the state space of the target task. We propose A2T (Attend, Adapt and Transfer), an attentive deep architecture which adapts and transfers from these source tasks. Our model is generic enough to effect transfer of either policies or value functions. Empirical evaluations on different learning algorithms show that A2T is an effective architecture for transfer by being able to avoid negative transfer while transferring selectively from multiple source tasks in the same domain.\n\n1 INTRODUCTION\nOne of the goals of Artificial Intelligence (AI) is to build autonomous agents that can learn and adapt to new environments. Reinforcement Learning (RL) is a key technique for achieving such adaptability. The goal of RL algorithms is to learn an optimal policy for choosing actions that maximize some notion of long term performance. Transferring knowledge gained from tasks solved earlier to solve a new target task can help, either in terms of speeding up the learning process or in terms of achieving a better solution, among other performance measures. When applied to RL, transfer could be accomplished in many ways (see Taylor & Stone (2009; 2011) for a very good survey of the field). One could use the value function from the source task as an initial estimate in the target task to cut down exploration [Sorg & Singh (2009)]. Alternatively one could use policies from the source task(s) in the target task. This can take one of two forms - (i) the derived policies can be used as initial exploratory trajectories [Atkeson & Schaal (1997); Niekum et al. (2013)] in the target task and (ii) the derived policy could be used to define macro-actions which may then be used by the agent in solving the target task [Mannor et al. (2004); Brunskill & Li (2014)].\n∗Authors contributed equally\nWhile transfer in RL has been much explored, there are two crucial issues that have not been adequately addressed in the literature. The first is negative transfer, which occurs when the transfer results in a performance that is worse when compared to learning from scratch in the target task. This severely limits the applicability of many transfer techniques only to cases for which some measure of relatedness between source and target tasks can be guaranteed beforehand. This brings us to the second problem with transfer, which is the issue of identifying an appropriate source task from which to transfer. In some scenarios, different source tasks might be relevant and useful for different parts of the state space of the target task. As a real world analogy, consider multiple players (experts) who are good at different aspects of a game (say, tennis). For example, Player 1 is good at playing backhand shots while Player 2 is good at playing forehand shots. Consider the case of a new player (agent) who wants to learn tennis by selectively learning from these two experts. We handle such a situation in our architecture by allowing the agent to learn how to pick and use solutions from multiple and different source tasks while solving a target task, selectively applicable for different parts of the state space. We call this selective transfer. Our agent can transfer knowledge from Player 1 when required to play backhand shots and Player 2 for playing forehand shots. Further, let us consider consider the situation that both Player 1 and Player 2 are bad at playing drop shots. Apart from the source tasks, we maintain a base network that learns from scratch on the target task. The agent can pick and use the solution of the base network when solving the target task at the parts of the state space where transferring from the source tasks is negative. Such a situation could arise when the source task solutions are irrelevant for solving the target task over a specific portion of the state space, or when the transferring from the source tasks is negative over a specific portion of the state space (for example, transferring the bad drop shot abilities of Players 1 and 2). This situation also entails the first problem of avoiding negative transfer. Our framework allows an agent to avoid transferring from both Players 1 and 2 while learning to play drop shots, and rather acquire the drop shot skill by learning to use the base network. The architecture is trained such that the base network uses not just the experience obtained through the usage of its solutions in the target task, but the overall experience acquired using the combined knowledge of the source tasks and itself. This enables the base network solutions to get closer to the behavior of the overall architecture (which uses the source task solutions as well). This makes it easier for the base network to assist the architecture to fine tune the useful source task solutions to suit the target task perfectly over time.\nThe key contribution in the architecture is a deep attention network, that decides which solutions to attend to, for a given input state. The network learns solutions as a function of current state thereby aiding the agent in adopting different solutions for different parts of the state space in the target task.\nTo this end, we propose A2T: Attend, Adapt and Transfer, an Attentive Deep Architecture for Adaptive Transfer, that avoids negative transfer while performing selective transfer from multiple source tasks in the same domain. In addition to the tennis example, A2T is a fairly generic framework that can be used to selectively transfer different skills available from different experts as appropriate to the situation. For instance, a household robot can appropriately use skills from different experts for different household chores. This would require the skill to transfer manipulation skills across objects, tasks and robotic actuators. With a well developed attention mechanism, the most appropriate and helpful combination of object-skill-controller can be identified for aiding the learning on a related new task. Further, A2T is generic enough to effect transfer of either action policies or actionvalue functions, as the case may be. We also adapt different algorithms in reinforcement learning as appropriate for the different settings and empirically demonstrate that the A2T is effective for transfer learning for each setting.\n\n2 RELATED WORK\nAs mentioned earlier, transfer learning approaches could deal with transferring policies or value functions. For example, Banerjee & Stone (2007) describe a method for transferring value functions by constructing a Game tree. Similarly, Sorg & Singh (2009) use the value function from a source task as the initial estimate of the value function in the target task.\nAnother method to achieve transfer is to reuse policies derived in the source task(s) in the target task. Probabilistic Policy Reuse as discussed in Fernández & Veloso (2006) maintains a library of policies and selects a policy based on a similarity metric, or a random policy, or a max-policy from the knowledge obtained. This is different from the proposed approach in that the proposed approach\ncan transfer policies at the granularity of individual states which is not possible in policy-reuse rendering it unable to learn customized policy at that granularity.Atkeson & Schaal (1997); Niekum et al. (2013) evaluated the idea of having the transferred policy from the source tasks as explorative policies instead of having a random exploration policy. This provides better exploration behavior provided the tasks are similar. Talvitie & Singh (2007) try to find the promising policy from a set of candidate policies that are generated using different action mapping to a single solved task. In contrast, we make use of one or more source tasks to selectively transfer policies at the granularity of state. Apart from policy transfer and value transfer as discussed above, Ferguson & Mahadevan (2006) discuss representation transfer using Proto Value Functions.\nThe idea of negative and selective transfer have been discussed earlier in the literature. For example, Lazaric & Restelli (2011) address the issue of negative transfer in transferring samples for a related task in a multi-task setting. Konidaris et al. (2012) discuss the idea of exploiting shared common features across related tasks. They learn a shaping function that can be used in later tasks.\nThe two recent works that are very relevant to the proposed architecture are discussed in Parisotto et al. (2015) and Rusu et al. (2016). Parisotto et al. (2015) explore transfer learning in RL across Atari games by trying to learn a multi-task network over the source tasks available and directly finetune the learned multi-task network on the target task. However, fine-tuning as a transfer paradigm cannot address the issue of negative transfer which they do observe in many of their experiments. Rusu et al. (2016) try to address the negative transfer issue by proposing a sequential learning mechanism where the filters of the network being learned for an ongoing task are dependent through lateral connections on the lower level filters of the networks learned already for the previous tasks. The idea is to ensure that dependencies that characterize similarity across tasks could be learned through these lateral connections. Even though they do observe better transfer results than direct fine-tuning, they are still not able to avoid negative transfer in some of their experiments.\n\n3 PROPOSED ARCHITECTURE\nLet there be N source tasks and let K1,K2, . . .KN be the solutions of these source tasks 1, . . . N respectively. Let KT be the solution that we learn in the target task T . Source tasks refer to tasks that we have already learnt to perform and target task refers to the task that we are interested in learning now. These solutions could be for example policies or state-action values. Here the source tasks should be in the same domain as the target task, having the same state and action spaces. We propose a setting where KT is learned as a function of K1, . . . ,KN ,KB , where KB is the solution of a base network which starts learning from scratch while acting on the target task. In this work, we use a convex combination of the solutions to obtain KT .\nKT (s) = wN+1,sKB(s) + N∑ i=1 wi,sKi(s) (1)\nN+1∑ i=1 wi,s = 1, wi,s ∈ [0, 1] (2)\nwi,s is the weight given to the ith solution at state s.\nThe agent uses KT to act in the target task. Figure 1a shows the proposed architecture. While the source task solutionsK1, . . . ,KN remain fixed, the base network solutions are learnt and henceKB can change over time. There is a central network which learns the weights (wi,s, i ∈ 1, 2, . . . , N+1), given the input state s. We refer to this network as the attention network. The [0, 1] weights determine the attention each solution gets allowing the agent to selectively accept or reject the different solutions, depending on the input state. We adopt a soft-attention mechanism whereby more than one weight can be non-zero [Bahdanau et al. (2014)] as opposed to a hard-attention mechanism [Mnih et al. (2014)] where we are forced to have only one non-zero weight.\nwi,s = exp (ei,s) N+1∑ j=1 exp (ej,s) , i ∈ {1, 2, . . . , N + 1} (3)\n(e1,s, e2,s, . . . , eN+1,s) = f(s; θa) (4)\nHere, f(s; θa) is a deep neural network (attention network), which could consist of convolution layers and fully connected layers depending on the representation of input. It is parametrised by θa and takes as input a state s and outputs a vector of length N + 1, which gives the attention scores for the N + 1 solutions at state s. Eq.(3) normalises this score to get the weights that follow Eq.(2).\nIf the ith source task solution is useful at state s, then wi,s is set to a high value by the attention network. Working at the granularity of states allows the attention network to attend to different source tasks, for different parts of the state space of the target task, thus giving it the ability to perform selective transfer. For parts of the state space in the target task, where the source task solutions cause negative transfer or where the source task solutions are not relevant, the attention network learns to give high weight to the base network solution (which can be learnt and improved), thus avoiding negative transfer.\nDepending on the feedback obtained from the environment upon following KT , the attention network’s parameters θa are updated to improve performance.\nAs mentioned earlier, the source task solutions, K1, . . . ,KN remain fixed. Updating these source task’s parameters would cause a significant amount of unlearning in the source tasks solutions and result in a weaker transfer, which we observed empirically. This also enables the use of source task solutions, as long as we have the outputs alone, irrespective of how and where they come from.\nEven though the agent follows KT , we update the parameters of the base network that produces KB , as if the action taken by the agent was based only on KB . Due to this special way of updating KB , apart from the experience got through the unique and individual contribution of KB to KT in parts of the state space where the source task solutions are not relevant, KB also uses the valuable experience got by using KT which uses the solutions of the source tasks as well.\nThis also means that, if there is a source task whose solution Kj is useful for the target task in some parts of its state space, then KB tries to replicate Kj in those parts of the state space. In practise, the source task solutions though useful, might need to be modified to suit perfectly for the target task. The base network takes care of these modifications required to make the useful source task solutions perfect for the target task. The special way of training the base network assists the architecture in achieving this faster. Note that the agent could follow/useKj throughKT even when KB does not attain its replication in the corresponding parts of the state space. This allows for a good performance of the agent in earlier stages training itself, when a useful source task is available and identified.\nSince the attention is soft, our model has the flexibility to combine multiple solutions. The use of deep neural networks allow the model to work even for large, complex RL problems. The deep attention network, allows the agent to learn complex selection functions, without worrying about\nrepresentation issues a priori. To summarise, for a given state, A2T learns to attend to specific solutions and adapts this attention over different states, hence attaining useful transfer. A2T is general and can be used for transfer of solutions such as policy and value.\n\n3.1 POLICY TRANSFER\nThe solutions that we transfer here are the source task policies, taking advantage of which, we learn a policy for the target task. Thus, we have K1, . . . ,KN ,KB ,KT ← π1, . . . πN , πB , πT . Here π represents a stochastic policy, a probability distribution over all the actions. The agent acts in the target task, by sampling actions from the probability distribution πT . The target task policy πT is got as described in Eq.(1) and Eq.(2). The attention network that produces the weights for the different solutions, is trained by the feedback got after taking action following πT . The base network that produces πB is trained as if the sampled action came from πB (though it originally came from πT ), the implications of which were discussed in the previous section. When the attention network’s weight for the policy πB is high, the mixture policy πT is dominated by πB , and the base network learning is nearly on-policy. In the other cases, πB undergoes off-policy learning. But if we look closely, even in the latter case, since πB moves towards πT , it tries to be nearly on-policy all the time. Empirically, we observe that πB converges. This architecture for policy transfer can be used alongside any algorithm that has an explicit representation of the policy. Here we describe two instantiations of A2T for policy transfer, one for direct policy search using REINFORCE algorithm and another in the Actor-Critic setup.\n\n3.1.1 POLICY TRANSFER IN REINFORCE ALGORITHMS USING A2T:\nREINFORCE algorithms [Williams (1992)] can be used for direct policy search by making weight adjustments in a direction that lies along the gradient of the expected reinforcement. The full architecture is same as the one shown in Fig.1a with K ← π. We do direct policy search, and the parameters are updated using REINFORCE. Let the attention network be parametrized by θa and the base network which outputs πB be parametrized by θb. The updates are given by:\nθa ← θa + αθa(r − b) ∂ ∑M t=1 log(πT (st, at))\n∂θa (5)\nθb ← θb + αθb(r − b) ∂ ∑M t=1 log(πB(st, at))\n∂θb (6)\nwhere αθa , αθb are non-negative factors, r is the return obtained in the episode, b is some baseline and M is the length of the episode. at is the action sampled by the agent at state st following πT . Note that while πT (st, at) is used in the update of the attention network, πB(st, at) is used in the update of the base network.\n\n3.1.2 POLICY TRANSFER IN ACTOR-CRITIC USING A2T:\nActor-Critic methods [Konda & Tsitsiklis (2000)] are Temporal Difference (TD) methods that have two separate components, viz., an actor and a critic. The actor proposes a policy whereas the critic estimates the value function to critique the actor’s policy. The updates to the actor happens through TD-error which is the one step estimation error that helps in reinforcing an agent’s behaviour.\nWe use A2T for the actor part of the Actor-Critic. The architecture is shown in Fig.1b. The actor, A2T is aware of all the previous learnt tasks and tries to use those solution policies for its benefit. The critic evaluates the action selection from πT on the basis of the performance on the target task. With the same notations as REINFORCE for st, at, θa, θb, αθa , αθb , πB , πT ; let action at dictated by πT lead the agent to next state st+1 with a reward rt+1 and let V (st) represent the value of state st and γ the discount factor. Then, the update equations for the actor are as below:\nδt = rt+1 + γV (st+1)− V (st) (7)\nθa ← θa + αθaδt ∂ log πT (st,at) ∂θa∣∣∣∂ log πT (st,at)∂θa ∣∣∣ (8)\nθb ← θb + αθbδt ∂ log πB(st,at) ∂θb∣∣∣∂ log πB(st,at)∂θb ∣∣∣ (9) Here, δt is the TD error. The state-value function V of the critic is learnt using TD learning.\n\n3.2 VALUE TRANSFER\nIn this case, the solutions being transferred are the source tasks’ action-value functions, which we will call as Q functions. Thus, K1, . . . ,KN ,KB ,KT ← Q1, . . . , QN , QB , QT . Let A represent the discrete action space for the tasks and Qi(s) = {Q(s, aj) ∀ aj ∈ A}. The agent acts by using QT in the target task, which is got as described in Eq.(1) and Eq.(2). The attention network and the base network of A2T are updated as described in the architecture.\n\n3.2.1 VALUE TRANSFER IN Q LEARNING USING A2T:\nThe state-action value Q function is used to guide the agent to selecting the optimal action a at a state s, whereQ(s, a) is a measure of the long-term return obtained by taking action a at state s. One way to learn optimal policies for an agent is to estimate the optimal Q(s, a) for the task. Q-learning [Watkins & Dayan (1992)] is an off-policy Temporal Difference (TD) learning algorithm that does so. The Q-values are updated iteratively through the Bellman optimality equation [Puterman (1994)] with the rewards obtained from the task as below:\nQ(s, a)← E[r(s, a, s′) + γmaxa′Q(s′, a′)]\nIn high dimensional state spaces, it is infeasible to update Q-value for all possible state-action pairs. One way to address this issue is by approximating Q(s, a) through a parametrized function approximator Q(s, a; θ),thereby generalizing over states and actions by operating on higher level features [Sutton & Barto (1998)]. The DQN [Mnih et al. (2015)] approximates the Q-value function with a deep neural network to be able to predict Q(s, a) over all actions a, for all states s.\nThe loss function used for learning a Deep Q Network is as below: L(θ) = Es,a,r,s′ [ ( yDQN −Q(s, a; θ) )2 ],\nwith yDQN = ( r + γmaxa′Q(s′, a′, θ−) ) Here, L represents the expected TD error corresponding to current parameter estimate θ. θ− represents the parameters of a separate target network, while θ represents the parameters of the online network. The usage of a target network is to improve the stability of the learning updates. The gradient descent step is shown below:\n∇θL(θ) = Es,a,r,s′ [(yDQN −Q(s, a; θ))∇θQ(s, a)]\nTo avoid correlated updates from learning on the same transitions that the current network simulates, an experience replay [Lin (1993)] D (of fixed maximum capacity) is used, where the experiences are pooled in a FIFO fashion.\nWe use DQN to learn our expertsQi, i ∈ 1, 2 . . . N on the source tasks. Q-learning is used to ensure QT (s) is driven to a good estimate of Q functions for the target task. Taking advantage of the offpolicy nature of Q-learning, both QB and QT can be learned from the experiences gathered by an -greedy behavioral policy based on QT . Let the attention network that outputs w be parametrised by θa and the base network outputting QB be parametrised by θb. Let θa− and θb− represent the parameters of the respective target networks. Note that the usage of target here is to signify the parameters (θ−a , θ − b ) used to calculate the target value in the Q-learning update and is different from its usage in the context of the target task. The update equations are:\nyQT = (r + γmaxa′QT (s′, a′; θa−, θb−)) (10)\nLQT (θa, θb) = Es,a,r,s′ [(yQT −QT (s, a; θa, θb))2] (11)\nLQB (θb) = Es,a,r,s′ [(yQT −QB(s, a; θb))2] (12)\n∇θaLQT = E[(yQT −QT (s, a))∇θaQT (s, a)] (13)\n∇θbLQB = E[(yQT −QB(s, a))∇θbQR(s, a)] (14) θa and θb are updated with the above gradients using RMSProp. Note that the Q-learning updates for both the attention network (Eq.(11)) and the base network (Eq.(12)) use the target value generated by QT . We use target networks for both QB and QT to stabilize the updates and reduce the nonstationarity as in DQN training. The parameters of the target networks are periodically updated to that of the online networks.\n\n4 EXPERIMENTS AND DISCUSSION\nWe evaluate the performance of our architecture A2T on policy transfer using two simulated worlds, viz., chain world and puddle world as described below. The main goal of these experiments is to test the consistency of results with the algorithm motivation. Chain world: Figure 2a shows the chain world where the goal of the agent is to go from one point in the chain (starting state) to another point (goal state) in the least number of steps. At each state the agent can choose to either move one position to the left or to the right. After reaching the goal state the agent gets a reward that is inversely proportional to the number of steps taken to reach the goal.\nPuddle worlds: Figures 2b and 2c show the discrete version of the standard puddle world that is widely used in Reinforcement Learning literature. In this world, the goal of the agent is to go from a specified start position to the goal position, maximising its return. At each state the agent can choose one of these four actions: move one position to the north, south, east or west.With 0.9 probability the agent moves in the chosen direction and with 0.1 probability it moves in a random direction irrespective of its choice of action. On reaching the goal state, the agent gets a reward of +10. On reaching other parts of the grid the agent gets different penalties as mentioned in the legend of the figures. . We evaluate the performance of our architecture on value transfer using the Arcade Learning Environment (ALE) platform [Bellemare et al. (2012)]. Atari 2600: ALE provides a simulator for Atari 2600 games. This is one of the most commonly used benchmark tasks for deep reinforcement learning algorithms [Mnih et al. (2015), Mnih et al. (2016), Parisotto et al. (2015), Rusu et al. (2016)]. We perform our adaptive transfer learning experiments on the Atari 2600 game Pong.\n\n4.1 ABILITY TO DO SELECTIVE TRANSFER\nIn this section, we consider the case when multiple partially favorable source tasks are available such that each of them can assist the learning process for different parts of the state space of the target task. The objective here is to first show the effectiveness of the attention network in learning to focus only on the source task relevant to the state the agent encounters while trying to complete the target task and then evaluating the full architecture with an additional randomly initialised base network.\nThis is illustrated for the Policy Transfer setting using the chain world shown in (Fig. 2a). Consider that the target task LT is to start in A or B with uniform probability and reach C in the least number of steps. Now, consider that two learned source tasks, viz., L1 and L2, are available. L1 is the source task where the agent has learned to reach the left end (A) starting from the right end (B). In contrast, L2 is the source task where the agent has learned to reach the right end (B) starting from the left end (A). Intuitively, it is clear that the target task should benefit from the policies learnt for tasks L1 and L2. We learn to solve the task LT using REINFORCE given the policies learned for L1 and L2. Figure 3a (i) shows the weights given by the attention network to the two source task policies for different parts of the state space at the end of learning. We observe that the attention network has learned to ignore L1, and L2 for the left, and right half of the state space of the target task, respectively. Next, we add base network and evaluate the full architecture on this task. Figure 3a (ii) shows the weights given by the attention network to the different source policies for different parts of the state space at the end of learning. We observe that the attention network has learned to ignore L1, and L2 for the left, and right half of the state space of the target task, respectively. As the base network replicates πT over time, it has a high weight throughout the state space of the target task.\nWe also evaluate our architecture in a relatively more complex puddle world shown in Figure 2c. In this case, L1 is the task of moving from S1 to G1, and L2 is the task of moving from S2 to G1. In the target task LT , the agent has to learn to move to G1 starting from either S1 or S2 chosen with uniform probability. We learn the task LT using Actor-Critic method, where the following are available (i) learned policy for L1 (ii) learned policy for L2 and (iii) a randomly initialized policy network (the base network). Figure 3b shows the performance results. We observe that actor-critic using A2T is able to use the policies learned for L1, and L2 and performs better than a network learning from scratch without any knowledge of source tasks.\nWe do a similar evaluation of the attention network, followed by our full architecture for value transfer as well. We create partially useful source tasks through a modification of the Atari 2600 game Pong. We take inspiration from a real world scenario in the sport Tennis, where one could imagine two different right-handed (or left) players with the first being an expert player on the forehand but weak on the backhand, while the second is an expert player on the backhand but weak on the forehand. For someone who is learning to play tennis with the same style (right/left) as the experts, it is easy to follow the forehand expert player whenever he receives a ball on the forehand and follow the backhand expert whenever he receives a ball on the backhand.\nWe try to simulate this scenario in Pong. The trick is to blur the part of the screen where we want to force the agent to be weak at returning the ball. The blurring we use is to just black out all pixels in the specific region required. To make sure the blurring doesn’t contrast with the background, we modify Pong to be played with a black background (pixel value 0) instead of the existing gray (pixel value 87). We construct two partially helpful source task experts L1 and L2. L1 is constructed by\ntraining a DQN on Pong with the upper quadrant (the agent’s side) blurred, while L2 is constructed by training a DQN with the lower quadrant (the agent’s side) blurred. This essentially results in the ball being invisible when it is in the upper quadrant for L1 and lower quadrant for L2. We therefore expect L1 to be useful in guiding to return balls on the lower quadrant, and L2 for the upper quadrant. The goal of the attention network is to learn suitable filters and parameters so that it will focus on the correct source task for a specific situation in the game. The source task experts L1 and L2 scored an average of 9.2 and 8 respectively on Pong game play with black background. With an attention network to suitably weigh the value functions of L1 and L2, an average performance of 17.2 was recorded just after a single epoch (250,000 frames) of training. (The score in Pong is in the range of [−21, 21]). This clearly shows that the attention mechanism has learned to take advantage of the experts adaptively. Fig. 4 shows a visualisation of the attention weights for the same.\nWe then evaluate our full architecture (A2T) in this setting, i.e with an addition of DQN learning from scratch (base network) to the above setting. The architecture can take advantage of the knowledge of the source task experts selectively early on during the training while using the expertise of the base network wherever required, to perform well on the target task. Figure 5 summarizes the results, where it is clear that learning with both the partially useful experts is better than learning with only one of them which in turn is better than learning from scratch without any additional knowledge.\n\nTO TRANSFER FROM FAVORABLE TASK\nWe first consider the case when only one learned source task is available such that its solution K1 (policy or value) can hamper the learning process of the new target task. We refer to such a source task as an unfavorable source task. In such a scenario, the attention network shown in Figure 1a should learn to assign a very low weight (ignore) to K1 . We also consider a modification of this setting by adding another source task whose solution K2 is favorable to the target task. In such a scenario, the attention network should learn to assign high weight (attend) to K2 while ignoringK1.\nWe now define an experiment using the puddle world from Figure 2b for policy transfer. The target task in our experiment is to maximize the return in reaching the goal state G1 starting from any one of the states S1, S2, S3, S4. We artificially construct an unfavorable source task by first learning to solve the above task and then negating the weights of the topmost layer of the actor network. We then add a favorable task to the above setting. We artificially construct a favorable source task\nsimply by learning to solve the target task and using the learned actor network. Figure 6 shows the results. The target task for the value transfer experiment is to reach expert level performance on Pong. We construct two kinds of unfavorable source tasks for this experiment. Inverse-Pong: A DQN on Pong trained with negated reward functions, that is with R′(s, a) = −R(s, a) where R(s, a) is the reward provided by the ALE emulator for choosing action a at state s. Freeway: An expert DQN on another Atari 2600 game, Freeway, which has the same range of optimal value functions and same action space as Pong. We empirically verified that the Freeway expert DQN leads to negative transfer when directly initialized and fine-tuned on Pong which makes this a good proxy for a negative source task expert even though the target task Pong has a different state space.\nWe artificially construct a favorable source task by learning a DQN to achieve expertise on the target task (Pong) and use the learned network. Figure 7a compares the performance of the various scenarios when the unfavorable source task is Inverse-Pong, while Figure 7b offers a similar comparison with the negative expert being Freeway.\nFrom all the above results, we can clearly see that A2T does not get hampered by the unfavorable source task by learning to ignore the same and performs competitively with just a randomly initialized learning on the target task without any expert available. Secondly, in the presence of an additional source task that is favorable, A2T learns to transfer useful knowledge from the same while ignoring the unfavorable task, thereby reaching expertise on the target task much faster than the other scenarios.\n\n4.3 VISUALIZATION: EVOLUTION OF ATTENTION WEIGHTS WITH ONE POSITIVE AND ONE NEGATIVE EXPERT\nWe present the evolution of attention weights for the experiment described in Section 4.2 where we focus on the efficacy of the A2T framework in providing an agent the ability to avoid negative transfer and transfer from a favorable source task (perfect expert). Figure 8 depicts the evolution of\nthe attention weights (normalised in the range of [0, 1]) during the training of the A2T framework. The corresponding experiment is the case where the target task is to solve Pong, while there are two source task experts, one being a perfect Pong playing trained DQN (to serve as positive expert), and the other being the Inverse-Pong DQN trained with negated reward functions (to serve as negative expert). Additionally, there’s also the base network that learns from scratch using the experience gathered by the attentively combined behavioral policy from the expert networks, the base network and itself.\nWe train the framework for 30 epochs, and the plot illustrates the attention weights every second epoch. We clearly see from figure 8 that there is no weird co-adaptation that happens in the training, and the attention on the negative expert is uniformly low throughout. Initially, the framework needs to collect some level of experience to figure out that the positive expert is optimal (or close to optimal). Till then, the attention is mostly on the base network, which is learning from scratch. The attention then shifts to the positive expert which in turn provides more rewarding episodes and transition tuples to learn from. Finally, the attention drifts slowly to the base network from the positive expert again, after which the attention is roughly random in choosing between the execution of positive expert and the base network. This is because the base network has acquired sufficient expertise as the positive expert which happens to be optimal for the tar-\nget task. This visualization clearly shows that A2T is a powerful framework in ignoring a negative expert throughout and using a positive expert appropriately to learn quickly from the experience gathered and acquire sufficient expertise on the target task.\n\n4.4 WHEN A PERFECT EXPERT IS NOT AVAILABLE AMONG THE SOURCE TASKS\nIn our experiments in the previous subsection dealing with prevention of negative transfer and using a favorable source task, we consider the positive expert as a perfect (close to optimal) expert on the same task we treat as the target task. This raises the question of relying on the presence of a perfect expert as a positive expert. If we have such a situation, the obvious solution is to execute each of the experts on the target task and vote for them with probabilities proportional to the average performance of each.\nThe A2T framework is however generic and not intended to just do source task selection. We illustrate this with an additional baseline experiment, where the positive source task is an imperfect expert on the target task. In such a case, just having a weighted average voting among the available source task networks based on their individual average rewards is upper bounded by the\nperformance of the best available positive expert, which happens to be an imperfect expert on the target task. Rather, the base network has to acquire new skills not present in the source task networks. We choose a partially trained network on Pong, that scores an average of 8 (max: 21). The graph in figure 9 clearly shows that the A2T framework with a partial Pong expert and a negative expert performs better than i) learning from scratch, ii) A2T with only one negative expert, and performs worse than A2T with one perfect positive expert and one negative expert. This is expected because\na partial expert cannot provide as much of expert knowledge as a perfect expert, but still provides some useful knowledge in speeding the process of solving the target task. An important conclusion from this experiment is that the A2T framework is capable of discovering new skills not available among any of the experts when such skills are required for optimally solving the target task. To maintain consistency, we perform the same number of runs for averaging scores and experimented with both learning rates and pick the better performing one (0.00025).\n\n5 CONCLUSION AND FUTURE WORK\nIn this paper we present a very general deep neural network architecture, A2T, for transfer learning that avoids negative transfer while enabling selective transfer from multiple source tasks in the same domain. We show simple ways of using A2T for policy transfer and value transfer. We empirically evaluate its performance with different algorithms, using simulated worlds and games, and show that it indeed achieves its stated goals. Apart from transferring task solutions, A2T can also be used for transferring other useful knowledge such as the model of the world.\nWhile in this work we focused on transfer between tasks that share the same state and action spaces and are in the same domain, the use of deep networks opens up the possibility of going beyond this setting. For example, a deep neural network can be used to learn common representations [Parisotto et al. (2015)] for multiple tasks thereby enabling transfer between related tasks that could possibly have different state-action spaces. A hierarchical attention over the lower level filters across source task networks while learning the filters for the target task network is another natural extension to transfer across tasks with different state-action spaces. The setup from Progressive Neural Networks [Rusu et al. (2016)] could be borrowed for the filter transfer, while the A2T setup can be retained for the policy/value transfer. Exploring this setting for continuous control tasks so as to transfer from modular controllers as well avoid negative transfer is also a potential direction for future research.\nThe nature of tasks considered in our experiments is naturally connected to Hierarchical Reinforcement Learning and Continual Learning. For instance, the blurring experiments inspired from Tennis based on experts for specific skills like Forehand and Backhand could be considered as learning from sub-goals (program modules) like Forehand and Backhand to solve a more complex and broader task like Tennis by invoking the relevant sub-goals (program modules). This structure could be very useful to build a household robot for general purpose navigation and manipulation whereby specific skills such as manipulation of different objects, navigating across different source-destination points, etc could be invoked when necessary. The attention network in the A2T framework is essentially a soft meta-controller and hence presents itself as a powerful differentiable tool for Continual and Meta Learning. Meta-Controllers have typically been been designed with discrete decision structure over high level subgoals. This paper presents an alternate differentiable meta-controller with a soft-attention scheme. We believe this aspect can be exploited for differentiable meta-learning architectures for hierarchical reinforcement learning. Over all, we believe that A2T is a novel way to approach different problems like Transfer Learning, Meta-Learning and Hierarchical Reinforcement Learning and further refinements on top of this design can be a good direction to explore.\n\nACKNOWLEDGEMENTS\nThanks to the anonymous reviewers of ICLR 2017 who have provided thoughtful remarks and helped us revise the paper. We would also like to thank Sherjil Ozair, John Schulman, Yoshua Bengio, Sarath Chandar, Caglar Gulchere and Charu Chauhan for useful feedback about the work.\n\nAPPENDIX A: DETAILS OF THE NETWORK ARCHITECTURE IN VALUE TRANSFER EXPERIMENTS\nFor the source task expert DQNs, we use the same architecture as [Mnih et al. (2015)] where the input is 84 × 84 × 4 with 32 convolution filters, dimensions 8 × 8, stride 4 × 4 followed by 64 convolution filters with dimensions 4× 4 and stride 2× 2, again followed by 64 convolution filters of size 3×3 and stride 1×1. This is then followed by a fully connected layer of 512 units and finally by a fully connected output layer with as many units as the number of actions in Pong (Freeway) which is 3. We use ReLU nonlinearity in all the hidden layers.\nWith respect to the A2T framework architecture, we have experimented with two possible architectures:\n• The base and attention networks following the NIPS architecture of Mnih et al. (2013) except that the output layer is softmax for the attention network.\n• The base and attention networks following the Nature architecture of Mnih et al. (2015) with a softmax output layer for the attention network.\nSpecifically, the NIPS architecture of Mnih et al. (2013) takes in a batch of 84 × 84 × 4 inputs, followed by 16 convolution filters of dimensions 8× 8 with stride 4× 4, 32 convolution filters with dimensions 4 × 4 and stride 2 × 2, a fully connected hidden layer of 256 units, followed by the output layer. For the Selective Transfer with Blurring experiments described in Section 4.1, we use the second option above. For the other experiments in Section 4.2 and the additional experiments in Appendix, we use the first option. The attention network has N + 1 outputs where N is the number of source tasks.\n\nAPPENDIX B: TRAINING DETAILS\nTRAINING ALGORITHM\nFor all our experiments in Value Transfer, we used RMSProp as in [Mnih et al. (2015)] for updating gradient. For Policy Transfer, since the tasks were simple, stochastic gradient descent was sufficient to provide stable updates. We also use reward clipping, target networks and experience replay for our value transfer experiments in exactly the same way (all hyper parameters retained) as [Mnih et al. (2015)]. A training epoch is 250,000 frames and for each training epoch, we evaluate the networks with a testing epoch that lasts 125,000 frames. We report the average score over the completed episodes for each testing epoch. The average scores obtained this way are averaged over 2 runs with different random seeds. In the testing epochs, we use = 0.05 in the -greedy policy.\nLEARNING RATE\nIn all our experiments, we trained the architecture using the learning rates, 0.0025 and 0.0005. In general, the lower learning rate provided more stable (less variance) training curves. While comparing across algorithms, we picked the best performing learning rate out of the two (0.0025 and 0.0005) for each training curve.\n\nAPPENDIX C: BLURRING EXPERIMENTS ON PONG\nThe experts are trained with blurring (hiding the ball) and black background as illustrated in APPENDIX A. Therefore, to compare the learning with that of a random network without any additional knowledge, we ran the baseline DQN on Pong with a black background too. Having a black background provides a rich contrast between the white ball and the black background, thereby making training easier and faster, which is why the performance curves in that setting are different to the other two settings reported for Inverse Pong and Freeway Negative transfer experiments where no blacking is done and Pong is played with a gray background. The blurring mechanism in Pong is illustrated in APPENDIX E.\n\nAPPENDIX D: BLURRING EXPERIMENTS ON BREAKOUT\nSimilar to our Blurring experiment on Pong, we additionally ran another experiment on the Atari 2600 game, Breakout, to validate the efficiency of our attention mechanism. We consider a setup with two experts L1 and L2 along with our attention network. The experts L1 and L2 were trained by blurring the lower left and right quadrants of the breakout screen respectively. We don’t have to make the background black like in the case of Pong because the background is already black in Breakout and direct blurring is sufficient to hiding the ball in the respective regions without any contrasts introduced. We blur only the lower part so as to make it easy for the agent to at least anticipate the ball based on the movement at the top. We empirically observed that blurring the top half (as well) makes it hard to learn any meaningful partially useful experts L1 and L2.\nThe goal of this experiment is to show that the attention network can learn suitable filters so as to dynamically adapt and learn to select the expert appropriate to the situation (game screen) in the task. The expert L1 which was blurred on the left bottom half is bound to weak at returning balls on that region while L2 is expected to be weak on the right. This is in the same vein as the forehandbackhand example in Tennis and its synthetic simulation for Pong by blurring the upper and lower quadrants. During game play, the attention mechanism is expected to ignore L2 when the ball is on the bottom right half (while focusing on L1) and similarly ignore L2 (while focusing on L1) when the ball is on the left bottom half. We learn experts L1 and L2 which score 42.2 and 39.8 respectively. Using the attention mechanism to select the correct expert, we were able to achieve a score of 94.5 after training for 5 epochs. Each training epoch corresponds to 250, 000 decision steps, while the scores are averaged over completed episodes run for 125, 000 decision steps. This shows that the attention mechanism learns to select the suitable expert. Though the performance is limited by the weaknesses of the respective experts, our goal is to show that the attention paradigm is able to take advantage of both experts appropriately. This is evident from the scores achieved by standalone experts and the attention mechanism. Additionally, we also present a visualization of the attention mechanism weights assigned to the experts L1 and L2 during game play in APPENDIX G. The weights assigned are in agreement with what we expect in terms of selective attention. The blurring mechanism is visually illustrated in APPENDIX F.", "ground_truth": "ICLR 2017 conference submission\n\n---\n\nAttend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain\n\n---\n\nTransferring knowledge from prior source tasks in solving a new target task can be useful in several learning applications. The application of transfer poses two serious challenges which have not been adequately addressed. First, the agent should be able to avoid negative transfer, which happens when the transfer hampers or slows down the learning instead of helping it. Second, the agent should be able to selectively transfer, which is the ability to select and transfer from different and multiple source tasks for different parts of the state space of the target task. We propose A2T (Attend Adapt and Transfer), an attentive deep architecture which adapts and transfers from these source tasks. Our model is generic enough to effect transfer of either policies or value functions. Empirical evaluations on different learning algorithms show that A2T is an effective architecture for transfer by being able to avoid negative transfer while transferring selectively from multiple source tasks in the same domain.\n\n---\n\nIn this paper a well known soft mixture of experts model is adapted for, and applied to, a specific type of transfer learning problem in reinforcement learning (RL), namely transfer of action policies and value functions between similar tasks. Although not treated as such, the experimental setup is reminiscent of hierarchical RL works, an aspect which the paper does not consider at length, regrettably.\nOne possible implication of this work is that architecture and even learning algorithm choices could simply be stated in terms of the objective of the target task, rather than being hand-engineered by the experimenter. This is clearly an interesting direction of future work which the paper illuminates.\n\nPros:\nThe paper diligently explains how the network architecture fits in with various widely used reinforcement learning setups, which does facilitate continuation of this work.\nThe experiments are good proofs of concept, but do not go beyond that i.m.h.o. \nEven so, this work provides convincing clues that collections of deep networks, which were trained on not entirely different tasks, generalize better to related tasks when used together rather than through conventional transfer learning (e.g. fine-tuning).\n\nCons:\nAs the paper well recounts in the related work section, libraries of fixed policies have long been formally proposed for reuse while learning similar tasks. Indeed, it is well understood in hierarchical RL literature that it can be beneficial to reuse libraries of fixed (Fernandez & Veloso 2006) or jointly learned policies which may not apply to the entire state space, e.g. options (Pricop et. al). What is not well understood is how to build such libraries, and this paper does not convincingly shed light in that direction, as far as I can tell.\nThe transfer tasks have been picked to effectively illustrate the potential of the proposed architecture, but the paper does not tackle negative transfer or compositional reuse in well known challenging situations outlined in previous work (e.g. Parisotto et. al 2015, Rusu el. al 2015, 2016).\nSince the main contributions are of an empirical nature, I am curious how the results shown in figures 6 & 7 look plotted against wall-clock time, since relatively low data efficiency is not a limitation for achieving perfect play in Pong (see Mnih. et al, 2015). It would be more illuminating to consider tasks where final performance is plausibly limited by data availability. It would also be interesting if the presented results were achieved with reduced amounts of computation, or reduced representation sizes compared to learning from scratch, especially when one of the useful source tasks is an actual policy trained on the target task.\nFinally, it is perhaps underwhelming that it takes a quarter of the data required for learning Pong from scratch just to figure out that a perfect Pong policy is already in the expert library. Simply evaluating each expert for 10 episodes and using an average-score-weighted majority vote to mix action choices would probably achieve the same final performance for a smaller fraction of the data.\n\n---\n\nICLR committee final decision\n\n---\n\nICLR 2017 pcs\n\n---\n\nThe authors present a mixture of experts framework to combine learnt policies to improve multi-task learning while avoiding negative transfer. The key to their approach is a soft attention mechanism, learnt with RL, which enables positive transfer. They give limited empirical validation of their approach. This is a general attention method which can be applied for policy gradient or value iteration learning methods. The authors were very responsive and added additional experiments based on the reviews.\n\n---\n\n06 Feb 2017\n\n---\n\nSummary of Revisions\n\n---\n\nAravind Lakshminarayanan\n\n---\n\n1) Appendix H - Visualization of Attention Weight Evolution - (Asked by AnonReviewer4)\n2) Appendix I - Partial Expert Positive Source Task A2T experiment - (Asked by AnonReviewer3 WRT base network learning complementary skills and AnonReviewer5 pointing out a con that experiments relied on source tasks containing the optimal policy for the target task.)\n3) Appendix J - Sparse Pong Target Task Added - (Asked by AnonReviewer5 for illustrating transfer learning in a case where the target task performance is limited by data availability)\n4) Appendix A - Revised to give precise details of network architecture (Asked by AnonReviewer5 specifically WRT comparing # of parameters in A2T vs Learning from scratch when source tasks contain an optimal policy for the target task, where it is better if there are fewer parameters for A2T). \n5) Mentioned in the Conclusion Section about extensions of A2T to HRL and Lifelong Learning. (Pointed out by AnonReviewer5)\n\n---\n\n16 Jan 2017 (modified: 18 Jan 2017)\n\n---\n\nICLR 2017 conference AnonReviewer5\n\n---\n\nFinal Review: Learned convex combination of many fixed and a jointly learned expert is used to represent action policies in proof-of-concept, transfer/hierarchical RL, settings.\n\n---\n\nIn this paper a well known soft mixture of experts model is adapted for, and applied to, a specific type of transfer learning problem in reinforcement learning (RL), namely transfer of action policies and value functions between similar tasks. Although not treated as such, the experimental setup is reminiscent of hierarchical RL works, an aspect which the paper does not consider at length, regrettably.\nOne possible implication of this work is that architecture and even learning algorithm choices could simply be stated in terms of the objective of the target task, rather than being hand-engineered by the experimenter. This is clearly an interesting direction of future work which the paper illuminates.\n\nPros:\nThe paper diligently explains how the network architecture fits in with various widely used reinforcement learning setups, which does facilitate continuation of this work.\nThe experiments are good proofs of concept, but do not go beyond that i.m.h.o. \nEven so, this work provides convincing clues that collections of deep networks, which were trained on not entirely different tasks, generalize better to related tasks when used together rather than through conventional transfer learning (e.g. fine-tuning).\n\nCons:\nAs the paper well recounts in the related work section, libraries of fixed policies have long been formally proposed for reuse while learning similar tasks. Indeed, it is well understood in hierarchical RL literature that it can be beneficial to reuse libraries of fixed (Fernandez & Veloso 2006) or jointly learned policies which may not apply to the entire state space, e.g. options (Pricop et. al). What is not well understood is how to build such libraries, and this paper does not convincingly shed light in that direction, as far as I can tell.\nThe transfer tasks have been picked to effectively illustrate the potential of the proposed architecture, but the paper does not tackle negative transfer or compositional reuse in well known challenging situations outlined in previous work (e.g. Parisotto et. al 2015, Rusu el. al 2015, 2016).\nSince the main contributions are of an empirical nature, I am curious how the results shown in figures 6 & 7 look plotted against wall-clock time, since relatively low data efficiency is not a limitation for achieving perfect play in Pong (see Mnih. et al, 2015). It would be more illuminating to consider tasks where final performance is plausibly limited by data availability. It would also be interesting if the presented results were achieved with reduced amounts of computation, or reduced representation sizes compared to learning from scratch, especially when one of the useful source tasks is an actual policy trained on the target task.\nFinally, it is perhaps underwhelming that it takes a quarter of the data required for learning Pong from scratch just to figure out that a perfect Pong policy is already in the expert library. Simply evaluating each expert for 10 episodes and using an average-score-weighted majority vote to mix action choices would probably achieve the same final performance for a smaller fraction of the data.\n\n---\n\n02 Jan 2017 (modified: 25 Jan 2017)\n\n---\n\nICLR 2017 conference AnonReviewer3\n\n---\n\nNo Title\n\n---\n\nThis paper studies the problem of transferring solutions of existing tasks to tackle a novel task under the framework of reinforcement learning and identifies two important issues of avoiding negative transfer and being selective transfer. The proposed approach is based on a convex combination of existing solutions and the being-learned solution to the novel task. The non-negative weight of each solution implies that the solution of negative effect is ignored and more weights are allocated to more relevant solution in each state. This paper derives this so-called \"A2T\" learning algorithm for policy transfer and value transfer for REINFORCE and ACTOR-CRITIC algorithms and experiments with synthetic Chain World and Puddle World simulation and Atari 2600 game Pong. \n+This paper presents a novel approach for transfer reinforcement learning.\n+The experiments are cleverly designed to demonstrate the ability of the proposed method.\n-An important aspect of transfer learning is that the algorithm can automatically figure out if the existing solutions to known tasks are sufficient to solve the novel task so that it can save the time and energy of learning-from-scratch. This issue is not studied in this paper as most of experiments have a learning-from-scratch solution as base network. It will be interesting to see how well the algorithm performs without base network. In addition, from Figure 3, 5 and 6, the proposed algorithm seems to accelerate the learning speed, but the overall network seems not better than the solo base network. It will be more convincing to show some example that existing solutions are complementary to the base network.\n-If ignoring the base network, the proposed network can be considered as ensemble reinforcement learning that take advantages of learned agents with different expertise to solve the novel task.\n\n---\n\n30 Dec 2016\n\n---\n\nICLR 2017 conference AnonReviewer4\n\n---\n\nReview\n\n---\n\nThe paper tackles important problems in multi-task reinforcement learning: avoid negative transfer and allow finer selective transfer. The method is based on soft attention mechanism, very general, and demonstrated to be applicable in both policy gradient and value iteration methods. The introduction of base network allows learning new policy if the prior policies aren't directly applicable. State-dependent sub policy selection allows finer control and can be thought of assigning state space to different sub policies/experts. The tasks are relatively simplistic but sufficient to demonstrate the benefits. One limitation is that the method is simple and the results/claims are mostly empirical. It would be interesting to see extensions to option-based framework, stochastic hard attention mechanism, sub-policy pruning, progressive networks. \n\nIn figure 6, the read curve seems to perform worse than the rest in terms of final performance. Perhaps alternative information to put with figures is the attention mask activation statistics during learning, so that we may observe that it learns to turn off adversarial sub-policies and rely on newly learned base policy mostly. This is also generally good to check to see if any weird co-adaptation is happening.\n\n---\n\n16 Dec 2016\n\n---\n\ndetails on re-training during transfer\n\n---\n\nICLR 2017 conference AnonReviewer4\n\n---\n\n02 Dec 2016\n\n---\n\nJanarthanan Rajendran, Aravind Lakshminarayanan, Mitesh M. Khapra, Prasanna P, Balaraman Ravindran\n\n---\n\nWe propose a general architecture for transfer that can avoid negative transfer and transfer selectively from multiple source tasks in the same domain."}
{"paper_id": "384", "paper_text": "ABSTRACT: Machine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al. (2015) to constrain the output tokens to be from the input sequences. We propose two ways of using Pointer Net for our tasks. Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al. (2016) using logistic regression and manually crafted features. Besides, our boundary model also achieves the best performance on the MSMARCO dataset (Nguyen et al., 2016).\n\n1 INTRODUCTION\nMachine comprehension of text is one of the ultimate goals of natural language processing. While the ability of a machine to understand text can be assessed in many different ways, in recent years, several benchmark datasets have been created to focus on answering questions as a way to evaluate machine comprehension (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2016; Weston et al., 2016; Rajpurkar et al., 2016; Nguyen et al., 2016). In this setup, typically the machine is first presented with a piece of text such as a news article or a story. The machine is then expected to answer one or multiple questions related to the text.\nIn most of the benchmark datasets, a question can be treated as a multiple choice question, whose correct answer is to be chosen from a set of provided candidate answers (Richardson et al., 2013; Hill et al., 2016). Presumably, questions with more given candidate answers are more challenging. The Stanford Question Answering Dataset (SQuAD) introduced recently by Rajpurkar et al. (2016) contains such more challenging questions whose correct answers can be any sequence of tokens from the given text. Moreover, unlike some other datasets whose questions and answers were created automatically in Cloze style (Hermann et al., 2015; Hill et al., 2016), the questions and answers in SQuAD were created by humans through crowdsourcing, which makes the dataset more realistic. Another real dataset, the Human-Generated MAchine Reading COmprehension dataset (MSMARCO) (Nguyen et al., 2016), provided a query together with several related documents collected from Bing Index. The answer to the query is generated by human and the answer words can not only come from the given text.\nGiven these advantages of the SQuAD and MSMARCO datasets, in this paper, we focus on these new datasets to study machine comprehension of text. A sample piece of text and three of its associated questions from SQuAD are shown in Table 1. Traditional solutions to this kind of question answering tasks rely on NLP pipelines that involve multiple steps of linguistic analyses and feature engineering, including syntactic parsing, named entity recognition, question classification, semantic parsing, etc. Recently, with the advances of applying neural network models in NLP, there has been\nmuch interest in building end-to-end neural architectures for various NLP tasks, including several pieces of work on machine comprehension (Hermann et al., 2015; Hill et al., 2016; Yin et al., 2016; Kadlec et al., 2016; Cui et al., 2016). However, given the properties of previous machine comprehension datasets, existing end-to-end neural architectures for the task either rely on the candidate answers (Hill et al., 2016; Yin et al., 2016) or assume that the answer is a single token (Hermann et al., 2015; Kadlec et al., 2016; Cui et al., 2016), which make these methods unsuitable for the SQuAD/MSMARCO dataset. In this paper, we propose a new end-to-end neural architecture to address the machine comprehension problem as defined in the SQuAD/MSMARCO dataset. And for the MSMARCO dataset, we will only make use of the words in the given text to generate the answer.\nSpecifically, observing that in the SQuAD/MSMARCO dataset many questions could be entailed from some sentences in the original text, we adopt a match-LSTM model that we developed earlier for textual entailment (Wang & Jiang, 2016) as one layer of our model. We build a bi-directional match-LSTM on the given passage with attentions on the question for each word so that each position in the paragraph will have a hidden representation reflecting its relation to the question. Then we further adopt the Pointer Net (Ptr-Net) model developed by Vinyals et al. (2015) to select the words in these positions based on the hidden representations built by match-LSTM as an answer. We propose two ways to apply the Ptr-Net model for our task: a sequence model which selects the answer word by word, and a boundary model which only selects the start and end points of the answer span. Experiments on the SQuAD dataset show that our two models both outperform the best performance reported by Rajpurkar et al. (2016). Moreover, using an ensemble of several of our models, we can achieve very competitive performance on SQuAD. For the MSMARCO dataset, a real query based problem, our boundary model outperforms our sequence model with a big margin. It also outperforms the golden passage baseline.\nOur contributions can be summarized as follows: (1) We propose two new end-to-end neural network models for machine comprehension, which combine match-LSTM and Ptr-Net to handle the special properties of the SQuAD dataset. To the best of our knowledge, we are the first to propose the boundary model which is more suitable to the SQuAD/MSMARCO tasks. And we are the first to integrate the attention-based word pair matching into machine comprehension tasks. (2) We have achieved the performance of an exact match score of 71.3% and an F1 score of 80.8% on the unseen SQuAD test dataset, which is much better than the feature-engineered solution (Rajpurkar et al., 2016). Our performance is also close to the state of the art on SQuAD, which is 74.8% in terms of exact match and 82.2% in terms of F1 collected from the SQuAD Leaderboard 1. Besides, our boundary model achieves the state-of-art performance on the MSMARCO dataset with BLUE1/2/3/4 40.7/33.9/30.6/28.7 and Rouge-L 37.3 2. (3) Our further visualization of the models reveals some useful insights of the attention mechanism for reasoning the questions. And we also show that the boundary model can overcome the early stop prediction problem in the sequence model. Besides, we also made our code available online 3.\n1https://rajpurkar.github.io/SQuAD-explorer/ 2http://www.msmarco.org/leaders.aspx 3 https://github.com/shuohangwang/SeqMatchSeq\n\n2 METHOD\nIn this section, we first briefly review match-LSTM and Pointer Net. These two pieces of existing work lay the foundation of our method. We then present our end-to-end neural architecture for machine comprehension.\n\n2.1 MATCH-LSTM\nIn a recent work on learning natural language inference, we proposed a match-LSTM model for predicting textual entailment (Wang & Jiang, 2016). In textual entailment, two sentences are given where one is a premise and the other is a hypothesis. To predict whether the premise entails the hypothesis, the match-LSTM model goes through the tokens of the hypothesis sequentially. At each position of the hypothesis, attention mechanism is used to obtain a weighted vector representation of the premise. This weighted premise is then to be combined with a vector representation of the current token of the hypothesis and fed into an LSTM, which we call the match-LSTM. The matchLSTM essentially sequentially aggregates the matching of the attention-weighted premise to each token of the hypothesis and uses the aggregated matching result to make a final prediction.\n\n2.2 POINTER NET\nVinyals et al. (2015) proposed a Pointer Network (Ptr-Net) model to solve a special kind of problems where we want to generate an output sequence whose tokens must come from the input sequence. Instead of picking an output token from a fixed vocabulary, Ptr-Net uses attention mechanism as a pointer to select a position from the input sequence as an output symbol. The pointer mechanism has inspired some recent work on language processing (Gu et al., 2016; Kadlec et al., 2016). Here we adopt Ptr-Net in order to construct answers using tokens from the input text.\n\n2.3 OUR METHOD\nFormally, the problem we are trying to solve can be formulated as follows. We are given a piece of text, which we refer to as a passage, and a question related to the passage. The passage is\nrepresented by matrix P ∈ Rd×P , where P is the length (number of tokens) of the passage and d is the dimensionality of word embeddings. Similarly, the question is represented by matrix Q ∈ Rd×Q where Q is the length of the question. Our goal is to identify a subsequence from the passage as the answer to the question.\nAs pointed out earlier, since the output tokens are from the input, we would like to adopt the Pointer Net for this problem. A straightforward way of applying Ptr-Net here is to treat an answer as a sequence of tokens from the input passage but ignore the fact that these tokens are consecutive in the original passage, because Ptr-Net does not make the consecutivity assumption. Specifically, we represent the answer as a sequence of integers a = (a1, a2, . . .), where each ai is an integer between 1 and P , indicating a certain position in the passage.\nAlternatively, if we want to ensure consecutivity, that is, if we want to ensure that we indeed select a subsequence from the passage as an answer, we can use the Ptr-Net to predict only the start and the end of an answer. In this case, the Ptr-Net only needs to select two tokens from the input passage, and all the tokens between these two tokens in the passage are treated as the answer. Specifically, we can represent the answer to be predicted as two integers a = (as, ae), where as an ae are integers between 1 and P .\nWe refer to the first setting above as a sequence model and the second setting above as a boundary model. For either model, we assume that a set of training examples in the form of triplets {(Pn,Qn,an)}Nn=1 are given. An overview of the two neural network models are shown in Figure 1. Both models consist of three layers: (1) An LSTM preprocessing layer that preprocesses the passage and the question using LSTMs. (2) A match-LSTM layer that tries to match the passage against the question. (3) An Answer Pointer (Ans-Ptr) layer that uses Ptr-Net to select a set of tokens from the passage as the answer. The difference between the two models only lies in the third layer.\nLSTM Preprocessing Layer\nThe purpose for the LSTM preprocessing layer is to incorporate contextual information into the representation of each token in the passage and the question. We use a standard one-directional LSTM (Hochreiter & Schmidhuber, 1997) to process the passage 4 and the question separately, as shown below:\nHp = −−−→ LSTM(P), Hq = −−−→ LSTM(Q). (1)\nThe resulting matrices Hp ∈ Rl×P and Hq ∈ Rl×Q are hidden representations of the passage and the question, where l is the dimensionality of the hidden vectors. In other words, the ith column vector hpi (or h q i ) in H\np (or Hq) represents the ith token in the passage (or the question) together with some contextual information from the left.\nMatch-LSTM Layer\nWe apply the match-LSTM model (Wang & Jiang, 2016) proposed for textual entailment to our machine comprehension problem by treating the question as a premise and the passage as a hypothesis. The match-LSTM sequentially goes through the passage. At position i of the passage, it first uses the standard word-by-word attention mechanism to obtain attention weight vector −→α i ∈ R1×Q as follows:\n−→ Gi = tanh(WqHq + (Wph p i + W r−→h ri−1 + bp)⊗ eQ), −→α i = softmax(wᵀ −→ Gi + b⊗ eQ), (2)\nwhere Wq,Wp,Wr ∈ Rl×l, bp,w ∈ Rl×1 and b ∈ R are parameters to be learned, −→ Gi ∈ Rl×Q is the intermediate result, −→ h ri−1 ∈ Rl×1 is the hidden vector of the one-directional match-LSTM (to be explained below) at position i−1, and the outer product (·⊗eQ) produces a matrix or row vector by repeating the vector or scalar on the left for Q times.\nEssentially, the resulting attention weight −→α i,j above indicates the degree of matching between the ith token in the passage with the jth token in the question. Next, we use the attention weight vector\n4For the MSMARCO dataset, P is actually consisted of several unrelated documents. The previous state of pre-processing LSTM and match-LSTM to compute the first state of each document is set to zero.\n−→α i to obtain a weighted version of the question and combine it with the current token of the passage to form a vector −→z i:\n−→z i = [\nhpi Hq−→α ᵀi\n] , (3)\nwhere Hq ∈ Rl×Q, −→α i ∈ R1×Q and hpi ∈ Rl×1. This vector −→z i is fed into a standard onedirectional LSTM to form our so-called match-LSTM: −→ h ri = −−−→ LSTM(−→z i, −→ h ri−1), (4) where −→ h ri ∈ Rl×1.\nWe further build a similar match-LSTM in the reverse direction. The purpose is to obtain a representation that encodes the contexts from both directions for each token in the passage.\nLet −→ Hr ∈ Rl×P represent the hidden states [ −→ h r1, −→ h r2, . . . , −→ h rP ] and ←− Hr ∈ Rl×P represent [ ←− h r1, ←− h r2, . . . , ←− h rP ], the hidden states of match-LSTM in the reverse direction. We define H\nr ∈ R2l×P as the concatenation of the two:\nHr = [−→ Hr ←− Hr ] . (5)\nAnswer Pointer Layer\nThe top layer, the Answer Pointer (Ans-Ptr) layer, is motivated by the Pointer Net introduced by Vinyals et al. (2015). This layer uses the sequence Hr as input. Recall that we have two different models: The sequence model produces a sequence of answer tokens but these tokens may not be consecutive in the original passage. The boundary model produces only the start token and the end token of the answer, and then all the tokens between these two in the original passage are considered to be the answer. We now explain the two models separately.\nThe Sequence Model: Recall that in the sequence model, the answer is represented by a sequence of integers a = (a1, a2, . . .) indicating the positions of the selected tokens in the original passage. The Ans-Ptr layer models the generation of these integers in a sequential manner. Because the length of an answer is not fixed, in order to stop generating answer tokens at certain point, we allow each ak to take up an integer value between 1 and P + 1, where P + 1 is a special value indicating the end of the answer. Once ak is set to be P + 1, the generation of the answer stops.\nIn order to generate the kth answer token indicated by ak, first, the attention mechanism is used again to obtain an attention weight vector βk ∈ R1×(P+1), where βk,j (1 ≤ j ≤ P + 1) is the probability of selecting the jth token from the passage as the kth token in the answer, and βk,(P+1) is the probability of stopping the answer generation at position k. βk is modeled as follows:\nFk = tanh(VH̃r + (Wahak−1 + b a)⊗ e(P+1)), (6) βk = softmax(vᵀFk + c⊗ e(P+1)), (7)\nwhere H̃r ∈ R2l×(P+1) is the concatenation of Hr with a zero vector, defined as H̃r = [Hr;0], V ∈ Rl×2l,Wa ∈ Rl×l, ba,v ∈ Rl×1 and c ∈ R are parameters to be learned, Fk ∈ Rl×(P+1) is the intermediate result, (· ⊗ e(P+1)) follows the same definition as before, and hak−1 ∈ Rl×1 is the hidden vector at position k − 1 of an answer LSTM as defined below:\nhak = −−−→ LSTM(H̃rβᵀk ,h a k−1). (8)\nWe can then model the probability of generating the answer sequence as p(a|Hr) = ∏ k p(ak|a1, a2, . . . , ak−1,Hr), (9)\nand\np(ak = j|a1, a2, . . . , ak−1,Hr) = βk,j . (10)\nTo train the model, we minimize the following loss function based on the training examples:\n− N∑\nn=1\nlog p(an|Pn,Qn). (11)\nThe Boundary Model: The boundary model works in a way very similar to the sequence model above, except that instead of predicting a sequence of indices a1, a2, . . ., we only need to predict two indices as and ae. So the main difference from the sequence model above is that in the boundary model we do not need to add the zero padding to Hr, and the probability of generating an answer is simply modeled as\np(a|Hr) = p(as|Hr)p(ae|as,Hr). (12)\nAs this boundary model could point to a span covering too many tokens without any restriction, we try to manually limit the length of the predicted span and then search the span with the highest probability computed by p(as)× p(ae|as) as the answer.\n\n3 EXPERIMENTS\nIn this section, we present our experiment results and perform some analyses to better understand how our models works.\n\n3.1 DATA\nWe use the Stanford Question Answering Dataset (SQuAD) v1.1 and the human-generated Microsoft MAchine Reading COmprehension (MSMARCO) dataset v1.1 to conduct our experiments.\nPassages in SQuAD come from 536 articles in Wikipedia covering a wide range of topics. Each passage is a single paragraph from a Wikipedia article, and each passage has around 5 questions associated with it. In total, there are 23,215 passages and 107,785 questions. The data has been split into a training set (with 87,599 question-answer pairs), a development set (with 10,570 questionanswer pairs) and a hidden test set.\nFor the MSMARCO dataset, the questions are user queries issued to the Bing search engine, the context passages are real Web documents and the answers are human-generated. We select the span that has the highest F1 score with the gold standard answer for training and only predict the span in the passages during evaluation. The data has been split into a training set (82326 pairs), a development set (10047 pairs) and a test set (9650 pairs).\n\n3.2 EXPERIMENT SETTINGS\nWe first tokenize all the passages, questions and answers. We use word embeddings from GloVe (Pennington et al., 2014) to initialize the model. Words not found in GloVe are initialized as zero vectors. The word embeddings are not updated during the training of the model.\nThe dimensionality l of the hidden layers is set to be 150. We use ADAMAX (Kingma & Ba, 2015) with the coefficients β1 = 0.9 and β2 = 0.999 to optimize the model. Each update is computed through a minibatch of 30 instances. We do not use L2-regularization.\nFor the SQuAD dataset, the performance is measured by two metrics: percentage of exact match with the ground truth answers and word-level F1 score when comparing the tokens in the predicted answers with the tokens in the ground truth answers. Note that in the development set and the test set each question has around three ground truth answers. F1 scores with the best matching answers are used to compute the average F1 score. For the MSMARCO dataset, the metrics in the official tool of MSMARCO evaluation are BLEU-1/2/3/4 and Rouge-L, which are widely used in many domains.\n\n3.3 RESULTS\nThe SQuAD and MSMARCO results of our models as well as the results of the baselines (Rajpurkar et al., 2016; Yu et al., 2016) are shown in Table 2. For the “LSTM with Ans-Ptr” models, they are\nthe experiments with the ablation of attention mechanism in match-LSTM. Specifically, we use the final representation of the question to replace the weighted sum of the question representations. For the MSMARCO dataset, as the context for each question is consisted of around 10 documents, the “Golden Passage” is to directly use the human labeled document which could answer the question as the prediction.\nFrom the results in Table 2, we can see that the boundary model could clearly outperform the sequence model in a big margin on both datasets. We hypothesis that the sequence model is more likely to stop word generation earlier, and the boundary model can somehow overcome this problem. We have a statistical analysis on the answers generated by our sequence and boundary models shown in Table 3. We can see that the length of the answers generated by the sequence model is much shorter than the ground truth. Especially for the MSMARCO task where the answers are usually much longer, the sequence model could only generate 7 words on average, while the ground truth answers are 16 on average and the boundary model could generate nearly the same number of words with the ground truth. Several answers generated by our models are shown in Appendix A. From Table 2, we can also see that the performance gets poorer by removing the attention mechanism in match-LSTM, while for the MSMARCO dataset, the attention mechanism effects less, with no more than 2 percent reduction in BLEU and Rouge-L scores by attention mechanism ablation.\nBased on the effectiveness of boundary pointer and match-LSTM, we conduct further exploration of the boundary model by adding element-wise comparison (hpi−H qαᵀi ) and (h p i H\nqαᵀi ) into Eqn 3 in match-LSTM layer, adding 2 more bi-directional LSTM layers between match-LSTM and Ans-Ptr layers, and adding bi-directional Ans-Ptr. We show the ablation study of this further tuned model in Table 4. We can see that adding element-wise matching could make the biggest improvement for our boundary model. We also try to remove the phrase-level representation by removing the pre-process LSTM and using the word-level representations as the inputs of match-LSTM. Interestingly, we find the phrase-level representation effects little on the MSMARCO task.\nOverall, we can see that both of our match-LSTM models have clearly outperformed the logistic regression model by Rajpurkar et al. (2016), which relies on carefully designed features. The improvement of our models over the logistic regression model shows that our end-to-end neural network models without much feature engineering are very effective on these tasks and datasets. Our boundary model also outperformed the DCR model (Yu et al., 2016), which maximizes the probability of the gold standard span from all the candidate spans through a neural network structure.\n\n3.4 FURTHER ANALYSES\nTo better understand the strengths and weaknesses of our models, we perform some further analyses of the results below.\nFirst, we suspect that longer answers are harder to predict. To verify this hypothesis, we analysed the performance in terms of both exact match and F1 score with respect to the answer length on the development set, as shown in Figure 2. For example, for questions whose answers contain more than 9 tokens, the F1 score of the boundary model drops to around 55% and the exact match score drops to only around 30%, compared to the F1 score and exact match score of close to 72% and 67%, respectively, for questions with single-token answers. And that supports our hypothesis.\nNext, we analyze the performance of our models on different groups of questions, as shown in Figure 2. We use a crude way to split the questions into different groups based on a set of question words we have defined, including “what,” “how,” “who,” “when,” “which,” “where,” and “why.” These different question words roughly refer to questions with different types of answers. For example, “when” questions look for temporal expressions as answers, whereas “where” questions look for locations as answers. According to the performance on the development dataset, our models work the best for “when” questions. This may be because in this dataset temporal expressions are relatively easier to recognize. Other groups of questions whose answers are noun phrases, such as “what” questions, “which” questions and “where” questions, also get relatively better results. On the other hand, “why” questions are the hardest to answer. This is not surprising because the answers to “why” questions can be very diverse, and they are not restricted to any certain type of phrases.\nFinally, we would like to check whether the attention mechanism used in the match-LSTM layer is effective in helping the model locate the answer. We show the attention weights α in Figure 3. In the figure the darker the color is the higher the weight is. We can see that some words have been well aligned based on the attention weights. For example, the word “German” in the passage is aligned well to the word “language” in the first question, and the model successfully predicts “German” as the answer to the question. For the question word “who” in the second question, the\nword “teacher” actually receives relatively higher attention weight, and the model has predicted the phrase “Martin Sekulic” after that as the answer, which is correct. For the third question that starts with “why”, the attention weights are more evenly distributed and it is not clear which words have been aligned to “why”. For the last question, we can see that the word knowledge needed for generating the answer can also be detected by match-LSTM. For example, the words “European”, “Parliament”, “Council”, “European” and “Union” have higher attention weights on “governing” in the question. Even though our models can solve this type of questions, they are still not able to solve the questions that need multi-sentences reasoning. More answers generated by our models for the questions related to different kinds of reasoning are shown in Appendix B.\n\n4 RELATED WORK\nMachine comprehension of text has gained much attention in recent years, and increasingly researchers are building data-drive, end-to-end neural network models for the task. We will first review the recently released datasets and then some end-to-end models on this task.\n\n4.1 DATASETS\nA number of datasets for studying machine comprehension were created in Cloze style by removing a single token from a sentence in the original corpus, and the task is to predict the missing word. For example, Hermann et al. (2015) created questions in Cloze style from CNN and Daily Mail highlights. Hill et al. (2016) created the Children’s Book Test dataset, which is based on children’s stories. Cui et al. (2016) released two similar datasets in Chinese, the People Daily dataset and the Children’s Fairy Tale dataset.\nInstead of creating questions in Cloze style, a number of other datasets rely on human annotators to create real questions. Richardson et al. (2013) created the well-known MCTest dataset and Tapaswi et al. (2016) created the MovieQA dataset. In these datasets, candidate answers are provided for each question. Similar to these two datasets, the SQuAD dataset (Rajpurkar et al., 2016) was also created by human annotators. Different from the previous two, however, the SQuAD dataset does not provide candidate answers, and thus all possible subsequences from the given passage have to be considered as candidate answers.\nBesides the datasets above, there are also a few other datasets created for machine comprehension, such as WikiReading dataset (Hewlett et al., 2016) and bAbI dataset (Weston et al., 2016), but they are quite different from the datasets above in nature.\n\n4.2 END-TO-END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION\nThere have been a number of studies proposing end-to-end neural network models for machine comprehension. A common approach is to use recurrent neural networks (RNNs) to process the given text and the question in order to predict or generate the answers (Hermann et al., 2015). Attention mechanism is also widely used on top of RNNs in order to match the question with the given passage (Hermann et al., 2015; Chen et al., 2016). Given that answers often come from the given passage, Pointer Network has been adopted in a few studies in order to copy tokens from the given passage as answers (Kadlec et al., 2016; Trischler et al., 2016). Compared with existing work, we use match-LSTM to match a question and a given passage, and we use Pointer Network in a different way such that we can generate answers that contain multiple tokens from the given passage.\nMemory Networks (Weston et al., 2015) have also been applied to machine comprehension (Sukhbaatar et al., 2015; Kumar et al., 2016; Hill et al., 2016), but its scalability when applied to a large dataset is still an issue. In this work, we did not consider memory networks for the SQuAD/MSMARCO datasets.\nThe setting of visual question answering (Antol et al., 2015) is quite similar to machine comprehension, while their answers are usually very short. So the sequence order of the word-level attention representation used to align the figure and the question(Xu & Saenko, 2016; Fukui et al., 2016; Lu et al., 2016), are not used in VQA. While our model focus on the word-by-word attention and use\nLSTM to concatenate the aligned pairs and that would be helpful to generate a longer sequence as answer.\n\n5 CONCLUSIONS\nIn this paper, We developed two models for the machine comprehension problem defined in the Stanford Question Answering (SQuAD) and A Human-Generated MAchine Reading COmprehension (MSMARCO) datasets, both making use of match-LSTM and Pointer Network. Experiments on the SQuAD and MSMARCO datasets showed that our second model, the boundary model, could achieve a performance close to the state-of-the-art performance on the SQuAD dataset and achieved the state-of-the-art on the MSMARCO dataset. We also show the boundary model could overcome the early stop prediction problem of the sequence model.\nIn the future, we plan to look further into the different types of questions and focus on those questions which currently have low performance, such as the “why’ questions and multi-sentences related questions. We also plan to test how our models could be applied to other machine comprehension datasets.\n\n6 ACKNOWLEDGMENTS\nThis research is supported by the National Research Foundation, Prime Ministers Office, Singapore under its International Research Centres in Singapore Funding Initiative.\nWe thank Pranav Rajpurkar for testing our model on the hidden test dataset and Percy Liang for helping us with the Dockerfile for Codalab.\n\nA APPENDIX\nWe show the predictions our boundary and sequence models on two cases from two datasets in Table 5. It can be seen that the sequence model is more likely to predict a shorter sequence which is the problem of early stop prediction.\n\nB APPENDIX\nWe show how four different models work on different type of questions in SQuAD dataset through Table 6. After the analysis of a hundred cases, we see that our models are not able to solve the questions that need multi-sentences reasoning. And the model without attention mechanism has less power to identify the important key word like the third case shown in Table 6.", "ground_truth": "ICLR 2017 conference submission\n\n---\n\nMachine Comprehension Using Match-LSTM and Answer Pointer\n\n---\n\nMachine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al. (2015) to constrain the output tokens to be from the input sequences. We propose two ways of using Pointer Net for our tasks. Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al. (2016) using logistic regression and manually crafted features. Besides, our boundary model also achieves the best performance on the MSMARCO dataset (Nguyen et al. 2016).\n\n---\n\nSUMMARY.\nThis paper proposes a new neural network architectures for solving the task of reading comprehension question answering where the goal is answering a questions regarding a given text passage.\nThe proposed model combines two well-know neural network architectures match-lstm and pointer nets.\nFirst the passage and the questions are encoded with a unidirectional LSTM.\nThen the encoded words in the passage and the encoded words in the questions are combined with an attention mechanism so that each word of the passage has a certain degree of compatibility with the question.\nFor each word in the passage the word representation and the weighted representation of the query is concatenated and passed to an forward lstm.\nThe same process is done in the opposite direction with a backward lstm.\nThe final representation is a concatenation of the two lstms.\nAs a decoded a pointer network is used.\nThe authors tried with two approaches: generating the answer word by word, and generating the first index and the last index of the answer.\n\nThe proposed model is tested on the Stanford Question Answering Dataset.\nAn ensemble of the proposed model achieves performance close to state-of-the-art models.\n\n----------\n\nOVERALL JUDGMENT\n\nI think the model is interesting mainly because of the use of pointer networks as a decoder.\nOne thing that the authors could have tried is a multi-hop approach. It has been shown in many works to be extremely beneficial in the joint encoding of passage and query. The authors can think of it as a deep match-lstm.\nThe analysis of the model is interesting and insightful.\nThe sharing of the code is good.\n\n---\n\nICLR committee final decision\n\n---\n\nICLR 2017 pcs\n\n---\n\nThis paper provides two approaches to question answering: pointing to spans, and use of match-LSTM. The models are evaluated on SQuAD and MSMARCO. The reviewers we satisfied that, with the provision of additional comparisons and ablation studies submitted during discussion, the paper was acceptable to the conference, albeit marginally so.\n\n---\n\n06 Feb 2017\n\n---\n\nICLR 2017 conference AnonReviewer3\n\n---\n\nReview: Interesting combination of existing approaches with encouraging results\n\n---\n\nThe paper looks at the problem of locating the answer to a question in a text (For this task the answer is always part of the input text). For this the paper proposes to combine two existing works: Match-LSTM to relate question and text representations and Pointer Net to predict the location of the answer in the text.\n\nStrength:\n- The suggested approach makes sense for the task and achieves good performance, (although as the authors mention, recent concurrent works achieve better results)\n- The paper is evaluated on the SQuAD dataset and achieves significant improvements over prior work.\n\nWeaknesses:\n1. It is unclear from the paper how well it is applicable to other problem scenarios where the answer is not a subset of the input text.\n2. Experimental evaluation\n2.1. It is not clear why the Bi-Ans-Ptr in Table 2 is not used for the ensemble although it achieves the best performance.\n2.2. It would be interested if this approach generalizes to other datasets.\n\nOther (minor/discussion points)\n- The task and approach seem to have some similarity of locating queries in images and visual question answering. The authors might want to consider pointing to related works in this direction.\n- I am wondering how much this task can be seen as a “guided extractive summarization”, i.e. where the question guides the summarization process.\n- Page 6, last paragraph: missing “.”: “… searching This…”\n\nSummary:\nWhile the paper presents an interesting combination of two approaches for the task of answer extraction, the novelty is moderate. While the experimental results are encouraging, it remains unclear how well this approach generalizes to other scenarios as it seems a rather artificial task.\n\n---\n\n17 Dec 2016\n\n---\n\nICLR 2017 conference AnonReviewer1\n\n---\n\nMore analyses / ablation studies / insights needed regarding the functioning of the proposed model\n\n---\n\nSummary:\nThe paper presents a deep neural network for the task of machine comprehension on the SQuAD dataset. The proposed model is based on two previous works -- match-LSTM and Pointer Net. Match-LSTM produces attention over each word in the given question for each word in the given passage, and sequentially aggregates this matching of each word in the passage with the words in the question. The pointer net is used to generate the answer by either generating each word in the answer or by predicting the starting and ending tokens in the answer from the provided passage. The experimental results show that both the variants of the proposed model outperform the baseline presented in the SQuAD paper. The paper also shows some analysis of the results obtained such as variation of performance across answer lengths and question types.\n\nStrengths:\n1. A novel end-to-end model for the task of machine comprehension rather than using hand-crafted features.\n2. Significant performance boost over the baseline presented in the SQuAD paper.\n3. Some insightful analyses of the results such as performance is better when answers are short, \"why\" questions are difficult to answer.\n\nWeaknesses/Questions/Suggestions:\n1. The paper does not show quantitatively how much modelling attention in match-LSTM and answer pointer layer helps. So, it would be insightful if authors could compare the model performance with and without attention in match-LSTM, and with and without attention in answer pointer layer.\n2. It would be good if the paper could provide some insights into why there is a huge performance gap between boundary model and sequence model in the answer pointer layer.\n3. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.\n4. Could authors please explain why the activations resulting from {h^p}_i and {h^r}_{i-1} in G_i in equation 2 are being repeated across dimension of Q. Why not learn different activations for each dimension? \n5. I wonder why Bi-Ans-Ptr is not used in the ensemble model (last row in table 2) when it is shown that Bi-Ans-Ptr improves performance by 1.2% in F1.\n6. Could authors please discuss and compare the DCR model (in table 2) in the paper in more detail?\n\nReview Summary: The paper presents a reasonable end-to-end model for the task of machine comprehension on the SQuAD dataset, which outperforms the baseline model significantly. However, it would be good if more analyses / ablation studies / insights are included regarding -- how much attention helps, why is boundary model better than sequence model, how does the performance change when the reasoning required becomes difficult.\n\n---\n\n17 Dec 2016 (modified: 20 Jan 2017)\n\n---\n\nICLR 2017 conference AnonReviewer2\n\n---\n\nNo Title\n\n---\n\nSUMMARY.\nThis paper proposes a new neural network architectures for solving the task of reading comprehension question answering where the goal is answering a questions regarding a given text passage.\nThe proposed model combines two well-know neural network architectures match-lstm and pointer nets.\nFirst the passage and the questions are encoded with a unidirectional LSTM.\nThen the encoded words in the passage and the encoded words in the questions are combined with an attention mechanism so that each word of the passage has a certain degree of compatibility with the question.\nFor each word in the passage the word representation and the weighted representation of the query is concatenated and passed to an forward lstm.\nThe same process is done in the opposite direction with a backward lstm.\nThe final representation is a concatenation of the two lstms.\nAs a decoded a pointer network is used.\nThe authors tried with two approaches: generating the answer word by word, and generating the first index and the last index of the answer.\n\nThe proposed model is tested on the Stanford Question Answering Dataset.\nAn ensemble of the proposed model achieves performance close to state-of-the-art models.\n\n----------\n\nOVERALL JUDGMENT\n\nI think the model is interesting mainly because of the use of pointer networks as a decoder.\nOne thing that the authors could have tried is a multi-hop approach. It has been shown in many works to be extremely beneficial in the joint encoding of passage and query. The authors can think of it as a deep match-lstm.\nThe analysis of the model is interesting and insightful.\nThe sharing of the code is good.\n\n---\n\n16 Dec 2016\n\n---\n\nAn updated pdf version\n\n---\n\nShuohang Wang\n\n---\n\nDear reviewers,\n\nThank you for your valuable comments again! We have made the corresponding revisions and updated a new pdf version. We briefly list the changes here:\n\nAnonReviewer1:\n1.We clarify the dimension of row vector $\\alpha_i$ to be $1\\times Q$.\n2.We add the visualization of the $\\alpha$ values for the question requiring world knowledge in Figure 2 and add the corresponding analysis at the end of the section \"Experiments\".\n\nAnonReviewer2:\nWe revise the description of the state-of-the-art results in the last paragraph in \"Introduction\".\nWe clarify the dimension of $G$ to be $l\\time Q$, the row vector $\\alpha_i$ to be $1\\times Q$, the column vector $w$ to be $l\\times 1$ for equation (2). So is the equation (8).\nWe clarify the statement of footnote 3 about the output gates in the pre-processing layer.\nWe clarify the description of global search on the spans in both the boundary model description part and the Table 2.\n\nAnonReviewer3:\nWe clarify the integration of match-LSTM and pointer network in the last two paragraphs of the \"Introduction\".\nWe directly cite the works of the baselines in Table 2.\n\nThanks,\nShuohang\n\n---\n\n13 Dec 2016\n\n---\n\nClarify contribution\n\n---\n\nICLR 2017 conference AnonReviewer3\n\n---\n\n03 Dec 2016\n\n---\n\nClarification Questions\n\n---\n\nICLR 2017 conference AnonReviewer1\n\n---\n\n02 Dec 2016\n\n---\n\nfew doubts\n\n---\n\nICLR 2017 conference AnonReviewer2\n\n---\n\n01 Dec 2016\n\n---\n\nShuohang Wang, Jing Jiang\n\n---\n\nUsing Match-LSTM and Answer Pointer to select a variable length answer from a paragraph"}
{"paper_id": "790", "paper_text": "ABSTRACT: We ask whether neural networks can learn to use secret keys to protect information from other neural networks. Specifically, we focus on ensuring confidentiality properties in a multiagent system, and we specify those properties in terms of an adversary. Thus, a system may consist of neural networks named Alice and Bob, and we aim to limit what a third neural network named Eve learns from eavesdropping on the communication between Alice and Bob. We do not prescribe specific cryptographic algorithms to these neural networks; instead, we train end-to-end, adversarially. We demonstrate that the neural networks can learn how to perform forms of encryption and decryption, and also how to apply these operations selectively in order to meet confidentiality goals.\n\n1 INTRODUCTION\nAs neural networks are applied to increasingly complex tasks, they are often trained to meet endto-end objectives that go beyond simple functional specifications. These objectives include, for example, generating realistic images (e.g., (Goodfellow et al., 2014a)) and solving multiagent problems (e.g., (Foerster et al., 2016a;b; Sukhbaatar et al., 2016)). Advancing these lines of work, we show that neural networks can learn to protect their communications in order to satisfy a policy specified in terms of an adversary.\nCryptography is broadly concerned with algorithms and protocols that ensure the secrecy and integrity of information. Cryptographic mechanisms are typically described as programs or Turing machines. Attackers are also described in those terms, with bounds on their complexity (e.g., limited to polynomial time) and on their chances of success (e.g., limited to a negligible probability). A mechanism is deemed secure if it achieves its goal against all attackers. For instance, an encryption algorithm is said to be secure if no attacker can extract information about plaintexts from ciphertexts. Modern cryptography provides rigorous versions of such definitions (Goldwasser & Micali, 1984).\nAdversaries also play important roles in the design and training of neural networks. They arise, in particular, in work on adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014b) and on generative adversarial networks (GANs) (Goodfellow et al., 2014a). In this latter context, the adversaries are neural networks (rather than Turing machines) that attempt to determine whether a sample value was generated by a model or drawn from a given data distribution. Furthermore, in contrast with definitions in cryptography, practical approaches to training GANs do not consider all possible adversaries in a class, but rather one or a small number of adversaries that are optimized by training. We build on these ideas in our work.\nNeural networks are generally not meant to be great at cryptography. Famously, the simplest neural networks cannot even compute XOR, which is basic to many cryptographic algorithms. Nevertheless, as we demonstrate, neural networks can learn to protect the confidentiality of their data from other neural networks: they discover forms of encryption and decryption, without being taught specific algorithms for these purposes.\nKnowing how to encrypt is seldom enough for security and privacy. Interestingly, neural networks can also learn what to encrypt in order to achieve a desired secrecy property while maximizing utility. Thus, when we wish to prevent an adversary from seeing a fragment of a plaintext, or from estimating a function of the plaintext, encryption can be selective, hiding the plaintext only partly.\n∗Visiting from Carnegie Mellon University.\nThe resulting cryptosystems are generated automatically. In this respect, our work resembles recent research on automatic synthesis of cryptosystems, with tools such as ZooCrypt (Barthe et al., 2013), and contrasts with most of the literature, where hand-crafted cryptosystems are the norm. ZooCrypt relies on symbolic theorem-proving, rather than neural networks.\nClassical cryptography, and tools such as ZooCrypt, typically provide a higher level of transparency and assurance than we would expect by our methods. Our model of the adversary, which avoids quantification, results in much weaker guarantees. On the other hand, it is refreshingly simple, and it may sometimes be appropriate.\nConsider, for example, a neural network with several components, and suppose that we wish to guarantee that one of the components does not rely on some aspect of the input data, perhaps because of concerns about privacy or discrimination. Neural networks are notoriously difficult to explain, so it may be hard to characterize how the component functions. A simple solution is to treat the component as an adversary, and to apply encryption so that it does not have access to the information that it should not use. In this respect, the present work follows the recent research on fair representations (Edwards & Storkey, 2015; Louizos et al., 2015), which can hide or remove sensitive information, but goes beyond that work by allowing for the possibility of decryption, which supports richer dataflow structures.\nClassical cryptography may be able to support some applications along these lines. In particular, homomorphic encryption enables inference on encrypted data (Xie et al., 2014; Gilad-Bachrach et al., 2016). On the other hand, classical cryptographic functions are generally not differentiable, so they are at odds with training by stochastic gradient descent (SGD), the main optimization technique for deep neural networks. Therefore, we would have trouble learning what to encrypt, even if we know how to encrypt. Integrating classical cryptographic functions—and, more generally, integrating other known functions and relations (e.g., (Neelakantan et al., 2015))—into neural networks remains a fascinating problem.\nPrior work at the intersection of machine learning and cryptography has focused on the generation and establishment of cryptographic keys (Ruttor, 2006; Kinzel & Kanter, 2002), and on corresponding attacks (Klimov et al., 2002). In contrast, our work takes these keys for granted, and focuses on their use; a crucial, new element in our work is the reliance on adversarial goals and training. More broadly, from the perspective of machine learning, our work relates to the application of neural networks to multiagent tasks, mentioned above, and to the vibrant research on generative models and on adversarial training (e.g., (Goodfellow et al., 2014a; Denton et al., 2015; Salimans et al., 2016; Nowozin et al., 2016; Chen et al., 2016; Ganin et al., 2015)). From the perspective of cryptography, it relates to big themes such as privacy and discrimination. While we embrace a playful, exploratory approach, we do so with the hope that it will provide insights useful for further work on these topics.\nSection 2 presents our approach to learning symmetric encryption (that is, shared-key encryption, in which the same keys are used for encryption and for decryption) and our corresponding results. Appendix A explains how the same concepts apply to asymmetric encryption (that is, public-key encryption, in which different keys are used for encryption and for decryption). Section 3 considers selective protection. Section 4 concludes and suggests avenues for further research. Appendix B is a brief review of background on neural networks.\n\n2 LEARNING SYMMETRIC ENCRYPTION\nThis section discusses how to protect the confidentiality of plaintexts using shared keys. It describes the organization of the system that we consider, and the objectives of the participants in this system. It also explains the training of these participants, defines their architecture, and presents experiments.\n\n2.1 SYSTEM ORGANIZATION\nA classic scenario in security involves three parties: Alice, Bob, and Eve. Typically, Alice and Bob wish to communicate securely, and Eve wishes to eavesdrop on their communications. Thus, the desired security property is secrecy (not integrity), and the adversary is a “passive attacker” that can intercept communications but that is otherwise quite limited: it cannot initiate sessions, inject messages, or modify messages in transit.\nWe start with a particularly simple instance of this scenario, depicted in Figure 1, in which Alice wishes to send a single confidential message P to Bob. The message P is an input to Alice. When Alice processes this input, it produces an output C. (“P ” stands for “plaintext” and “C” stands for “ciphertext”.) Both Bob and Eve receive C, process it, and attempt to recover P . We represent what they compute by PBob and PEve, respectively. Alice and Bob have an advantage over Eve: they share a secret key K. We treat K as an additional input to Alice and Bob. We assume one fresh key K per plaintext P , but, at least at this abstract level, we do not impose that K and P have the same length.\nFor us, Alice, Bob, and Eve are all neural networks. We describe their structures in Sections 2.4 and 2.5. They each have parameters, which we write θA, θB , and θE , respectively. Since θA and θB need not be equal, encryption and decryption need not be the same function even if Alice and Bob have the same structure. As is common for neural networks, Alice, Bob, and Eve work over tuples of floating-point numbers, rather than sequences of bits. In other words, K, P , PBob, PEve, and C are all tuples of floating-point numbers. Note that, with this formulation, C, PBob, and PEve may consist of arbitrary floating-point numbers even if P and K consist of 0s and 1s. In practice, our implementations constrain these values to the range (−1, 1), but permit the intermediate values. We have explored alternatives (based on Williams’ REINFORCE algorithm (Williams, 1992) or on Foerster et al.’s discretization technique (Foerster et al., 2016b)), but omit them as they are not essential to our main points.\nThis set-up, although rudimentary, suffices for basic schemes, in particular allowing for the possibility that Alice and Bob decide to rely on K as a one-time pad, performing encryption and decryption simply by XORing the key K with the plaintext P and the ciphertext C, respectively. However, we do not require that Alice and Bob function in this way—and indeed, in our experiments in Section 2.5, they discover other schemes. For simplicity, we ignore the process of generating a key from a seed. We also omit the use of randomness for probabilistic encryption (Goldwasser & Micali, 1984). Such enhancements may be the subject of further work.\n\n2.2 OBJECTIVES\nInformally, the objectives of the participants are as follows. Eve’s goal is simple: to reconstruct P accurately (in other words, to minimize the error between P and PEve). Alice and Bob want to communicate clearly (to minimize the error between P and PBob), but also to hide their communication from Eve. Note that, in line with modern cryptographic definitions (e.g., (Goldwasser & Micali, 1984)), we do not require that the ciphertext C “look random” to Eve. A ciphertext may even contain obvious metadata that identifies it as such. Therefore, it is not a goal for Eve to distinguish C from a random value drawn from some distribution. In this respect, Eve’s objectives contrast with common ones for the adversaries of GANs. On the other hand, one could try to reformulate Eve’s goal in terms of distinguishing the ciphertexts constructed from two different plaintexts.\nGiven these objectives, instead of training each of Alice and Bob separately to implement some known cryptosystem (Dourlens, 1996), we train Alice and Bob jointly to communicate successfully and to defeat Eve without a pre-specified notion of what cryptosystem they may discover for this\npurpose. Much as in the definitions of GANs, we would like Alice and Bob to defeat the best possible version of Eve, rather than a fixed Eve. Of course, Alice and Bob may not win for every plaintext and every key, since knowledge of some particular plaintexts and keys may be hardwired into Eve. (For instance, Eve could always output the same plaintext, and be right at least once.) Therefore, we assume a distribution on plaintexts and keys, and phrase our goals for Alice and Bob in terms of expected values.\nWe write A(θA, P,K) for Alice’s output on input P,K, write B(θB , C,K) for Bob’s output on input C,K, and write E(θE , C) for Eve’s output on input C. We introduce a distance function d on plaintexts. Although the exact choice of this function is probably not crucial, for concreteness we take the L1 distance d(P, P ′) = Σi=1,N |Pi − P ′i | where N is the length of plaintexts. We define a per-example loss function for Eve:\nLE(θA, θE , P,K) = d(P,E(θE , A(θA, P,K)))\nIntuitively, LE(θA, θE , P,K) represents how much Eve is wrong when the plaintext is P and the key is K. We also define a loss function for Eve over the distribution on plaintexts and keys by taking an expected value:\nLE(θA, θE) = EP,K(d(P,E(θE , A(θA, P,K))))\nWe obtain the “optimal Eve” by minimizing this loss:\nOE(θA) = argminθE (LE(θA, θE))\nSimilarly, we define a per-example reconstruction error for Bob, and extend it to the distribution on plaintexts and keys:\nLB(θA, θB , P,K) = d(P,B(θB , A(θA, P,K),K))\nLB(θA, θB) = EP,K(d(P,B(θB , A(θA, P,K),K)))\nWe define a loss function for Alice and Bob by combining LB and the optimal value of LE :\nLAB(θA, θB) = LB(θA, θB)− LE(θA, OE(θA))\nThis combination reflects that Alice and Bob want to minimize Bob’s reconstruction error and to maximize the reconstruction error of the “optimal Eve”. The use of a simple subtraction is somewhat arbitrary; below we describe useful variants. We obtain the “optimal Alice and Bob” by minimizing LAB(θA, θB):\n(OA, OB) = argmin(θA,θB)(LAB(θA, θB))\nWe write “optimal” in quotes because there need be no single global minimum. In general, there are many equi-optimal solutions for Alice and Bob. As a simple example, assuming that the key is of the same size as the plaintext and the ciphertext, Alice and Bob may XOR the plaintext and the ciphertext, respectively, with any permutation of the key, and all permutations are equally good as long as Alice and Bob use the same one; moreover, with the way we architect our networks (see Section 2.4), all permutations are equally likely to arise.\nTraining begins with the Alice and Bob networks initialized randomly. The goal of training is to go from that state to (OA, OB), or close to (OA, OB). We explain the training process next.\n\n2.3 TRAINING REFINEMENTS\nOur training method is based upon SGD. In practice, much as in work on GANs, our training method cuts a few corners and incorporates a few improvements with respect to the high-level description of objectives of Section 2.2. We present these refinements next, and give further details in Section 2.5.\nFirst, the training relies on estimated values calculated over “minibatches” of hundreds or thousands of examples, rather than on expected values over a distribution.\nWe do not compute the “optimal Eve” for a given value of θA, but simply approximate it, alternating the training of Eve with that of Alice and Bob. Intuitively, the training may for example proceed roughly as follows. Alice may initially produce ciphertexts that neither Bob nor Eve understand at all. By training for a few steps, Alice and Bob may discover a way to communicate that allows Bob\nto decrypt Alice’s ciphertexts at least partly, but which is not understood by (the present version of) Eve. In particular, Alice and Bob may discover some trivial transformations, akin to rot13. After a bit of training, however, Eve may start to break this code. With some more training, Alice and Bob may discover refinements, in particular codes that exploit the key material better. Eve eventually finds it impossible to adjust to those codes. This kind of alternation is typical of games; the theory of continuous games includes results about convergence to equilibria (e.g., (Ratliff et al., 2013)) which it might be possible to apply in our setting.\nFurthermore, in the training of Alice and Bob, we do not attempt to maximize Eve’s reconstruction error. If we did, and made Eve completely wrong, then Eve could be completely right in the next iteration by simply flipping all output bits! A more realistic and useful goal for Alice and Bob is, generally, to minimize the mutual information between Eve’s guess and the real plaintext. In the case of symmetric encryption, this goal equates to making Eve produce answers indistinguishable from a random guess. This approach is somewhat analogous to methods that aim to prevent overtraining GANs on the current adversary (Salimans et al., 2016, Section 3.1). Additionally, we can tweak the loss functions so that they do not give much importance to Eve being a little lucky or to Bob making small errors that standard error-correction could easily address.\nFinally, once we stop training Alice and Bob, and they have picked their cryptosystem, we validate that they work as intended by training many instances of Eve that attempt to break the cryptosystem. Some of these instances may be derived from earlier phases in the training.\n\n2.4 NEURAL NETWORK ARCHITECTURE\nThe Architecture of Alice, Bob, and Eve Because we wish to explore whether a general neural network can learn to communicate securely, rather than to engineer a particular method, we aimed to create a neural network architecture that was sufficient to learn mixing functions such as XOR, but that did not strongly encode the form of any particular algorithm.\nTo this end, we chose the following “mix & transform” architecture. It has a first fully-connected (FC) layer, where the number of outputs is equal to the number of inputs. The plaintext and key bits are fed into this FC layer. Because each output bit can be a linear combination of all of the input bits, this layer enables—but does not mandate—mixing between the key and the plaintext bits. In particular, this layer can permute the bits. The FC layer is followed by a sequence of convolutional layers, the last of which produces an output of a size suitable for a plaintext or ciphertext. These convolutional layers learn to apply some function to groups of the bits mixed by the previous layer, without an a priori specification of what that function should be. Notably, the opposite order (convolutional followed by FC) is much more common in image-processing applications. Neural networks developed for those applications frequently use convolutions to take advantage of spatial locality. For neural cryptography, we specifically wanted locality—i.e., which bits to combine—to be a learned property, instead of a pre-specified one. While it would certainly work to manually pair each input plaintext bit with a corresponding key bit, we felt that doing so would be uninteresting.\nWe refrain from imposing further constraints that would simplify the problem. For example, we do not tie the parameters θA and θB , as we would if we had in mind that Alice and Bob should both learn the same function, such as XOR.\n\n2.5 EXPERIMENTS\nAs a proof-of-concept, we implemented Alice, Bob, and Eve networks that takeN -bit random plaintext and key values, and produce N -entry floating-point ciphertexts, for N = 16, 32, and 64. Both plaintext and key values are uniformly distributed. Keys are not deliberately reused, but may reoccur because of random selection. (The experiments in Section 3 consider more interesting distributions and also allow plaintext and key values to have different sizes.)\nWe implemented our experiments in TensorFlow (Abadi et al., 2016a;b). We ran them on a workstation with one GPU; the specific computation platform does not affect the outcome of the experiments. We plan to release the source code for the experiments.\nNetwork Structure Our networks follow the “mix & transform” pattern described in Section 2.4. The Alice network concatenates two N -bit inputs (the plaintext and the key) into a 2N -entry vector,\nusing−1 and 1 to represent the bit values. This vector is processed through a 2N×2N FC layer, then sent through a succession of four 1-D convolutional layers. The convolutional layers are described in terms of their window size, input depth, and output depth. Each has a “stride”—the amount by which the window is shifted at each step. The concrete layers are [4, 1, 2], [2, 2, 4], [1, 4, 4], and [1, 4, 1], with strides of 1, 2, 1, 1. Intuitively, the first layer slides a size-4 window across the 2N output elements from the FC layer, outputting two numbers (output depth 2), with stride 1. We use a sigmoid nonlinear unit after each layer except the final one. After the final layer, where the output is reduced to N elements, we use a tanh nonlinear unit. (Recall that tanh has an output domain between [−1, 1], bringing the values back to a range that can map to binary values.) The Bob network is identical to the Alice network The Eve network takes only the ciphertext as input, and thus has a first N × 2N FC layer.\nTraining To train the networks, we use a “minibatch” size ranging from 256 to 4096 entries. Unless otherwise specified, we present the results from the training runs with 4096 entries. We use TensorFlow’s Adam (Kingma & Ba, 2014) optimizer with a learning rate of 0.0008. We do not reduce the learning rate over time because we wish Alice, Bob, and Eve to be able to continue to respond strongly to changes in other components, until Alice and Bob—hopefully—achieve a solution that is robust to additional changes in Eve.\nTraining alternates between Alice/Bob and Eve, with Alice/Bob training for one minibatch, and then Eve training for two minibatches. We chose this ratio in order to give a slight computational edge to the adversary Eve without training it so much that it becomes excessively specific to the exact current parameters of Alice and Bob. Empirically, this value has worked well.\nLoss Functions Eve’s loss function is exactly as described above: the L1 distance between Eve’s guess and the input plaintext. The loss function for Alice and Bob is more complex, as indicated in Sections 2.2 and 2.3. This function has two components, related to Bob’s reconstruction error and to the eavesdropper’s success. The first component is simply the L1 distance between Bob’s output and the input plaintext. The latter component, on the other hand, is (N/2− Eve L1 error)2/(N/2)2. This definition expresses the goal, described in Section 2.3, that Eve should not do better than random guessing. Accordingly, this component is minimized when half of the message bits are wrong and half are right. We choose a quadratic formula in order to place more emphasis on making Eve have a large error, and to impose less of a penalty when Eve guesses a few bits correctly, as should happen occasionally even if Eve’s guesses are effectively random. Adopting this formulation allowed us to have a meaningful per-example loss function (instead of looking at larger batch statistics), and improved the robustness of training. Its cost is that our final, trained Alice and Bob typically allow Eve to reconstruct slightly more bits than purely random guessing would achieve. We have not obtained satisfactory results for loss functions that depend linearly (rather than quadratically) on Eve’s reconstruction error. The best formulation remains an open question.\nPost-training Evaluation After successfully training the networks so that they cross an accuracy threshold (e.g., at most 0.05 bits of reconstruction error for Alice and Bob, with Eve achieving only 1-2 bits more than random guessing would predict), we reset the Eve network and train it from scratch 5 times, each for up to 250,000 steps, recording the best result achieved by any Eve. An Alice/Bob combination that fails to achieve the target thresholds within 150,000 steps is a training failure. If the retrained Eves obtain a substantial advantage, the solution is non-robust. Otherwise, we consider it a successful training outcome.\nResults Figure 2 shows, for one successful run, the evolution of Bob’s reconstruction error and Eve’s reconstruction error vs. the number of training steps for N = 16 bit plaintext and key values, using a minibatch size of 4096. Each point in the graph is the mean error across 4096 examples. An ideal result would have Bob’s reconstruction error drop to zero and Eve’s reconstruction error reach 8 (half the bits wrong). In this example, both reconstruction errors start high. After a period of time, Alice and Bob start to communicate quite effectively, but in a way that allows Eve to improve its understanding as well, gradually. Then, around step 10,000, Alice and Bob counter Eve’s progress. By about step 15,000, the training goals are effectively achieved. The remaining steps merely increase Eve’s reconstruction error slightly.\nThis training graph does not look like a typical result for neural networks, where monotonicity in the number of steps is generally expected. Instead, the dynamics of this adversarial training appear somewhat more reminiscent of evolutionary processes. These dynamics appear to depend on somewhat random-seeming change to cause the bits to mix slightly, but once there is some mixing, the gradient descent can rapidly drive it farther.\nSupporting this interpretation is the observation that training is not always successful. WithN = 16, six of twenty initial runs were failures that never got Bob’s reconstruction error under the 0.05 threshold, or failed to drive Eve’s reconstruction error above 7.3 bits (of 16). In order to test the robustness of the other fourteen Alice/Bob combinations, we retrained Eve five times, and obtained reconstruction errors for Eve that ranged from 4.67 to 6.97 bits, with a mean of 6.1. Figure 3 shows the final reconstruction errors of Bob and of the most effective retrained Eve for those fourteen Alice/Bob combinations. If we somewhat arbitrarily define success as maintaining Bob’s reconstruction error at or under 0.05 bits, and requiring that Eve get at least 6 bits wrong, on average, then training succeeded half of the time (ten of twenty cases).\nAlthough training with an adversary is often unstable (Salimans et al., 2016), we suspect that some additional engineering of the neural network and its training may be able to increase this overall success rate. With a minibatch size of only 512, for example, we achieved a success rate of only 1/3 (vs. the 1/2 that we achieved with a minibatch size of 4096). In the future, it may be worth studying the impact of minibatch sizes, and also that of other parameters such as the learning rate.\nAnalogous results hold in general for N = 32 and N = 64-bit keys and plaintexts; training appears to be successful somewhat more often for N = 64. Basically, the experiments for N = 32 and N = 64 indicate that there is nothing special about N = 16 which, to a cryptographer, may look suspiciously tiny. We focus our presentation on the case of N = 16 because, first, the experiments run more rapidly, and second, it is modestly easier to examine their behavior.\nFor one successful training run, we studied the changes in the ciphertext induced by various plaintext/key pairs. Although we did not perform an exhaustive analysis of the encryption method, we did make a few observations. First, it is key-dependent: changing the key and holding the plaintext constant results in different ciphertext output. It is also plaintext-dependent, as required for successful communication. However, it is not simply XOR. In particular, the output values are often floating-point values other than 0 and 1. Moreover, the effect of a change to either a key bit or a plaintext bit is spread across multiple elements in the ciphertext, not constrained to a single bit as it would be with XOR. A single-bit flip in the key typically induces significant changes in three to six of the 16 elements in the ciphertext, and smaller changes in other elements. Plaintext bits are similarly diffused across the ciphertext.\n\n3 LEARNING WHAT TO PROTECT\nBuilding on the results of Section 2, we study selective protection. In other words, we consider the question of whether neural networks can learn what information to protect, given confidentiality objectives described in terms of an adversary. In the simplest case, a plaintext may have several components; if we wish to keep the adversary from seeing one of the components, it may suffice to encrypt it. More generally, we may wish to publish some values correlated with the plaintext but prevent the adversary from calculating other values. In such scenarios, protecting information selectively while maximizing utility is both challenging and interesting.\n\n3.1 THE PROBLEM\nTo test these ideas and to demonstrate the viability of selective protection, we focus on a concrete experiment. We construct an artificial dataset consisting of tuples of four values, 〈A, B, C, D〉. We aim to build and train a system that outputs two predictions of D, given the first three values as inputs: a “true” prediction of D (that is, the most accurate possible estimate of D given 〈A, B, C〉), and a “privacy-preserving” estimate of D, which we call D-public, defined as the best possible estimate of D that does not reveal any information about the value of C.\nThe system organization is essentially that of Figure 1, with Alice and Bob sharing a key, but here Alice receives 〈A, B, C〉 and produces D-public in addition to a ciphertext; both Bob and Eve have ac-\ncess to Alice’s outputs; Bob uses them for producing an improved estimate of D, while Eve attempts to recover C. Thus, our goal is to demonstrate that the adversarial training permits approximating D without revealing C, and that this approximation can be combined with encrypted information and with a key in order to obtain a better approximation of D.\nThe distributions of A, B, C, and D are constructed so that there is substantial correlation between them. In particular, knowing D exactly would convey some information about the likely values of C. In our experiments, A, B, C, and D are normally distributed floating-point values with a correlation matrix imposed upon them. The specifics of how we generate A, B, C, and D are standard: we compute the Cholesky decomposition LL∗ of a 4 × 4 correlation matrix that specifies the pairwise correlations, we generate vectors of 4 normally distributed floating-point values, and then we multiply these vectors by L in order to generate values of A, B, C, and D.\nAlthough A, B, C, and D do not correspond to any real-world data, these experiments are loosely inspired by real-world tasks. For example, A, B, and C may represent attributes of a user, which may in general be correlated, and D may represent a decision about the user, a prediction of the user’s behavior, or a recommendation to the user.\n\n3.2 NEURAL NETWORK ARCHITECTURE AND TRAINING GOALS\nIn the experiments that follow, we use an augmented version of the neural network architecture of Section 2.4. The inputs first go into a new FC layer (12 inputs—eight key bits and four values—and 12 outputs); the outputs of that first layer are fed into a network with the architecture of Section 2.4. Intuitively, we chose this augmented architecture because a single FC layer should be capable of predicting D from A, B, and C, as well as making a prediction decorrelated with C; and the architecture of Section 2.4 suffices to encrypt any of the output of the first layer under the key. We therefore believed this augmented architecture would be sufficient to accomplish its task, though it may be more than is necessary to do so.\nWe train Eve with the goal of minimizing the squared error of its estimate of C. We train Alice and Bob with the goal of minimizing a linear combination of three terms: the squared error of each of the two estimates of D, and the absolute value of the covariance of Eve’s estimate of C with respect to the actual value of C. We compute this covariance on a batch of training examples, and define the loss function for Alice and Bob batch-wise.\nA human engineer would naturally try to design the system somewhat differently—likely first computing the desired quantities, and then encrypting the values to be hidden, relying on the encryption and decryption components developed in Section 2 as modules. We prefer instead an end-to-end approach, because our goal is to understand whether a neural network can learn to hide information selectively, given only a high-level specification (“do not leak information about C, but let people with the right key learn as much as possible about D”). In this end-to-end approach, the system does not have distinct, pre-assigned units for encryption and decryption, and all components are trained jointly. In particular, although we benefit from some of the insights from Section 2, we do not rely on the components developed there. End-to-end approaches are sometimes preferred in machine learning because they can more efficiently use resources and information. In our case, this feature may mean, in particular, that the cryptographic operations are optimized for the given distribution of inputs. Nevertheless, our choices have the primary goal of maximizing insight, not efficiency.\n\n3.3 RESULTS\nTo understand whether the system is learning to hide information properly, we train a separate evaluator that we call “Blind Eve”, which is aware of the distribution of C. Blind Eve tries to guess C relying only upon this baseline information, whereas the real Eve also knows the intermediate output (D-public) and the ciphertext. If Eve’s reconstruction error becomes equal to that of Blind Eve, we know that Eve is not successfully extracting information from the public estimate and the ciphertext.\nFigure 4 shows the results of adversarial training to learn to hide C. The red “public error” line shows the average error of D-public vs. D. The green “Bob error” line shows Bob’s average reconstruction error, which drops over time until Bob is making the best possible prediction of D given the input. The public error becomes low, but is slightly higher than Bob’s. The blue dots show Eve’s average error improvement over Blind Eve. After a few steps of training, Eve’s error is substantially lower\nthan that of Blind Eve (by 0.1–0.2). As time proceeds, Eve’s relative advantage becomes zero. Eve is not able to reconstruct any more information about C than would be possible by simply knowing the distribution of values of C.\n\n4 CONCLUSION\nIn this paper, we demonstrate that neural networks can learn to protect communications. The learning does not require prescribing a particular set of cryptographic algorithms, nor indicating ways of applying these algorithms: it is based only on a secrecy specification represented by the training objectives. In this setting, we model attackers by neural networks; alternative models may perhaps be enabled by reinforcement learning.\nThere is more to cryptography than encryption. In this spirit, further work may consider other tasks, for example steganography, pseudorandom-number generation, or integrity checks. Finally, neural networks may be useful not only for cryptographic protections but also for attacks. While it seems improbable that neural networks would become great at cryptanalysis, they may be quite effective in making sense of metadata and in traffic analysis.\n\nACKNOWLEDGMENTS\nWe are grateful to Samy Bengio, Laura Downs, Úlfar Erlingsson, Jakob Foerster, Nando de Freitas, Ian Goodfellow, Geoff Hinton, Chris Olah, Ananth Raghunathan, and Luke Vilnis for discussions on the matter of this paper.\n\nA LEARNING ASYMMETRIC ENCRYPTION\nParalleling Section 2, this section examines asymmetric encryption (also known as public-key encryption). It presents definitions and experimental results, but omits a detailed discussion of the objectives of asymmetric encryption, of the corresponding loss functions, and of the practical refinements that we develop for training, which are analogous to those for symmetric encryption.\nA.1 DEFINITIONS\nIn asymmetric encryption, a secret is associated with each principal. The secret may be seen as a seed for generating cryptographic keys, or directly as a secret key; we adopt the latter view. A public key can be derived from the secret, in such a way that messages encrypted under the public key can be decrypted only with knowledge of the secret.\nWe specify asymmetric encryption using a twist on our specification for symmetric encryption, shown in Figure 5. Instead of directly supplying the secret encryption key to Alice, we supply the secret key to a public-key generator, the output of which is available to every node. Only Bob has access to the underlying secret key. Much as in Section 2, several variants are possible, for instance to support probabilistic encryption.\nThe public-key generator is itself a neural network, with its own parameters. The loss functions treats these parameters much like those of Alice and Bob. In training, these parameters are adjusted at the same time as those of Alice and Bob.\nA.2 EXPERIMENTS\nIn our experiments on asymmetric encryption, we rely on the same approach as in Section 2.5. In particular, we adopt the same network structure and the same approach to training.\nThe results of these experiments are intriguing, but much harder to interpret than those for symmetric encryption. In most training runs, the networks failed to achieve a robust outcome. Often, although it appeared that Alice and Bob had learned to communicate secretly, upon resetting and retraining Eve, the retrained adversary was able to decrypt messages nearly as well as Bob was.\nHowever, Figure 6 shows the results of one training run, in which even after five reset/retrain cycles, Eve was unable to decrypt messages between Alice and Bob.\nOur chosen network structure is not sufficient to learn general implementations of many of the mathematical concepts underlying modern asymmetric cryptography, such as integer modular arithmetic. We therefore believe that the most likely explanation for this successful training run was that Alice and Bob accidentally obtained some “security by obscurity” (cf. the derivation of asymmetric schemes from symmetric schemes by obfuscation (Barak et al., 2012)). This belief is somewhat reinforced by the fact that the training result was fragile: upon further training of Alice and Bob, Eve was able to decrypt the messages. However, we cannot rule out that the networks trained into some set of hard-to-invert matrix operations resulting in “public-key-like” behavior. Our results suggest that this issue deserves more exploration.\nFurther work might attempt to strengthen these results, perhaps relying on new designs of neural networks or new training procedures. A modest next step may consist in trying to learn particular asymmetric algorithms, such as lattice-based ciphers, in order to identify the required neural network structure and capacity.\n\nB BACKGROUND ON NEURAL NETWORKS\nMost of this paper assumes only a few basic notions in machine learning and neural networks, as provided by general introductions (e.g., LeCun et al. (2015)). The following is a brief review.\nNeural networks are specifications of parameterized functions. They are typically constructed out of a sequence of somewhat modular building blocks. For example, the input to Alice is a vector of bits that represents the concatenation of the key and the plaintext. This vector (x) is input into a “fully-connected” layer, which consists of a matrix multiply (by A) and a vector addition (with b): Ax+ b. The result of that operation is then passed into a nonlinear function, sometimes termed an “activation function”, such as the sigmoid function, or the hyperbolic tangent function, tanh. In classical neural networks, the activation function represents a threshold that determines whether a neuron would “fire” or not, based upon its inputs. This threshold, and matrices and vectors such as A and b, are typical neural network “parameters”. “Training” a neural network is the process that finds values of its parameters that minimize the specified loss function over the training inputs.\nFully-connected layers are powerful but require substantial amounts of memory for a large network. An alternative to fully-connected layers are “convolutional” layers. Convolutional layers operate much like their counterparts in computer graphics, by sliding a parameterized convolution window across their input. The number of parameters in this window is much smaller than in an equivalent\nfully-connected layer. Convolutional layers are useful for applying the same function(s) at every point in an input.\nA neural network architecture consists of a graph of these building blocks (often, but not always, a DAG), specifying what the individual layers are (e.g., fully-connected or convolutional), how they are parameterized (number of inputs, number of outputs, etc.), and how they are wired.", "ground_truth": "ICLR 2017 conference submission\n\n---\n\nLearning to Protect Communications with Adversarial Neural Cryptography\n\n---\n\nWe ask whether neural networks can learn to use secret keys to protect information from other neural networks. Specifically, we focus on ensuring confidentiality properties in a multiagent system, and we specify those properties in terms of an adversary. Thus, a system may consist of neural networks named Alice and Bob, and we aim to limit what a third neural network named Eve learns from eavesdropping on the communication between Alice and Bob. We do not prescribe specific cryptographic algorithms to these neural networks; instead, we train end-to-end, adversarially. We demonstrate that the neural networks can learn how to perform forms of encryption and decryption, and also how to apply these operations selectively in order to meet confidentiality goals.\n\n---\n\nThis paper proposed to use GAN for encrypted communications.\n\nIn section 2, the authors proposed a 3 part neural network trained to encode and decode data. This model does not have any practical value except paving the way for describing the next model in section 3: it is strictly worse than any provable cryptography system.\n\nIn section 3, the authors designed a task where they want to hide part of the data, which has correlated fields, while publishing the rest. However, I'm having trouble thinking of an application where this system is better than simply decorrelating the data and encrypting the fields one wants to hide with a provable cryptography system while publishing the rest in plain text.\n\n---\n\nICLR committee final decision\n\n---\n\nICLR 2017 pcs\n\n---\n\nInteresting paper but not over the accept bar.\n\n---\n\n06 Feb 2017\n\n---\n\nICLR 2017 conference AnonReviewer2\n\n---\n\nInteresting thought experiment, but strong concerns about the practicality of the approach\n\n---\n\nThe submission proposes to modify the typical GAN architecture slightly to include \"encrypt\" (Alice) and \"decrypt\" (Bob) modules as well as a module trying to decrypt the signal without a key (Eve). Through repeated transmission of signals, the adversarial game is intended to converge to a system in which Alice and Bob can communicate securely (or at least a designated part of the signal should be secure), while a sophisticated Eve cannot break their code. Examples are given on toy data:\n\"As a proof-of-concept, we implemented Alice, Bob, and Eve networks that take N-bit random plain-text and key values, and produce N-entry floating-point ciphertexts, for N = 16, 32, and 64. Both plaintext and key values are uniformly distributed.\"\n\nThe idea considered here is cute. If some, but not necessarily all of the signal is meant to be secure, the modules can learn to encrypt and decrypt a signal, while an adversary is simultaneously learned that tries to break the encryption. In this way, some of the data can remain unencrypted, while the portion that is e.g. correlated with the encrypted signal will have to be encrypted in order for Eve to not be able to predict the encrypted part.\n\nWhile this is a nice thought experiment, there are significant barriers to this submission having a practical impact:\n1) GANs, and from the convergence figures also the objective considered here, are quite unstable to optimize. The only guarantees of privacy are for an Eve that is converged to a very strong adversary (stronger than a dedicated attack over time). I do not see how one can have any sort of reliable guarantee of the safety of the data transmission from the proposed approach, at least the paper does not outline such a guarantee.\n2) Public key encryption systems are readily available, computationally feasible, and successfully applied almost anywhere. The toy examples given in the paper do not at all convince me that this is solving a real-world problem at this point. Perhaps a good example will come up in the near future, and this work will be shown to be justified, but until such an example is shown, the approach is more of an interesting thought experiment.\n\n---\n\n19 Dec 2016\n\n---\n\nICLR 2017 conference AnonReviewer3\n\n---\n\nVery creative application of adversarial training to cryptography with some weaknesses in the toy examples\n\n---\n\nThe paper deals with an interesting application of adversarial training to encryption. It considers the standard scenario of Alice, Eve and Bob, where A and B aim to exchange messages conditioned on a shared key, while Eve should be unable to encrypt the message. Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy. The concepts, ideas and previous literature are quite nicely and carefully presented.\n\nThe only major concern I have - and I apologize to the authors for not raising this earlier - are the experiments in section 3. In particular, I don't quite get the scenario. The reasoning here seems to be as follows: given information < A, B, C, D >, I want to give the public the value of D (e.g. movies watched) without releasing information about C (e.g. gender). In this scenario, Eve would need to be able to reconstruct D as good as possible without gaining information about C. What is described in section 3, however, is that D and D-public are both reconstructed by Bob, but why would Bob reconstruct the latter (he is not public, in particular because he is allowed to reconstruct C, which is not tested here)? Also, Eve only tries to estimate C, thus rendering the scenario not different in any way to the scenario considered in section 2.\n\nI have two more minor concerns:\n\n1) As raised in the pre-review, Eve should actually be stronger then Alice and Bob in order to be able to compensate for the missing key. The authors noted they have been doing these experiments and are going to add the results.\n\n2) In any natural encryption case I would expect the length of the key to be much shorter then the length of the message. This, however, could potentially make the scenario much easier for Eve (although I doubt any of the results will change if the key is long enough).\n\nI like the creative application of adversarial training to a completely different domain, and I believe it could be the starting point of a very interesting direction in cryptographic systems or in privacy applications (although it is unclear whether the weak guarantees of neural network based approaches can ever be overcome). At the same time the application in the privacy setting leaves me quite confused, and the symmetric encryption example is not particularly strong either. I'd appreciate if the authors could address the major concern I raised above, and I will be quite happy to raise the score in case this confusion can be resolved.\n\n---\n\n16 Dec 2016\n\n---\n\nICLR 2017 conference AnonReviewer1\n\n---\n\nNo Title\n\n---\n\nThis paper proposed to use GAN for encrypted communications.\n\nIn section 2, the authors proposed a 3 part neural network trained to encode and decode data. This model does not have any practical value except paving the way for describing the next model in section 3: it is strictly worse than any provable cryptography system.\n\nIn section 3, the authors designed a task where they want to hide part of the data, which has correlated fields, while publishing the rest. However, I'm having trouble thinking of an application where this system is better than simply decorrelating the data and encrypting the fields one wants to hide with a provable cryptography system while publishing the rest in plain text.\n\n---\n\n16 Dec 2016\n\n---\n\npre review questions\n\n---\n\nICLR 2017 conference AnonReviewer2\n\n---\n\n02 Dec 2016\n\n---\n\nComplexity of Eve\n\n---\n\nICLR 2017 conference AnonReviewer3\n\n---\n\n29 Nov 2016\n\n---\n\nMartín Abadi, David G. Andersen\n\n---\n\nAdversarial training of neural networks to learn rudimentary forms of encryption with no pre-specified algorithms"}
{"paper_id": "690", "paper_text": "TITLE: AN ANALYSIS OF DEEP NEURAL NETWORK MODELS FOR PRACTICAL APPLICATIONS\n\nABSTRACT: Since the emergence of Deep Neural Networks (DNNs) as a prominent technique in the field of computer vision, the ImageNet classification challenge has played a major role in advancing the state-of-the-art. While accuracy figures have steadily increased, the resource utilisation of winning models has not been properly taken into account. In this work, we present a comprehensive analysis of important metrics in practical applications: accuracy, memory footprint, parameters, operations count, inference time and power consumption. Key findings are: (1) power consumption is independent of batch size and architecture; (2) accuracy and inference time are in a hyperbolic relationship; (3) energy constraint are an upper bound on the maximum achievable accuracy and model complexity; (4) the number of operations is a reliable estimate of the inference time. We believe our analysis provides a compelling set of information that helps design and engineer efficient DNNs.\n\n1 INTRODUCTION\nSince the breakthrough in 2012 ImageNet competition (Russakovsky et al., 2015) achieved by AlexNet (Krizhevsky et al., 2012) — the first entry that used a Deep Neural Network (DNN) — several other DNNs with increasing complexity have been submitted to the challenge in order to achieve better performance.\nIn the ImageNet classification challenge, the ultimate goal is to obtain the highest accuracy in a multi-class classification problem framework, regardless of the actual inference time. We believe that this has given rise to several problems. Firstly, it is now normal practice to run several trained instances of a given model over multiple similar instances of each validation image. This practice, also know as model averaging or ensemble of DNNs, dramatically increases the amount of computation required at inference time to achieve the published accuracy. Secondly, model selection is hindered by the fact that different submissions are evaluating their (ensemble of) models a different number of times on the validation images, and therefore the reported accuracy is biased on the specific sampling technique (and ensemble size). Thirdly, there is currently no incentive in speeding up inference time, which is a key element in practical applications of these models, and affects resource utilisation, power-consumption, and latency.\nThis article aims to compare state-of-the-art DNN architectures, submitted for the ImageNet challenge over the last 4 years, in terms of computational requirements and accuracy. We compare these architectures on multiple metrics related to resource utilisation in actual deployments: accuracy, memory footprint, parameters, operations count, inference time and power consumption. The purpose of this paper is to stress the importance of these figures, which are essential hard constraints for the optimisation of these networks in practical deployments and applications.\n\n2 METHODS\nIn order to compare the quality of different models, we collected and analysed the accuracy values reported in the literature. We immediately found that different sampling techniques do not allow for a direct comparison of resource utilisation. For example, central-crop (top-5 validation) errors of a\nsingle run of VGG-161 (Simonyan & Zisserman, 2014) and GoogLeNet (Szegedy et al., 2014) are 8.70% and 10.07% respectively, revealing that VGG-16 performs better than GoogLeNet. When models are run with 10-crop sampling,2 then the errors become 9.33% and 9.15% respectively, and therefore VGG-16 will perform worse than GoogLeNet, using a single central-crop. For this reason, we decided to base our analysis on re-evaluations of top-1 accuracies3 for all networks with a single central-crop sampling technique (Zagoruyko, 2016).\nFor inference time and memory usage measurements we have used Torch7 (Collobert et al., 2011) with cuDNN-v5 (Chetlur et al., 2014) and CUDA-v8 back-end. All experiments were conducted on a JetPack-2.3 NVIDIA Jetson TX1 board (nVIDIA): an embedded visual computing system with a 64-bit ARM R© A57 CPU, a 1 T-Flop/s 256-core NVIDIA Maxwell GPU and 4 GB LPDDR4 of shared RAM. We use this resource-limited device to better underline the differences between network architecture, but similar results can be obtained on most recent GPUs, such as the NVIDIA K40 or Titan X, to name a few. Operation counts were obtained using an open-source tool that we developed (Paszke, 2016). For measuring the power consumption, a Keysight 1146B Hall effect current probe has been used with a Keysight MSO-X 2024A 200MHz digital oscilloscope with a sampling period of 2 s and 50 kSa/s sample rate. The system was powered by a Keysight E3645A GPIB controlled DC power supply.\n\n3 RESULTS\nIn this section we report our results and comparisons. We analysed the following DDNs: AlexNet (Krizhevsky et al., 2012), batch normalised AlexNet (Zagoruyko, 2016), batch normalised Network In Network (NIN) (Lin et al., 2013), ENet (Paszke et al., 2016) for ImageNet (Culurciello, 2016), GoogLeNet (Szegedy et al., 2014), VGG-16 and -19 (Simonyan & Zisserman, 2014), ResNet-18, -34, -50, -101 and -152 (He et al., 2015), Inception-v3 (Szegedy et al., 2015) and Inception-v4 (Szegedy et al., 2016) since they obtained the highest performance, in these four years, on the ImageNet (Russakovsky et al., 2015) challenge.\n1 In the original paper this network is called VGG-D, which is the best performing network. Here we prefer to highlight the number of layer utilised, so we will call it VGG-16 in this publication.\n2 From a given image multiple patches are extracted: four corners plus central crop and their horizontal mirrored twins.\n3 Accuracy and error rate always sum to 100, therefore in this paper they are used interchangeably.\n\n3.1 ACCURACY\nFigure 1 shows one-crop accuracies of the most relevant entries submitted to the ImageNet challenge, from the AlexNet (Krizhevsky et al., 2012), on the far left, to the best performing Inception-v4 (Szegedy et al., 2016). The newest ResNet and Inception architectures surpass all other architectures by a significant margin of at least 7%.\nFigure 2 provides a different, but more informative view of the accuracy values, because it also visualises computational cost and number of network’s parameters. The first thing that is very apparent is that VGG, even though it is widely used in many applications, is by far the most expensive architecture — both in terms of computational requirements and number of parameters. Its 16- and 19-layer implementations are in fact isolated from all other networks. The other architectures form a steep straight line, that seems to start to flatten with the latest incarnations of Inception and ResNet. This might suggest that models are reaching an inflection point on this data set. At this inflection point, the costs — in terms of complexity — start to outweigh gains in accuracy. We will later show that this trend is hyperbolic.\n\n3.2 INFERENCE TIME\nFigure 3 reports inference time per image on each architecture, as a function of image batch size (from 1 to 64). We notice that VGG processes one image in a fifth of a second, making it a less likely contender in real-time applications on an NVIDIA TX1. AlexNet shows a speed up of roughly 3× going from batch of 1 to 64 images, due to weak optimisation of its fully connected layers. It is a very surprising finding, that will be further discussed in the next subsection.\n\n3.3 POWER\nPower measurements are complicated by the high frequency swings in current consumption, which required high sampling current read-out to avoid aliasing. In this work, we used a 200MHz digital oscilloscope with a current probe, as reported in section 2. Other measuring instruments, such as an AC power strip with 2Hz sampling rate, or a GPIB controlled DC power supply with 12Hz sampling rate, did not provide enough bandwidth to properly conduct power measurements.\nIn figure 4 we see that the power consumption is mostly independent with the batch size. Low power values for AlexNet (batch of 1) and VGG (batch of 2) are associated to slower forward times per image, as shown in figure 3.\n\n3.4 MEMORY\nWe analysed system memory consumption of the TX1 device, which uses shared memory for both CPU and GPU. Figure 5 shows that the maximum system memory usage is initially constant and then raises with the batch size. This is due the initial memory allocation of the network model — which is the large static component — and the contribution of the memory required while processing the batch, proportionally increasing with the number of images. In figure 6 we can also notice that the initial allocation never drops below 200MB, for network sized below 100MB, and it is linear afterwards, with respect to the parameters and a slope of 1.30.\n\n3.5 OPERATIONS\nOperations count is essential for establishing a rough estimate of inference time and hardware circuit size, in case of custom implementation of neural network accelerators. In figure 7, for a batch of 16 images, there is a linear relationship between operations count and inference time per image. Therefore, at design time, we can pose a constraint on the number of operation to keep processing speed in a usable range for real-time applications or resource-limited deployments.\n\n3.6 OPERATIONS AND POWER\nIn this section we analyse the relationship between power consumption and number of operations required by a given model. Figure 8 reports that there is no specific power footprint for different architectures. When full resources utilisation is reached, generally with larger batch sizes, all networks consume roughly an additional 11.8W, with a standard deviation of 0.7W. Idle power is 1.30W. This corresponds to the maximum system power at full utilisation. Therefore, if energy consumption is one of our concerns, for example for battery-powered devices, one can simply choose the slowest architecture which satisfies the application minimum requirements.\n\n3.7 ACCURACY AND THROUGHPUT\nWe note that there is a non-trivial linear upper bound between accuracy and number of inferences per unit time. Figure 9 illustrates that for a given frame rate, the maximum accuracy that can be achieved is linearly proportional to the frame rate itself. All networks analysed here come from several publications, and have been independently trained by other research groups. A linear fit of the accuracy shows all architecture trade accuracy vs. speed. Moreover, chosen a specific inference time, one can now come up with the theoretical accuracy upper bound when resources are fully\nutilised, as seen in section 3.6. Since the power consumption is constant, we can even go one step further, and obtain an upper bound in accuracy even for an energetic constraint, which could possibly be an essential designing factor for a network that needs to run on an embedded system.\nAs the spoiler in section 3.1 gave already away, the linear nature of the accuracy vs. throughput relationship translates into a hyperbolical one when the forward inference time is considered instead. Then, given that the operations count is linear with the inference time, we get that the accuracy has an hyperbolical dependency on the amount of computations that a network requires.\n\n3.8 PARAMETERS UTILISATION\nDNNs are known to be highly inefficient in utilising their full learning power (number of parameters / degrees of freedom). Prominent work (Han et al., 2015) exploits this flaw to reduce network file size up to 50×, using weights pruning, quantisation and variable-length symbol encoding. It is worth noticing that, using more efficient architectures to begin with may produce even more compact representations. In figure 10 we clearly see that, although VGG has a better accuracy than AlexNet (as shown by figure 1), its information density is worse. This means that the amount of degrees of freedom introduced in the VGG architecture bring a lesser improvement in terms of accuracy. Moreover, ENet (Paszke et al., 2016) — which we have specifically designed to be highly efficient and it has been adapted and retrained on ImageNet (Culurciello, 2016) for this work — achieves the highest score, showing that 24× less parameters are sufficient to provide state-of-the-art results.\n\n4 CONCLUSIONS\nIn this paper we analysed multiple state-of-the-art deep neural networks submitted to the ImageNet challenge, in terms of accuracy, memory footprint, parameters, operations count, inference time and power consumption. Our goal is to provide insights into the design choices that can lead to efficient neural networks for practical application, and optimisation of the often-limited resources in actual deployments, which lead us to the creation of ENet — or Efficient-Network — for ImageNet. We show that accuracy and inference time are in a hyperbolic relationship: a little increment in accuracy costs a lot of computational time. We show that number of operations in a network model can effectively estimate inference time. We show that an energy constraint will set a specific upper bound on the maximum achievable accuracy and model complexity, in terms of operations counts. Finally, we show that ENet is the best architecture in terms of parameters space utilisation, squeezing up to 13× more information per parameter used respect to the reference model AlexNet, and 24× respect VGG-19.\n\nACKNOWLEDGMENTS\nThis paper would have not look so pretty without the Python Software Foundation, the matplotlib library and the communities of stackoverflow and TEX of StackExchange which I ought to thank. This work is partly supported by the Office of Naval Research (ONR) grants N00014-12-10167, N00014-15-1-2791 and MURI N00014-10-1-0278. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the TX1, Titan X, K40 GPUs used for this research.", "ground_truth": "ICLR 2017 conference submission\n\n---\n\nAn Analysis of Deep Neural Network Models for Practical Applications\n\n---\n\nSince the emergence of Deep Neural Networks (DNNs) as a prominent technique in the field of computer vision, the ImageNet classification challenge has played a major role in advancing the state-of-the-art. While accuracy figures have steadily increased, the resource utilisation of winning models has not been properly taken into account. In this work, we present a comprehensive analysis of important metrics in practical applications: accuracy, memory footprint, parameters, operations count, inference time and power consumption. Key findings are: (1) power consumption is independent of batch size and architecture; (2) accuracy and inference time are in a hyperbolic relationship; (3) energy constraint are an upper bound on the maximum achievable accuracy and model complexity; (4) the number of operations is a reliable estimate of the inference time. We believe our analysis provides a compelling set of information that helps design and engineer efficient DNNs.\n\n---\n\nA few issues with this paper:\n1- I find finding #2 trivial and unworthy of mention, but the author don't seem to agree with me that it is. See discussions.\n2- Finding #1 relies on Fig #4, which appears very noisy and doesn't provide any error analysis. It makes me question how robust this finding is. One would have naively expected the power usage trend to mirror Fig #3, but given the level of noise, I can't convince myself whether the null hypothesis of there being no dependency between batch size and power consumption is more likely than the alternative.\n3- Paper is unfriendly to colorblind readers (or those with B/W printers)\n\nOverall, this paper is a reasonable review of where we are in terms of SOTA vision architectures, but doesn't provide much new insight. I found most interesting the clear illustration that VGG models stand out in terms of being a bad tradeoff in resource-constrained environments (too many researchers are tempted to benchmark their model compression algorithm on VGG-class models because that's always where one can show 10x improvements without doing much.)\n\n---\n\nICLR committee final decision\n\n---\n\nICLR 2017 pcs\n\n---\n\nThe paper presents an evaluation of off-the-shelf image classification architectures. The findings are not too surprising and don't provide much new insight.\n\n---\n\n06 Feb 2017\n\n---\n\nICLR 2017 conference AnonReviewer1\n\n---\n\nNo Title\n\n---\n\nThe paper evaluates recent development in competitive ILSVRC CNN architectures from the perspective of resource utilization. It is clear that a lot of work has been put into the evaluations. The findings are well presented and the topic itself is important.\n\nHowever, most of the results are not surprising to people working with CNNs on a regular basis. And even if they are, I am not convinced about their practical value. It is hard to tell what we actually learn from these findings when approaching new problems with computational constraints or when in production settings. In my opinion, this is mainly because the paper does not discuss realistic circumstances.\n\nMain concerns:\n1) The evaluation does not tell me much for realistic scenarios, that mostly involve fine-tuning networks, as ILSVRC is just a starting point in most cases. VGG for instance really shines for fine-tuning, but it is cumbersome to train from scratch. And VGG works well for compression, too. So possibly it is a very good choice if these by now standard steps are taken into account. Such questions are of high practical relevance!\n\n2) Compressed networks have a much higher acc/parameter density, so comparison how well models can be compressed is important, or at least comparing to some of the most well-known and publicly available compressed networks.\n\n3) There is no analysis on the actual topology of the networks and where the bottlenecks lie. This would be very useful to have as well.\n\nMinor concern:\n1) Why did the authors choose to use batch normalization in NiN and AlexNet?\n\n---\n\n18 Dec 2016\n\n---\n\nICLR 2017 conference AnonReviewer2\n\n---\n\nsolid work but not surprising\n\n---\n\nThe authors did solid work in collecting all the reported data. However, most findings don't seem to be too surprising to me:\n\n- Finding #1 mainly shows that all architectures and batch sizes manage to utilize the GPU fully (or to the same percentage).\n\n- Regarding Finding #2, I agree that from a linear relationship in Figure 9 you could conclude said hyperbolic relationship.\nHowever, for this finding to be relevant, it has to hold especially for the latest generations of models. These cluster in the upper left corner of Figure 9 and on their own do not seem to show too much of a linear behaviour. Therefore I think there is not enough evidence to conclude asymptotic hyperbolic behaviour: For this the linear behaviour would have to be the stronger, the more models approach the upper left corner.\n\n- Finding #3 seems to be a simple conclusion from finding #1: As long as slower models are better and faster models do draw the same power, finding #3 holds.\n\n- Finding #4 is again similar to finding #1: If all architectures manage to fully utilize the GPU, inference time should be proportional to the number of operations.\n\nMaybe the most interesting finding would be that all tested models seem to use the same percentage of computational resources available on the GPU, while one might expect that more complex models don't manage to utilize as much computational resources due to inter-dependencies. However actual GPU utilization was not evaluated and as the authors choose to use an older GPU, one would expect that all models manage to make use of all available computational power.\n\nAdditionally, I think these findings would have to be put in relation with compressing techniques or tested on actual production networks to be of more interest.\n\n---\n\n16 Dec 2016\n\n---\n\nICLR 2017 conference AnonReviewer3\n\n---\n\nInteresting paper. Some flaws.\n\n---\n\nA few issues with this paper:\n1- I find finding #2 trivial and unworthy of mention, but the author don't seem to agree with me that it is. See discussions.\n2- Finding #1 relies on Fig #4, which appears very noisy and doesn't provide any error analysis. It makes me question how robust this finding is. One would have naively expected the power usage trend to mirror Fig #3, but given the level of noise, I can't convince myself whether the null hypothesis of there being no dependency between batch size and power consumption is more likely than the alternative.\n3- Paper is unfriendly to colorblind readers (or those with B/W printers)\n\nOverall, this paper is a reasonable review of where we are in terms of SOTA vision architectures, but doesn't provide much new insight. I found most interesting the clear illustration that VGG models stand out in terms of being a bad tradeoff in resource-constrained environments (too many researchers are tempted to benchmark their model compression algorithm on VGG-class models because that's always where one can show 10x improvements without doing much.)\n\n---\n\n05 Dec 2016\n\n---\n\nQuestion on structure of paper\n\n---\n\nICLR 2017 conference AnonReviewer1\n\n---\n\n02 Dec 2016\n\n---\n\nRelationship to network compression etc\n\n---\n\nICLR 2017 conference AnonReviewer2\n\n---\n\n30 Nov 2016\n\n---\n\nHyperbolic relationships\n\n---\n\nICLR 2017 conference AnonReviewer3\n\n---\n\n29 Nov 2016\n\n---\n\nAlfredo Canziani, Adam Paszke, Eugenio Culurciello\n\n---\n\nAnalysis of ImageNet winning architectures in terms of accuracy, memory footprint, parameters, operations count, inference time and power consumption."}
